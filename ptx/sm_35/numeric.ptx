//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Jul 31 22:29:38 2014 (1406860178)
// Cuda compilation tools, release 6.5, V6.5.14
//

.version 4.1
.target sm_35
.address_size 64

.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.func  (.param .b64 func_retval0) __internal_lgamma_pos
(
	.param .b64 __internal_lgamma_pos_param_0
)
;

.weak .func  (.param .b32 func_retval0) cudaMalloc(
	.param .b64 cudaMalloc_param_0,
	.param .b64 cudaMalloc_param_1
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 func_retval0) cudaFuncGetAttributes(
	.param .b64 cudaFuncGetAttributes_param_0,
	.param .b64 cudaFuncGetAttributes_param_1
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 func_retval0) cudaDeviceGetAttribute(
	.param .b64 cudaDeviceGetAttribute_param_0,
	.param .b32 cudaDeviceGetAttribute_param_1,
	.param .b32 cudaDeviceGetAttribute_param_2
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 func_retval0) cudaGetDevice(
	.param .b64 cudaGetDevice_param_0
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessor(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_3
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___floorf(
	.param .b32 ___floorf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___floorf_param_0];
	cvt.rmi.f32.f32	%f2, %f1;
	st.param.f32	[func_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___floori(
	.param .b32 ___floori_param_0
)
{
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.u32 	%r1, [___floori_param_0];
	cvt.rn.f32.s32	%f1, %r1;
	cvt.rmi.f32.f32	%f2, %f1;
	cvt.rzi.s32.f32	%r2, %f2;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___floorj(
	.param .b32 ___floorj_param_0
)
{
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.u32 	%r1, [___floorj_param_0];
	cvt.rn.f32.u32	%f1, %r1;
	cvt.rmi.f32.f32	%f2, %f1;
	cvt.rzi.u32.f32	%r2, %f2;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___floorc(
	.param .b32 ___floorc_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.s8 	%rs1, [___floorc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	cvt.rmi.f32.f32	%f2, %f1;
	cvt.rzi.s32.f32	%r1, %f2;
	cvt.s32.s8 	%r2, %r1;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___floorh(
	.param .b32 ___floorh_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.u8 	%rs1, [___floorh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	cvt.rmi.f32.f32	%f2, %f1;
	cvt.rzi.u32.f32	%r1, %f2;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___floord(
	.param .b64 ___floord_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___floord_param_0];
	cvt.rmi.f64.f64	%fd2, %fd1;
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___ceilf(
	.param .b32 ___ceilf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___ceilf_param_0];
	cvt.rpi.f32.f32	%f2, %f1;
	st.param.f32	[func_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___ceili(
	.param .b32 ___ceili_param_0
)
{
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.u32 	%r1, [___ceili_param_0];
	cvt.rn.f32.s32	%f1, %r1;
	cvt.rpi.f32.f32	%f2, %f1;
	cvt.rzi.s32.f32	%r2, %f2;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___ceilj(
	.param .b32 ___ceilj_param_0
)
{
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.u32 	%r1, [___ceilj_param_0];
	cvt.rn.f32.u32	%f1, %r1;
	cvt.rpi.f32.f32	%f2, %f1;
	cvt.rzi.u32.f32	%r2, %f2;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___ceilc(
	.param .b32 ___ceilc_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.s8 	%rs1, [___ceilc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	cvt.rpi.f32.f32	%f2, %f1;
	cvt.rzi.s32.f32	%r1, %f2;
	cvt.s32.s8 	%r2, %r1;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___ceilh(
	.param .b32 ___ceilh_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<3>;


	ld.param.u8 	%rs1, [___ceilh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	cvt.rpi.f32.f32	%f2, %f1;
	cvt.rzi.u32.f32	%r1, %f2;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___ceild(
	.param .b64 ___ceild_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___ceild_param_0];
	cvt.rpi.f64.f64	%fd2, %fd1;
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___roundf(
	.param .b32 ___roundf_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<4>;
	.reg .f32 	%f<10>;


	ld.param.f32 	%f4, [___roundf_param_0];
	abs.f32 	%f5, %f4;
	mov.b32 	 %r1, %f4;
	and.b32  	%r2, %r1, -2147483648;
	or.b32  	%r3, %r2, 1056964608;
	mov.b32 	 %f6, %r3;
	add.f32 	%f7, %f6, %f4;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f4, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB17_2;

	cvt.rzi.f32.f32	%f9, %f4;

BB17_2:
	st.param.f32	[func_retval0+0], %f9;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___roundi(
	.param .b32 ___roundi_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<10>;


	ld.param.u32 	%r1, [___roundi_param_0];
	cvt.rn.f32.s32	%f1, %r1;
	abs.f32 	%f5, %f1;
	mov.b32 	 %r2, %f1;
	and.b32  	%r3, %r2, -2147483648;
	or.b32  	%r4, %r3, 1056964608;
	mov.b32 	 %f6, %r4;
	add.f32 	%f7, %f6, %f1;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f1, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB18_2;

	cvt.rzi.f32.f32	%f9, %f1;

BB18_2:
	cvt.rzi.s32.f32	%r5, %f9;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___roundj(
	.param .b32 ___roundj_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<10>;


	ld.param.u32 	%r1, [___roundj_param_0];
	cvt.rn.f32.u32	%f1, %r1;
	abs.f32 	%f5, %f1;
	mov.b32 	 %r2, %f1;
	and.b32  	%r3, %r2, -2147483648;
	or.b32  	%r4, %r3, 1056964608;
	mov.b32 	 %f6, %r4;
	add.f32 	%f7, %f6, %f1;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f1, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB19_2;

	cvt.rzi.f32.f32	%f9, %f1;

BB19_2:
	cvt.rzi.u32.f32	%r5, %f9;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___roundc(
	.param .b32 ___roundc_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<10>;


	ld.param.s8 	%rs1, [___roundc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f5, %f1;
	mov.b32 	 %r1, %f1;
	and.b32  	%r2, %r1, -2147483648;
	or.b32  	%r3, %r2, 1056964608;
	mov.b32 	 %f6, %r3;
	add.f32 	%f7, %f6, %f1;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f1, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB20_2;

	cvt.rzi.f32.f32	%f9, %f1;

BB20_2:
	cvt.rzi.s32.f32	%r4, %f9;
	cvt.s32.s8 	%r5, %r4;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___roundh(
	.param .b32 ___roundh_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<10>;


	ld.param.u8 	%rs1, [___roundh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f5, %f1;
	mov.b32 	 %r1, %f1;
	and.b32  	%r2, %r1, -2147483648;
	or.b32  	%r3, %r2, 1056964608;
	mov.b32 	 %f6, %r3;
	add.f32 	%f7, %f6, %f1;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f1, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB21_2;

	cvt.rzi.f32.f32	%f9, %f1;

BB21_2:
	cvt.rzi.u32.f32	%r4, %f9;
	and.b32  	%r5, %r4, 255;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___roundd(
	.param .b64 ___roundd_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f64 	%fd<9>;


	ld.param.f64 	%fd8, [___roundd_param_0];
	abs.f64 	%fd1, %fd8;
	setp.ge.f64	%p1, %fd1, 0d4330000000000000;
	@%p1 bra 	BB22_2;

	add.f64 	%fd5, %fd1, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd6, %fd5;
	setp.lt.f64	%p2, %fd1, 0d3FE0000000000000;
	selp.f64	%fd7, 0d0000000000000000, %fd6, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1, %temp}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd8;
	}
	and.b32  	%r4, %r3, -2147483648;
	or.b32  	%r5, %r2, %r4;
	mov.b64 	%fd8, {%r1, %r5};

BB22_2:
	st.param.f64	[func_retval0+0], %fd8;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___tgammaf(
	.param .b32 ___tgammaf_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<99>;


	ld.param.f32 	%f24, [___tgammaf_param_0];
	setp.ltu.f32	%p1, %f24, 0f00000000;
	@%p1 bra 	BB23_5;

	setp.gt.f32	%p2, %f24, 0f42100000;
	selp.f32	%f1, 0f42100000, %f24, %p2;
	setp.gt.f32	%p3, %f1, 0f42081EB8;
	add.f32 	%f2, %f1, 0fBF800000;
	selp.f32	%f86, %f2, %f1, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB23_2;
	bra.uni 	BB23_4;

BB23_2:
	mov.f32 	%f88, %f87;

BB23_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB23_3;

BB23_4:
	setp.ltu.f32	%p6, %f1, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f1;
	setp.lt.f32	%p7, %f1, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f2;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB23_12;

BB23_5:
	cvt.rmi.f32.f32	%f49, %f24;
	setp.eq.f32	%p9, %f49, %f24;
	selp.f32	%f50, 0f7FFFFFFF, %f24, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f13, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f13, 0fC2081EB8;
	add.f32 	%f51, %f13, 0f40C00000;
	selp.f32	%f95, %f51, %f13, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB23_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB23_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB23_7;

BB23_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f13, 0fC2081EB8;
	@%p14 bra 	BB23_12;

	add.f32 	%f70, %f13, 0f3F800000;
	mul.f32 	%f71, %f13, %f70;
	add.f32 	%f72, %f13, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f13, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f13, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f13, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f24, 0fC2280000;
	@%p15 bra 	BB23_12;

	cvt.rzi.s32.f32	%r1, %f24;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32	%p16, %r2, 1;
	@%p16 bra 	BB23_12;

	mov.f32 	%f98, 0f80000000;

BB23_12:
	st.param.f32	[func_retval0+0], %f98;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___tgammai(
	.param .b32 ___tgammai_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.u32 	%r1, [___tgammai_param_0];
	cvt.rn.f32.s32	%f1, %r1;
	setp.ltu.f32	%p1, %f1, 0f00000000;
	@%p1 bra 	BB24_5;

	setp.gt.f32	%p2, %f1, 0f42100000;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB24_2;
	bra.uni 	BB24_4;

BB24_2:
	mov.f32 	%f88, %f87;

BB24_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB24_3;

BB24_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB24_12;

BB24_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB24_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB24_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB24_7;

BB24_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB24_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f1, 0fC2280000;
	@%p15 bra 	BB24_12;

	cvt.rzi.s32.f32	%r2, %f1;
	and.b32  	%r3, %r2, 1;
	setp.eq.b32	%p16, %r3, 1;
	@%p16 bra 	BB24_12;

	mov.f32 	%f98, 0f80000000;

BB24_12:
	cvt.rzi.s32.f32	%r4, %f98;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___tgammaj(
	.param .b32 ___tgammaj_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.u32 	%r1, [___tgammaj_param_0];
	cvt.rn.f32.u32	%f1, %r1;
	setp.ltu.f32	%p1, %f1, 0f00000000;
	@%p1 bra 	BB25_5;

	setp.gt.f32	%p2, %f1, 0f42100000;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB25_2;
	bra.uni 	BB25_4;

BB25_2:
	mov.f32 	%f88, %f87;

BB25_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB25_3;

BB25_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB25_12;

BB25_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB25_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB25_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB25_7;

BB25_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB25_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f1, 0fC2280000;
	@%p15 bra 	BB25_12;

	cvt.rzi.s32.f32	%r2, %f1;
	and.b32  	%r3, %r2, 1;
	setp.eq.b32	%p16, %r3, 1;
	@%p16 bra 	BB25_12;

	mov.f32 	%f98, 0f80000000;

BB25_12:
	cvt.rzi.u32.f32	%r4, %f98;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___tgammac(
	.param .b32 ___tgammac_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.s8 	%rs1, [___tgammac_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	setp.lt.s16	%p1, %rs1, 0;
	@%p1 bra 	BB26_5;

	setp.gt.s16	%p2, %rs1, 36;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB26_2;
	bra.uni 	BB26_4;

BB26_2:
	mov.f32 	%f88, %f87;

BB26_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB26_3;

BB26_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB26_12;

BB26_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB26_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB26_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB26_7;

BB26_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB26_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.gt.s16	%p15, %rs1, -43;
	@%p15 bra 	BB26_12;

	cvt.rzi.s32.f32	%r1, %f1;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32	%p16, %r2, 1;
	@%p16 bra 	BB26_12;

	mov.f32 	%f98, 0f80000000;

BB26_12:
	cvt.rzi.s32.f32	%r3, %f98;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___tgammah(
	.param .b32 ___tgammah_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<45>;


	ld.param.u8 	%rs1, [___tgammah_param_0];
	cvt.rn.f32.u16	%f12, %rs1;
	setp.gt.u16	%p1, %rs1, 36;
	selp.f32	%f1, 0f42100000, %f12, %p1;
	setp.gt.f32	%p2, %f1, 0f42081EB8;
	add.f32 	%f2, %f1, 0fBF800000;
	selp.f32	%f42, %f2, %f1, %p2;
	setp.gt.f32	%p3, %f42, 0f3FC00000;
	add.f32 	%f43, %f42, 0fBF800000;
	mov.f32 	%f38, 0f3F800000;
	@%p3 bra 	BB27_1;
	bra.uni 	BB27_3;

BB27_1:
	mov.f32 	%f44, %f43;

BB27_2:
	mov.f32 	%f42, %f44;
	mul.f32 	%f38, %f38, %f42;
	add.f32 	%f44, %f42, 0fBF800000;
	setp.gt.f32	%p4, %f42, 0f3FC00000;
	mov.f32 	%f43, %f44;
	@%p4 bra 	BB27_2;

BB27_3:
	setp.ltu.f32	%p5, %f1, 0f3F000000;
	selp.f32	%f15, %f42, %f43, %p5;
	mov.f32 	%f16, 0f3BE86AA4;
	mov.f32 	%f17, 0fBA8AA19E;
	fma.rn.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0fBC1E2998;
	fma.rn.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBD2CBE4A;
	fma.rn.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f3E2A8A17;
	fma.rn.f32 	%f24, %f22, %f15, %f23;
	mov.f32 	%f25, 0fBD2C0CBB;
	fma.rn.f32 	%f26, %f24, %f15, %f25;
	mov.f32 	%f27, 0fBF27E7A3;
	fma.rn.f32 	%f28, %f26, %f15, %f27;
	mov.f32 	%f29, 0f3F13C468;
	fma.rn.f32 	%f30, %f28, %f15, %f29;
	mov.f32 	%f31, 0f3F800000;
	fma.rn.f32 	%f32, %f30, %f15, %f31;
	mul.f32 	%f33, %f32, %f1;
	setp.lt.f32	%p6, %f1, 0f3F000000;
	selp.f32	%f34, %f33, %f32, %p6;
	div.approx.f32 	%f35, %f38, %f34;
	mul.f32 	%f36, %f35, %f2;
	selp.f32	%f37, %f36, %f35, %p2;
	cvt.rzi.u32.f32	%r1, %f37;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___tgammad(
	.param .b64 ___tgammad_param_0
)
{
	.reg .pred 	%p<30>;
	.reg .s32 	%r<72>;
	.reg .f32 	%f<3>;
	.reg .s64 	%rd<3>;
	.reg .f64 	%fd<429>;


	ld.param.f64 	%fd52, [___tgammad_param_0];
	setp.ltu.f64	%p1, %fd52, 0d0000000000000000;
	@%p1 bra 	BB28_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd52;
	}
	setp.lt.s32	%p2, %r67, 1076756480;
	@%p2 bra 	BB28_17;

	setp.lt.f64	%p3, %fd52, 0d406573FAE561F648;
	@%p3 bra 	BB28_4;

	mov.f64 	%fd428, 0d7FF0000000000000;
	bra.uni 	BB28_45;

BB28_4:
	// inline asm
	rcp.approx.ftz.f64 %fd54,%fd52;
	// inline asm
	neg.f64 	%fd56, %fd52;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd54, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd54, %fd54;
	mov.f64 	%fd61, 0d3F4B8239C670E690;
	mov.f64 	%fd62, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0dBF0B1D75D3346711;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0dBF436773BDB97B48;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3F1247604839C038;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0d3F49B0FF6874F2C4;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0dBF2E13CE465FA859;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0dBF65F7268EDAB4C8;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3F6C71C71C71C71C;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	mov.f64 	%fd78, 0d3FB5555555555555;
	fma.rn.f64 	%fd79, %fd77, %fd60, %fd78;
	fma.rn.f64 	%fd1, %fd79, %fd60, %fd57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd52;
	}
	shr.u32 	%r68, %r67, 20;
	setp.ne.s32	%p4, %r68, 0;
	@%p4 bra 	BB28_6;

	mul.f64 	%fd80, %fd52, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd80;
	}
	shr.u32 	%r24, %r67, 20;
	add.s32 	%r68, %r24, -54;

BB28_6:
	add.s32 	%r69, %r68, -1023;
	and.b32  	%r25, %r67, -2146435073;
	or.b32  	%r26, %r25, 1072693248;
	mov.b64 	%fd400, {%r66, %r26};
	setp.lt.u32	%p5, %r26, 1073127583;
	@%p5 bra 	BB28_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd400;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd400;
	}
	add.s32 	%r29, %r28, -1048576;
	mov.b64 	%fd400, {%r27, %r29};
	add.s32 	%r69, %r68, -1022;

BB28_8:
	add.f64 	%fd83, %fd52, 0dBFE0000000000000;
	add.f64 	%fd82, %fd400, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd81,%fd82;
	// inline asm
	neg.f64 	%fd85, %fd82;
	fma.rn.f64 	%fd86, %fd85, %fd81, %fd57;
	fma.rn.f64 	%fd87, %fd86, %fd86, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd81, %fd81;
	add.f64 	%fd89, %fd400, 0dBFF0000000000000;
	mul.f64 	%fd90, %fd89, %fd88;
	fma.rn.f64 	%fd91, %fd89, %fd88, %fd90;
	mul.f64 	%fd92, %fd91, %fd91;
	mov.f64 	%fd93, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd94, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F6249249242B910;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F89999999999DFB;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	sub.f64 	%fd106, %fd89, %fd91;
	add.f64 	%fd107, %fd106, %fd106;
	neg.f64 	%fd108, %fd91;
	fma.rn.f64 	%fd109, %fd108, %fd89, %fd107;
	mul.f64 	%fd110, %fd88, %fd109;
	fma.rn.f64 	%fd111, %fd105, %fd92, 0d3FB5555555555555;
	sub.f64 	%fd113, %fd78, %fd111;
	fma.rn.f64 	%fd114, %fd105, %fd92, %fd113;
	add.f64 	%fd115, %fd114, 0d0000000000000000;
	add.f64 	%fd116, %fd115, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd117, %fd111, %fd116;
	sub.f64 	%fd118, %fd111, %fd117;
	add.f64 	%fd119, %fd118, %fd116;
	mul.rn.f64 	%fd120, %fd91, %fd91;
	neg.f64 	%fd121, %fd120;
	fma.rn.f64 	%fd122, %fd91, %fd91, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd110;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd110;
	}
	add.s32 	%r32, %r31, 1048576;
	mov.b64 	%fd123, {%r30, %r32};
	fma.rn.f64 	%fd124, %fd91, %fd123, %fd122;
	mul.rn.f64 	%fd125, %fd120, %fd91;
	neg.f64 	%fd126, %fd125;
	fma.rn.f64 	%fd127, %fd120, %fd91, %fd126;
	fma.rn.f64 	%fd128, %fd120, %fd110, %fd127;
	fma.rn.f64 	%fd129, %fd124, %fd91, %fd128;
	mul.rn.f64 	%fd130, %fd117, %fd125;
	neg.f64 	%fd131, %fd130;
	fma.rn.f64 	%fd132, %fd117, %fd125, %fd131;
	fma.rn.f64 	%fd133, %fd117, %fd129, %fd132;
	fma.rn.f64 	%fd134, %fd119, %fd125, %fd133;
	add.f64 	%fd135, %fd130, %fd134;
	sub.f64 	%fd136, %fd130, %fd135;
	add.f64 	%fd137, %fd136, %fd134;
	add.f64 	%fd138, %fd91, %fd135;
	sub.f64 	%fd139, %fd91, %fd138;
	add.f64 	%fd140, %fd139, %fd135;
	add.f64 	%fd141, %fd140, %fd137;
	add.f64 	%fd142, %fd141, %fd110;
	add.f64 	%fd143, %fd138, %fd142;
	sub.f64 	%fd144, %fd138, %fd143;
	add.f64 	%fd145, %fd144, %fd142;
	xor.b32  	%r33, %r69, -2147483648;
	mov.u32 	%r34, -2147483648;
	mov.u32 	%r35, 1127219200;
	mov.b64 	%fd146, {%r33, %r35};
	mov.b64 	%fd147, {%r34, %r35};
	sub.f64 	%fd148, %fd146, %fd147;
	mov.f64 	%fd149, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd150, %fd148, %fd149, %fd143;
	neg.f64 	%fd151, %fd148;
	fma.rn.f64 	%fd152, %fd151, %fd149, %fd150;
	sub.f64 	%fd153, %fd152, %fd143;
	sub.f64 	%fd154, %fd145, %fd153;
	mov.f64 	%fd155, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd156, %fd148, %fd155, %fd154;
	add.f64 	%fd157, %fd150, %fd156;
	sub.f64 	%fd158, %fd150, %fd157;
	add.f64 	%fd159, %fd158, %fd156;
	mul.rn.f64 	%fd160, %fd157, %fd83;
	neg.f64 	%fd161, %fd160;
	fma.rn.f64 	%fd162, %fd157, %fd83, %fd161;
	fma.rn.f64 	%fd163, %fd159, %fd83, %fd162;
	add.f64 	%fd164, %fd160, %fd163;
	sub.f64 	%fd165, %fd160, %fd164;
	add.f64 	%fd166, %fd165, %fd163;
	sub.f64 	%fd167, %fd164, %fd52;
	sub.f64 	%fd168, %fd164, %fd167;
	sub.f64 	%fd169, %fd168, %fd52;
	add.f64 	%fd170, %fd169, 0d0000000000000000;
	add.f64 	%fd171, %fd170, %fd166;
	add.f64 	%fd5, %fd167, %fd171;
	sub.f64 	%fd172, %fd167, %fd5;
	add.f64 	%fd6, %fd172, %fd171;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd5;
	}
	mov.b32 	 %f1, %r13;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p6, %f2, 0f40874911;
	@%p6 bra 	BB28_10;

	setp.lt.s32	%p7, %r13, 0;
	selp.f64	%fd173, 0d0000000000000000, 0d7FF0000000000000, %p7;
	abs.f64 	%fd174, %fd5;
	setp.gtu.f64	%p8, %fd174, 0d7FF0000000000000;
	add.f64 	%fd175, %fd5, %fd5;
	selp.f64	%fd402, %fd175, %fd173, %p8;
	bra.uni 	BB28_14;

BB28_10:
	mov.f64 	%fd399, 0d3FF0000000000000;
	mov.f64 	%fd176, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd177, %fd5, %fd176;
	mov.f64 	%fd178, 0d4338000000000000;
	add.rn.f64 	%fd179, %fd177, %fd178;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd179;
	}
	mov.f64 	%fd180, 0dC338000000000000;
	add.rn.f64 	%fd181, %fd179, %fd180;
	mov.f64 	%fd182, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd183, %fd181, %fd182, %fd5;
	mov.f64 	%fd184, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd185, %fd181, %fd184, %fd183;
	mov.f64 	%fd186, 0d3E928AF3FCA213EA;
	mov.f64 	%fd187, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0d3F81111111122322;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0d3FA55555555502A1;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	mov.f64 	%fd201, 0d3FC5555555555511;
	fma.rn.f64 	%fd202, %fd200, %fd185, %fd201;
	mov.f64 	%fd203, 0d3FE000000000000B;
	fma.rn.f64 	%fd204, %fd202, %fd185, %fd203;
	fma.rn.f64 	%fd206, %fd204, %fd185, %fd399;
	fma.rn.f64 	%fd401, %fd206, %fd185, %fd399;
	abs.s32 	%r36, %r14;
	setp.lt.s32	%p9, %r36, 1023;
	@%p9 bra 	BB28_12;

	add.s32 	%r37, %r14, 2046;
	shl.b32 	%r38, %r37, 19;
	and.b32  	%r39, %r38, -1048576;
	shl.b32 	%r40, %r37, 20;
	sub.s32 	%r70, %r40, %r39;
	mov.u32 	%r41, 0;
	mov.b64 	%fd207, {%r41, %r39};
	mul.f64 	%fd401, %fd401, %fd207;
	bra.uni 	BB28_13;

BB28_12:
	shl.b32 	%r42, %r14, 20;
	add.s32 	%r70, %r42, 1072693248;

BB28_13:
	mov.u32 	%r43, 0;
	mov.b64 	%fd208, {%r43, %r70};
	mul.f64 	%fd402, %fd401, %fd208;

BB28_14:
	abs.f64 	%fd209, %fd402;
	setp.eq.f64	%p10, %fd209, 0d7FF0000000000000;
	@%p10 bra 	BB28_16;

	fma.rn.f64 	%fd402, %fd402, %fd6, %fd402;

BB28_16:
	mul.f64 	%fd210, %fd402, 0dBCAA6A0D6F814637;
	mov.f64 	%fd211, 0d40040D931FF62706;
	fma.rn.f64 	%fd212, %fd402, %fd211, %fd210;
	mul.f64 	%fd428, %fd212, %fd1;
	bra.uni 	BB28_45;

BB28_17:
	setp.gt.s32	%p11, %r67, 1073217535;
	add.f64 	%fd425, %fd52, 0dBFF0000000000000;
	mov.f64 	%fd403, 0d3FF0000000000000;
	@%p11 bra 	BB28_19;

	mov.f64 	%fd424, %fd52;
	bra.uni 	BB28_21;

BB28_19:
	mov.f64 	%fd427, %fd425;
	mov.f64 	%fd426, %fd52;

BB28_20:
	mov.f64 	%fd19, %fd426;
	mov.f64 	%fd426, %fd427;
	neg.f64 	%fd215, %fd403;
	fma.rn.f64 	%fd403, %fd403, %fd19, %fd215;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd426;
	}
	setp.gt.s32	%p12, %r44, 1073217535;
	add.f64 	%fd427, %fd426, 0dBFF0000000000000;
	mov.f64 	%fd425, %fd427;
	mov.f64 	%fd424, %fd426;
	@%p12 bra 	BB28_20;

BB28_21:
	setp.gt.s32	%p13, %r67, 1071644671;
	selp.f64	%fd216, %fd425, %fd424, %p13;
	mov.f64 	%fd217, 0dBE8B338C457183B6;
	mov.f64 	%fd218, 0dBDFE6BDF8CC487CD;
	fma.rn.f64 	%fd219, %fd218, %fd216, %fd217;
	mov.f64 	%fd220, 0d3EB31831766A0388;
	fma.rn.f64 	%fd221, %fd219, %fd216, %fd220;
	mov.f64 	%fd222, 0dBEB4FC07FC9F1563;
	fma.rn.f64 	%fd223, %fd221, %fd216, %fd222;
	mov.f64 	%fd224, 0dBEF51D59DCE6A679;
	fma.rn.f64 	%fd225, %fd223, %fd216, %fd224;
	mov.f64 	%fd226, 0d3F20C8A6351CB1F9;
	fma.rn.f64 	%fd227, %fd225, %fd216, %fd226;
	mov.f64 	%fd228, 0dBF2C364D9E00D4CA;
	fma.rn.f64 	%fd229, %fd227, %fd216, %fd228;
	mov.f64 	%fd230, 0dBF5317112046830B;
	fma.rn.f64 	%fd231, %fd229, %fd216, %fd230;
	mov.f64 	%fd232, 0d3F7D919C50FF9416;
	fma.rn.f64 	%fd233, %fd231, %fd216, %fd232;
	mov.f64 	%fd234, 0dBF83B4AF28728BB0;
	fma.rn.f64 	%fd235, %fd233, %fd216, %fd234;
	mov.f64 	%fd236, 0dBFA59AF103C171DC;
	fma.rn.f64 	%fd237, %fd235, %fd216, %fd236;
	mov.f64 	%fd238, 0d3FC5512320B45D97;
	fma.rn.f64 	%fd239, %fd237, %fd216, %fd238;
	mov.f64 	%fd240, 0dBFA5815E8FA27607;
	fma.rn.f64 	%fd241, %fd239, %fd216, %fd240;
	mov.f64 	%fd242, 0dBFE4FCF4026AFA4B;
	fma.rn.f64 	%fd243, %fd241, %fd216, %fd242;
	mov.f64 	%fd244, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd245, %fd243, %fd216, %fd244;
	mov.f64 	%fd246, 0d3FF0000000000000;
	fma.rn.f64 	%fd247, %fd245, %fd216, %fd246;
	mul.f64 	%fd248, %fd247, %fd52;
	setp.lt.s32	%p14, %r67, 1071644672;
	selp.f64	%fd249, %fd248, %fd247, %p14;
	div.rn.f64 	%fd428, %fd403, %fd249;
	bra.uni 	BB28_45;

BB28_22:
	setp.lt.f64	%p15, %fd52, 0d0000000000000000;
	@%p15 bra 	BB28_24;

	add.f64 	%fd428, %fd52, %fd52;
	bra.uni 	BB28_45;

BB28_24:
	cvt.rzi.f64.f64	%fd250, %fd52;
	setp.neu.f64	%p16, %fd250, %fd52;
	@%p16 bra 	BB28_26;

	mov.f64 	%fd428, 0dFFF8000000000000;
	bra.uni 	BB28_45;

BB28_26:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd52;
	}
	setp.lt.u32	%p17, %r18, -1070727168;
	@%p17 bra 	BB28_41;

	setp.lt.u32	%p18, %r18, -1066983424;
	@%p18 bra 	BB28_29;

	cvt.rmi.f64.f64	%fd251, %fd52;
	mul.f64 	%fd252, %fd251, 0d3FE0000000000000;
	cvt.rmi.f64.f64	%fd253, %fd252;
	fma.rn.f64 	%fd254, %fd253, 0dC000000000000000, %fd251;
	setp.eq.f64	%p19, %fd254, 0d3FF0000000000000;
	selp.f64	%fd428, 0d8000000000000000, 0d0000000000000000, %p19;
	bra.uni 	BB28_45;

BB28_29:
	add.s32 	%r45, %r18, %r18;
	setp.lt.u32	%p20, %r45, -2038431743;
	mov.f64 	%fd423, %fd52;
	@%p20 bra 	BB28_31;

	mov.f64 	%fd255, 0d0000000000000000;
	mul.rn.f64 	%fd28, %fd52, %fd255;
	mov.f64 	%fd423, %fd28;

BB28_31:
	mov.f64 	%fd29, %fd423;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd29;
	}
	add.s32 	%r47, %r46, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd29;
	}
	mov.b64 	%fd256, {%r48, %r47};
	cvt.rni.f64.f64	%fd257, %fd256;
	cvt.rzi.s64.f64	%rd1, %fd257;
	cvt.u32.u64	%r49, %rd1;
	neg.f64 	%fd258, %fd257;
	mov.f64 	%fd259, 0d3FE0000000000000;
	fma.rn.f64 	%fd260, %fd258, %fd259, %fd29;
	mul.f64 	%fd261, %fd260, 0d3CA1A62633145C07;
	mov.f64 	%fd262, 0d400921FB54442D18;
	fma.rn.f64 	%fd263, %fd260, %fd262, %fd261;
	mul.rn.f64 	%fd264, %fd263, %fd263;
	mov.f64 	%fd265, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd266, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd267, %fd266, %fd264, %fd265;
	mov.f64 	%fd268, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd269, %fd267, %fd264, %fd268;
	mov.f64 	%fd270, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd271, %fd269, %fd264, %fd270;
	mov.f64 	%fd272, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd273, %fd271, %fd264, %fd272;
	mov.f64 	%fd274, 0d3FA5555555555551;
	fma.rn.f64 	%fd275, %fd273, %fd264, %fd274;
	mov.f64 	%fd276, 0dBFE0000000000000;
	fma.rn.f64 	%fd277, %fd275, %fd264, %fd276;
	mov.f64 	%fd278, 0d3FF0000000000000;
	fma.rn.f64 	%fd279, %fd277, %fd264, %fd278;
	mov.f64 	%fd280, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd281, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd282, %fd281, %fd264, %fd280;
	mov.f64 	%fd283, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd284, %fd282, %fd264, %fd283;
	mov.f64 	%fd285, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd286, %fd284, %fd264, %fd285;
	mov.f64 	%fd287, 0d3F81111111110818;
	fma.rn.f64 	%fd288, %fd286, %fd264, %fd287;
	mov.f64 	%fd289, 0dBFC5555555555554;
	fma.rn.f64 	%fd290, %fd288, %fd264, %fd289;
	mov.f64 	%fd291, 0d0000000000000000;
	fma.rn.f64 	%fd292, %fd290, %fd264, %fd291;
	fma.rn.f64 	%fd293, %fd292, %fd263, %fd263;
	and.b64  	%rd2, %rd1, 1;
	setp.eq.b64	%p21, %rd2, 1;
	not.pred 	%p22, %p21;
	selp.f64	%fd404, %fd293, %fd279, %p22;
	and.b32  	%r50, %r49, 2;
	setp.eq.s32	%p23, %r50, 0;
	@%p23 bra 	BB28_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd404;
	}
	xor.b32  	%r52, %r51, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd404;
	}
	mov.b64 	%fd404, {%r53, %r52};

BB28_33:
	cvt.rzi.f64.f64	%fd294, %fd29;
	setp.neu.f64	%p24, %fd29, %fd294;
	@%p24 bra 	BB28_35;

	mul.rn.f64 	%fd404, %fd29, %fd291;

BB28_35:
	mov.f64 	%fd296, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd297, %fd52, %fd296;
	mov.f64 	%fd298, 0d4338000000000000;
	add.rn.f64 	%fd299, %fd297, %fd298;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd299;
	}
	mov.f64 	%fd300, 0dC338000000000000;
	add.rn.f64 	%fd301, %fd299, %fd300;
	mov.f64 	%fd302, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd303, %fd301, %fd302, %fd52;
	mov.f64 	%fd304, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd305, %fd301, %fd304, %fd303;
	mov.f64 	%fd306, 0d3E928AF3FCA213EA;
	mov.f64 	%fd307, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd308, %fd307, %fd305, %fd306;
	mov.f64 	%fd309, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd310, %fd308, %fd305, %fd309;
	mov.f64 	%fd311, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd312, %fd310, %fd305, %fd311;
	mov.f64 	%fd313, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd314, %fd312, %fd305, %fd313;
	mov.f64 	%fd315, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd316, %fd314, %fd305, %fd315;
	mov.f64 	%fd317, 0d3F81111111122322;
	fma.rn.f64 	%fd318, %fd316, %fd305, %fd317;
	mov.f64 	%fd319, 0d3FA55555555502A1;
	fma.rn.f64 	%fd320, %fd318, %fd305, %fd319;
	mov.f64 	%fd321, 0d3FC5555555555511;
	fma.rn.f64 	%fd322, %fd320, %fd305, %fd321;
	mov.f64 	%fd323, 0d3FE000000000000B;
	fma.rn.f64 	%fd324, %fd322, %fd305, %fd323;
	fma.rn.f64 	%fd326, %fd324, %fd305, %fd278;
	fma.rn.f64 	%fd405, %fd326, %fd305, %fd278;
	abs.s32 	%r54, %r19;
	setp.lt.s32	%p25, %r54, 1023;
	@%p25 bra 	BB28_37;

	add.s32 	%r55, %r19, 2046;
	shl.b32 	%r56, %r55, 19;
	and.b32  	%r57, %r56, -1048576;
	shl.b32 	%r58, %r55, 20;
	sub.s32 	%r71, %r58, %r57;
	mov.u32 	%r59, 0;
	mov.b64 	%fd327, {%r59, %r57};
	mul.f64 	%fd405, %fd405, %fd327;
	bra.uni 	BB28_38;

BB28_37:
	shl.b32 	%r60, %r19, 20;
	add.s32 	%r71, %r60, 1072693248;

BB28_38:
	mov.u32 	%r61, 0;
	mov.b64 	%fd328, {%r61, %r71};
	mul.f64 	%fd38, %fd405, %fd328;
	abs.f64 	%fd39, %fd52;
	add.f64 	%fd406, %fd39, 0dBFE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd39;
	}
	setp.lt.s32	%p26, %r23, 1080131584;
	@%p26 bra 	BB28_40;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r62, %temp}, %fd406;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r63}, %fd406;
	}
	add.s32 	%r64, %r63, -1048576;
	mov.b64 	%fd406, {%r62, %r64};

BB28_40:
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd406;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd331, [retval0+0];
	}
	// Callseq End 0
	mul.f64 	%fd332, %fd38, %fd331;
	setp.gt.s32	%p27, %r23, 1080131583;
	selp.f64	%fd333, %fd332, %fd38, %p27;
	// inline asm
	rcp.approx.ftz.f64 %fd329,%fd39;
	// inline asm
	neg.f64 	%fd334, %fd39;
	fma.rn.f64 	%fd336, %fd334, %fd329, %fd278;
	fma.rn.f64 	%fd337, %fd336, %fd336, %fd336;
	fma.rn.f64 	%fd338, %fd337, %fd329, %fd329;
	mov.f64 	%fd339, 0d3F4B8239C670E690;
	fma.rn.f64 	%fd341, %fd291, %fd338, %fd339;
	mov.f64 	%fd342, 0dBF0B1D75D3346711;
	fma.rn.f64 	%fd343, %fd341, %fd338, %fd342;
	mov.f64 	%fd344, 0dBF436773BDB97B48;
	fma.rn.f64 	%fd345, %fd343, %fd338, %fd344;
	mov.f64 	%fd346, 0d3F1247604839C038;
	fma.rn.f64 	%fd347, %fd345, %fd338, %fd346;
	mov.f64 	%fd348, 0d3F49B0FF6874F2C4;
	fma.rn.f64 	%fd349, %fd347, %fd338, %fd348;
	mov.f64 	%fd350, 0dBF2E13CE465FA859;
	fma.rn.f64 	%fd351, %fd349, %fd338, %fd350;
	mov.f64 	%fd352, 0dBF65F7268EDAB4C8;
	fma.rn.f64 	%fd353, %fd351, %fd338, %fd352;
	mov.f64 	%fd354, 0d3F6C71C71C71C71C;
	fma.rn.f64 	%fd355, %fd353, %fd338, %fd354;
	mov.f64 	%fd356, 0d3FB5555555555555;
	fma.rn.f64 	%fd357, %fd355, %fd338, %fd356;
	fma.rn.f64 	%fd358, %fd357, %fd338, %fd278;
	mul.f64 	%fd359, %fd333, %fd358;
	mul.f64 	%fd360, %fd359, %fd39;
	mul.f64 	%fd361, %fd360, %fd404;
	rcp.rn.f64 	%fd362, %fd361;
	mul.f64 	%fd363, %fd362, 0dBC9A6A0D6F814637;
	mov.f64 	%fd364, 0d3FF40D931FF62706;
	fma.rn.f64 	%fd365, %fd362, %fd364, %fd363;
	div.rn.f64 	%fd428, %fd365, %fd331;
	bra.uni 	BB28_45;

BB28_41:
	setp.lt.u32	%p28, %r18, -1075838976;
	mov.f64 	%fd419, %fd52;
	mov.f64 	%fd420, %fd52;
	@%p28 bra 	BB28_44;

	mov.f64 	%fd421, %fd52;
	mov.f64 	%fd422, %fd52;

BB28_43:
	fma.rn.f64 	%fd422, %fd422, %fd421, %fd422;
	add.f64 	%fd421, %fd421, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd421;
	}
	setp.gt.u32	%p29, %r65, -1075838977;
	mov.f64 	%fd420, %fd422;
	mov.f64 	%fd419, %fd421;
	@%p29 bra 	BB28_43;

BB28_44:
	mov.f64 	%fd366, 0dBE8B338C457183B6;
	mov.f64 	%fd367, 0dBDFE6BDF8CC487CD;
	fma.rn.f64 	%fd368, %fd367, %fd419, %fd366;
	mov.f64 	%fd369, 0d3EB31831766A0388;
	fma.rn.f64 	%fd370, %fd368, %fd419, %fd369;
	mov.f64 	%fd371, 0dBEB4FC07FC9F1563;
	fma.rn.f64 	%fd372, %fd370, %fd419, %fd371;
	mov.f64 	%fd373, 0dBEF51D59DCE6A679;
	fma.rn.f64 	%fd374, %fd372, %fd419, %fd373;
	mov.f64 	%fd375, 0d3F20C8A6351CB1F9;
	fma.rn.f64 	%fd376, %fd374, %fd419, %fd375;
	mov.f64 	%fd377, 0dBF2C364D9E00D4CA;
	fma.rn.f64 	%fd378, %fd376, %fd419, %fd377;
	mov.f64 	%fd379, 0dBF5317112046830B;
	fma.rn.f64 	%fd380, %fd378, %fd419, %fd379;
	mov.f64 	%fd381, 0d3F7D919C50FF9416;
	fma.rn.f64 	%fd382, %fd380, %fd419, %fd381;
	mov.f64 	%fd383, 0dBF83B4AF28728BB0;
	fma.rn.f64 	%fd384, %fd382, %fd419, %fd383;
	mov.f64 	%fd385, 0dBFA59AF103C171DC;
	fma.rn.f64 	%fd386, %fd384, %fd419, %fd385;
	mov.f64 	%fd387, 0d3FC5512320B45D97;
	fma.rn.f64 	%fd388, %fd386, %fd419, %fd387;
	mov.f64 	%fd389, 0dBFA5815E8FA27607;
	fma.rn.f64 	%fd390, %fd388, %fd419, %fd389;
	mov.f64 	%fd391, 0dBFE4FCF4026AFA4B;
	fma.rn.f64 	%fd392, %fd390, %fd419, %fd391;
	mov.f64 	%fd393, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd394, %fd392, %fd419, %fd393;
	mov.f64 	%fd395, 0d3FF0000000000000;
	fma.rn.f64 	%fd396, %fd394, %fd419, %fd395;
	mul.f64 	%fd397, %fd420, %fd396;
	rcp.rn.f64 	%fd428, %fd397;

BB28_45:
	st.param.f64	[func_retval0+0], %fd428;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___lgammaf(
	.param .b32 ___lgammaf_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<269>;


	ld.param.f32 	%f39, [___lgammaf_param_0];
	abs.f32 	%f1, %f39;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB29_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB29_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f1;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB29_4;

	lg2.approx.f32 	%f261, %f1;
	bra.uni 	BB29_5;

BB29_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f51, %f1, 0f4B800000;
	selp.f32	%f52, %f51, %f1, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r3, %f52;
	and.b32  	%r4, %r3, 8388607;
	or.b32  	%r5, %r4, 1065353216;
	mov.b32 	 %f54, %r5;
	shr.u32 	%r6, %r3, 23;
	cvt.rn.f32.u32	%f55, %r6;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB29_5:
	add.f32 	%f76, %f1, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f1;
	add.rn.f32 	%f80, %f78, %f2;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f268, %f1, %f81, %p8;
	bra.uni 	BB29_15;

BB29_6:
	add.f32 	%f84, %f1, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB29_15;

BB29_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB29_9;

	add.f32 	%f100, %f1, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB29_15;

BB29_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB29_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f1;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB29_15;

BB29_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f1, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f1, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f1, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f1, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f1, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f1, %f154;
	mul.f32 	%f156, %f155, %f1;
	fma.rn.f32 	%f10, %f156, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB29_13;

	lg2.approx.f32 	%f262, %f10;
	bra.uni 	BB29_14;

BB29_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f159, %f10, 0f4B800000;
	selp.f32	%f160, %f159, %f10, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r7, %f160;
	and.b32  	%r8, %r7, 8388607;
	or.b32  	%r9, %r8, 1065353216;
	mov.b32 	 %f162, %r9;
	shr.u32 	%r10, %r7, 23;
	cvt.rn.f32.u32	%f163, %r10;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB29_14:
	neg.f32 	%f268, %f262;

BB29_15:
	setp.ge.f32	%p16, %f39, 0f00000000;
	@%p16 bra 	BB29_37;

	cvt.rmi.f32.f32	%f184, %f1;
	setp.neu.f32	%p17, %f1, %f184;
	@%p17 bra 	BB29_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB29_37;

BB29_18:
	setp.lt.f32	%p18, %f1, 0f1FEC1E4A;
	@%p18 bra 	BB29_33;

	add.f32 	%f185, %f1, %f1;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f1;
	mul.f32 	%f16, %f189, 0f40490FDB;
	mul.rn.f32 	%f17, %f16, %f16;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB29_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f17, %f190;
	bra.uni 	BB29_22;

BB29_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f17, %f192;

BB29_22:
	@%p19 bra 	BB29_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f17, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f17, %f196;
	bra.uni 	BB29_25;

BB29_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f17, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f17, %f199;

BB29_25:
	fma.rn.f32 	%f265, %f264, %f16, %f16;
	@%p19 bra 	BB29_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f17, %f200;

BB29_27:
	and.b32  	%r11, %r1, 2;
	setp.eq.s32	%p22, %r11, 0;
	@%p22 bra 	BB29_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB29_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f29, %f203, %f1;
	setp.gt.f32	%p23, %f29, 0f00000000;
	setp.lt.f32	%p24, %f29, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB29_31;

	lg2.approx.f32 	%f266, %f29;
	bra.uni 	BB29_32;

BB29_31:
	setp.lt.f32	%p26, %f29, 0f00800000;
	mul.f32 	%f206, %f29, 0f4B800000;
	selp.f32	%f207, %f206, %f29, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r12, %f207;
	and.b32  	%r13, %r12, 8388607;
	or.b32  	%r14, %r13, 1065353216;
	mov.b32 	 %f209, %r14;
	shr.u32 	%r15, %r12, 23;
	cvt.rn.f32.u32	%f210, %r15;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB29_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB29_37;

BB29_33:
	setp.gt.f32	%p28, %f1, 0f00000000;
	setp.lt.f32	%p29, %f1, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB29_35;

	lg2.approx.f32 	%f267, %f1;
	bra.uni 	BB29_36;

BB29_35:
	setp.lt.f32	%p31, %f1, 0f00800000;
	mul.f32 	%f235, %f1, 0f4B800000;
	selp.f32	%f236, %f235, %f1, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r16, %f236;
	and.b32  	%r17, %r16, 8388607;
	or.b32  	%r18, %r17, 1065353216;
	mov.b32 	 %f238, %r18;
	shr.u32 	%r19, %r16, 23;
	cvt.rn.f32.u32	%f239, %r19;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB29_36:
	neg.f32 	%f268, %f267;

BB29_37:
	st.param.f32	[func_retval0+0], %f268;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___lgammai(
	.param .b32 ___lgammai_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.u32 	%r3, [___lgammai_param_0];
	cvt.rn.f32.s32	%f1, %r3;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p1, %f2, 0f40400000;
	@%p1 bra 	BB30_7;

	setp.ltu.f32	%p2, %f2, 0f40F9999A;
	@%p2 bra 	BB30_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f2;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f3, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f2, 0f7F800000;
	setp.gt.f32	%p4, %f2, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB30_4;

	lg2.approx.f32 	%f261, %f2;
	bra.uni 	BB30_5;

BB30_4:
	setp.lt.f32	%p6, %f2, 0f00800000;
	mul.f32 	%f51, %f2, 0f4B800000;
	selp.f32	%f52, %f51, %f2, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r4, %f52;
	and.b32  	%r5, %r4, 8388607;
	or.b32  	%r6, %r5, 1065353216;
	mov.b32 	 %f54, %r6;
	shr.u32 	%r7, %r4, 23;
	cvt.rn.f32.u32	%f55, %r7;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB30_5:
	add.f32 	%f76, %f2, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f2;
	add.rn.f32 	%f80, %f78, %f3;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f2, 0f7F800000;
	selp.f32	%f268, %f2, %f81, %p8;
	bra.uni 	BB30_15;

BB30_6:
	add.f32 	%f84, %f2, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB30_15;

BB30_7:
	setp.ltu.f32	%p9, %f2, 0f3FC00000;
	@%p9 bra 	BB30_9;

	add.f32 	%f100, %f2, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB30_15;

BB30_9:
	setp.ltu.f32	%p10, %f2, 0f3F333333;
	@%p10 bra 	BB30_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f2;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB30_15;

BB30_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f2, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f2, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f2, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f2, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f2, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f2, %f154;
	mul.f32 	%f156, %f155, %f2;
	fma.rn.f32 	%f11, %f156, %f2, %f2;
	setp.gt.f32	%p11, %f11, 0f00000000;
	setp.lt.f32	%p12, %f11, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB30_13;

	lg2.approx.f32 	%f262, %f11;
	bra.uni 	BB30_14;

BB30_13:
	setp.lt.f32	%p14, %f11, 0f00800000;
	mul.f32 	%f159, %f11, 0f4B800000;
	selp.f32	%f160, %f159, %f11, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r8, %f160;
	and.b32  	%r9, %r8, 8388607;
	or.b32  	%r10, %r9, 1065353216;
	mov.b32 	 %f162, %r10;
	shr.u32 	%r11, %r8, 23;
	cvt.rn.f32.u32	%f163, %r11;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB30_14:
	neg.f32 	%f268, %f262;

BB30_15:
	setp.ge.f32	%p16, %f1, 0f00000000;
	@%p16 bra 	BB30_37;

	cvt.rmi.f32.f32	%f184, %f2;
	setp.neu.f32	%p17, %f2, %f184;
	@%p17 bra 	BB30_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB30_37;

BB30_18:
	setp.lt.f32	%p18, %f2, 0f1FEC1E4A;
	@%p18 bra 	BB30_33;

	add.f32 	%f185, %f2, %f2;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f2;
	mul.f32 	%f17, %f189, 0f40490FDB;
	mul.rn.f32 	%f18, %f17, %f17;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB30_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f18, %f190;
	bra.uni 	BB30_22;

BB30_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f18, %f192;

BB30_22:
	@%p19 bra 	BB30_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f18, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f18, %f196;
	bra.uni 	BB30_25;

BB30_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f18, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f18, %f199;

BB30_25:
	fma.rn.f32 	%f265, %f264, %f17, %f17;
	@%p19 bra 	BB30_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f18, %f200;

BB30_27:
	and.b32  	%r12, %r1, 2;
	setp.eq.s32	%p22, %r12, 0;
	@%p22 bra 	BB30_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB30_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f30, %f203, %f2;
	setp.gt.f32	%p23, %f30, 0f00000000;
	setp.lt.f32	%p24, %f30, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB30_31;

	lg2.approx.f32 	%f266, %f30;
	bra.uni 	BB30_32;

BB30_31:
	setp.lt.f32	%p26, %f30, 0f00800000;
	mul.f32 	%f206, %f30, 0f4B800000;
	selp.f32	%f207, %f206, %f30, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r13, %f207;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	 %f209, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32	%f210, %r16;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB30_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB30_37;

BB30_33:
	setp.gt.f32	%p28, %f2, 0f00000000;
	setp.lt.f32	%p29, %f2, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB30_35;

	lg2.approx.f32 	%f267, %f2;
	bra.uni 	BB30_36;

BB30_35:
	setp.lt.f32	%p31, %f2, 0f00800000;
	mul.f32 	%f235, %f2, 0f4B800000;
	selp.f32	%f236, %f235, %f2, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r17, %f236;
	and.b32  	%r18, %r17, 8388607;
	or.b32  	%r19, %r18, 1065353216;
	mov.b32 	 %f238, %r19;
	shr.u32 	%r20, %r17, 23;
	cvt.rn.f32.u32	%f239, %r20;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB30_36:
	neg.f32 	%f268, %f267;

BB30_37:
	cvt.rzi.s32.f32	%r21, %f268;
	st.param.b32	[func_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___lgammaj(
	.param .b32 ___lgammaj_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.u32 	%r3, [___lgammaj_param_0];
	cvt.rn.f32.u32	%f1, %r3;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p1, %f2, 0f40400000;
	@%p1 bra 	BB31_7;

	setp.ltu.f32	%p2, %f2, 0f40F9999A;
	@%p2 bra 	BB31_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f2;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f3, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f2, 0f7F800000;
	setp.gt.f32	%p4, %f2, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB31_4;

	lg2.approx.f32 	%f261, %f2;
	bra.uni 	BB31_5;

BB31_4:
	setp.lt.f32	%p6, %f2, 0f00800000;
	mul.f32 	%f51, %f2, 0f4B800000;
	selp.f32	%f52, %f51, %f2, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r4, %f52;
	and.b32  	%r5, %r4, 8388607;
	or.b32  	%r6, %r5, 1065353216;
	mov.b32 	 %f54, %r6;
	shr.u32 	%r7, %r4, 23;
	cvt.rn.f32.u32	%f55, %r7;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB31_5:
	add.f32 	%f76, %f2, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f2;
	add.rn.f32 	%f80, %f78, %f3;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f2, 0f7F800000;
	selp.f32	%f268, %f2, %f81, %p8;
	bra.uni 	BB31_15;

BB31_6:
	add.f32 	%f84, %f2, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB31_15;

BB31_7:
	setp.ltu.f32	%p9, %f2, 0f3FC00000;
	@%p9 bra 	BB31_9;

	add.f32 	%f100, %f2, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB31_15;

BB31_9:
	setp.ltu.f32	%p10, %f2, 0f3F333333;
	@%p10 bra 	BB31_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f2;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB31_15;

BB31_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f2, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f2, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f2, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f2, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f2, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f2, %f154;
	mul.f32 	%f156, %f155, %f2;
	fma.rn.f32 	%f11, %f156, %f2, %f2;
	setp.gt.f32	%p11, %f11, 0f00000000;
	setp.lt.f32	%p12, %f11, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB31_13;

	lg2.approx.f32 	%f262, %f11;
	bra.uni 	BB31_14;

BB31_13:
	setp.lt.f32	%p14, %f11, 0f00800000;
	mul.f32 	%f159, %f11, 0f4B800000;
	selp.f32	%f160, %f159, %f11, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r8, %f160;
	and.b32  	%r9, %r8, 8388607;
	or.b32  	%r10, %r9, 1065353216;
	mov.b32 	 %f162, %r10;
	shr.u32 	%r11, %r8, 23;
	cvt.rn.f32.u32	%f163, %r11;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB31_14:
	neg.f32 	%f268, %f262;

BB31_15:
	setp.ge.f32	%p16, %f1, 0f00000000;
	@%p16 bra 	BB31_37;

	cvt.rmi.f32.f32	%f184, %f2;
	setp.neu.f32	%p17, %f2, %f184;
	@%p17 bra 	BB31_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB31_37;

BB31_18:
	setp.lt.f32	%p18, %f2, 0f1FEC1E4A;
	@%p18 bra 	BB31_33;

	add.f32 	%f185, %f2, %f2;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f2;
	mul.f32 	%f17, %f189, 0f40490FDB;
	mul.rn.f32 	%f18, %f17, %f17;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB31_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f18, %f190;
	bra.uni 	BB31_22;

BB31_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f18, %f192;

BB31_22:
	@%p19 bra 	BB31_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f18, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f18, %f196;
	bra.uni 	BB31_25;

BB31_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f18, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f18, %f199;

BB31_25:
	fma.rn.f32 	%f265, %f264, %f17, %f17;
	@%p19 bra 	BB31_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f18, %f200;

BB31_27:
	and.b32  	%r12, %r1, 2;
	setp.eq.s32	%p22, %r12, 0;
	@%p22 bra 	BB31_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB31_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f30, %f203, %f2;
	setp.gt.f32	%p23, %f30, 0f00000000;
	setp.lt.f32	%p24, %f30, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB31_31;

	lg2.approx.f32 	%f266, %f30;
	bra.uni 	BB31_32;

BB31_31:
	setp.lt.f32	%p26, %f30, 0f00800000;
	mul.f32 	%f206, %f30, 0f4B800000;
	selp.f32	%f207, %f206, %f30, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r13, %f207;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	 %f209, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32	%f210, %r16;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB31_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB31_37;

BB31_33:
	setp.gt.f32	%p28, %f2, 0f00000000;
	setp.lt.f32	%p29, %f2, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB31_35;

	lg2.approx.f32 	%f267, %f2;
	bra.uni 	BB31_36;

BB31_35:
	setp.lt.f32	%p31, %f2, 0f00800000;
	mul.f32 	%f235, %f2, 0f4B800000;
	selp.f32	%f236, %f235, %f2, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r17, %f236;
	and.b32  	%r18, %r17, 8388607;
	or.b32  	%r19, %r18, 1065353216;
	mov.b32 	 %f238, %r19;
	shr.u32 	%r20, %r17, 23;
	cvt.rn.f32.u32	%f239, %r20;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB31_36:
	neg.f32 	%f268, %f267;

BB31_37:
	cvt.rzi.u32.f32	%r21, %f268;
	st.param.b32	[func_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___lgammac(
	.param .b32 ___lgammac_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.s8 	%rs1, [___lgammac_param_0];
	cvt.rn.f32.s16	%f39, %rs1;
	abs.f32 	%f1, %f39;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB32_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB32_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f1;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB32_4;

	lg2.approx.f32 	%f261, %f1;
	bra.uni 	BB32_5;

BB32_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f51, %f1, 0f4B800000;
	selp.f32	%f52, %f51, %f1, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r3, %f52;
	and.b32  	%r4, %r3, 8388607;
	or.b32  	%r5, %r4, 1065353216;
	mov.b32 	 %f54, %r5;
	shr.u32 	%r6, %r3, 23;
	cvt.rn.f32.u32	%f55, %r6;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB32_5:
	add.f32 	%f76, %f1, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f1;
	add.rn.f32 	%f80, %f78, %f2;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f268, %f1, %f81, %p8;
	bra.uni 	BB32_15;

BB32_6:
	add.f32 	%f84, %f1, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB32_15;

BB32_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB32_9;

	add.f32 	%f100, %f1, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB32_15;

BB32_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB32_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f1;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB32_15;

BB32_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f1, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f1, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f1, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f1, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f1, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f1, %f154;
	mul.f32 	%f156, %f155, %f1;
	fma.rn.f32 	%f10, %f156, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB32_13;

	lg2.approx.f32 	%f262, %f10;
	bra.uni 	BB32_14;

BB32_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f159, %f10, 0f4B800000;
	selp.f32	%f160, %f159, %f10, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r7, %f160;
	and.b32  	%r8, %r7, 8388607;
	or.b32  	%r9, %r8, 1065353216;
	mov.b32 	 %f162, %r9;
	shr.u32 	%r10, %r7, 23;
	cvt.rn.f32.u32	%f163, %r10;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB32_14:
	neg.f32 	%f268, %f262;

BB32_15:
	setp.gt.s16	%p16, %rs1, -1;
	@%p16 bra 	BB32_37;

	cvt.rmi.f32.f32	%f184, %f1;
	setp.neu.f32	%p17, %f1, %f184;
	@%p17 bra 	BB32_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB32_37;

BB32_18:
	setp.lt.f32	%p18, %f1, 0f1FEC1E4A;
	@%p18 bra 	BB32_33;

	add.f32 	%f185, %f1, %f1;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f1;
	mul.f32 	%f16, %f189, 0f40490FDB;
	mul.rn.f32 	%f17, %f16, %f16;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB32_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f17, %f190;
	bra.uni 	BB32_22;

BB32_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f17, %f192;

BB32_22:
	@%p19 bra 	BB32_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f17, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f17, %f196;
	bra.uni 	BB32_25;

BB32_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f17, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f17, %f199;

BB32_25:
	fma.rn.f32 	%f265, %f264, %f16, %f16;
	@%p19 bra 	BB32_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f17, %f200;

BB32_27:
	and.b32  	%r11, %r1, 2;
	setp.eq.s32	%p22, %r11, 0;
	@%p22 bra 	BB32_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB32_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f29, %f203, %f1;
	setp.gt.f32	%p23, %f29, 0f00000000;
	setp.lt.f32	%p24, %f29, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB32_31;

	lg2.approx.f32 	%f266, %f29;
	bra.uni 	BB32_32;

BB32_31:
	setp.lt.f32	%p26, %f29, 0f00800000;
	mul.f32 	%f206, %f29, 0f4B800000;
	selp.f32	%f207, %f206, %f29, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r12, %f207;
	and.b32  	%r13, %r12, 8388607;
	or.b32  	%r14, %r13, 1065353216;
	mov.b32 	 %f209, %r14;
	shr.u32 	%r15, %r12, 23;
	cvt.rn.f32.u32	%f210, %r15;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB32_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB32_37;

BB32_33:
	setp.gt.f32	%p28, %f1, 0f00000000;
	setp.lt.f32	%p29, %f1, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB32_35;

	lg2.approx.f32 	%f267, %f1;
	bra.uni 	BB32_36;

BB32_35:
	setp.lt.f32	%p31, %f1, 0f00800000;
	mul.f32 	%f235, %f1, 0f4B800000;
	selp.f32	%f236, %f235, %f1, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r16, %f236;
	and.b32  	%r17, %r16, 8388607;
	or.b32  	%r18, %r17, 1065353216;
	mov.b32 	 %f238, %r18;
	shr.u32 	%r19, %r16, 23;
	cvt.rn.f32.u32	%f239, %r19;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB32_36:
	neg.f32 	%f268, %f267;

BB32_37:
	cvt.rzi.s32.f32	%r20, %f268;
	cvt.s32.s8 	%r21, %r20;
	st.param.b32	[func_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___lgammah(
	.param .b32 ___lgammah_param_0
)
{
	.reg .pred 	%p<16>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<11>;
	.reg .f32 	%f<164>;


	ld.param.u8 	%rs1, [___lgammah_param_0];
	cvt.rn.f32.u16	%f16, %rs1;
	abs.f32 	%f1, %f16;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB33_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB33_6;

	// inline asm
	rcp.approx.ftz.f32 %f17,%f1;
	// inline asm
	mul.f32 	%f19, %f17, %f17;
	mov.f32 	%f20, 0fBB360953;
	mov.f32 	%f21, 0f3A4BE755;
	fma.rn.f32 	%f22, %f21, %f19, %f20;
	mov.f32 	%f23, 0f3DAAAAA3;
	fma.rn.f32 	%f24, %f22, %f19, %f23;
	mov.f32 	%f25, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f24, %f17, %f25;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB33_4;

	lg2.approx.f32 	%f161, %f1;
	bra.uni 	BB33_5;

BB33_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f28, %f1, 0f4B800000;
	selp.f32	%f29, %f28, %f1, %p6;
	selp.f32	%f30, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r1, %f29;
	and.b32  	%r2, %r1, 8388607;
	or.b32  	%r3, %r2, 1065353216;
	mov.b32 	 %f31, %r3;
	shr.u32 	%r4, %r1, 23;
	cvt.rn.f32.u32	%f32, %r4;
	add.f32 	%f33, %f30, %f32;
	setp.gt.f32	%p7, %f31, 0f3FAE147B;
	mul.f32 	%f34, %f31, 0f3F000000;
	add.f32 	%f35, %f33, 0f3F800000;
	selp.f32	%f36, %f34, %f31, %p7;
	selp.f32	%f37, %f35, %f33, %p7;
	add.f32 	%f27, %f36, 0f3F800000;
	add.f32 	%f38, %f36, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f26,%f27;
	// inline asm
	neg.f32 	%f39, %f38;
	mul.f32 	%f40, %f38, %f39;
	mul.rn.f32 	%f41, %f26, %f40;
	add.rn.f32 	%f42, %f38, %f41;
	mul.f32 	%f43, %f42, %f42;
	mov.f32 	%f44, 0f3C4C6A36;
	mov.f32 	%f45, 0f3B1E94E6;
	fma.rn.f32 	%f46, %f45, %f43, %f44;
	mov.f32 	%f47, 0f3DAAAB1A;
	fma.rn.f32 	%f48, %f46, %f43, %f47;
	mul.f32 	%f49, %f48, %f43;
	fma.rn.f32 	%f50, %f49, %f42, %f41;
	add.f32 	%f51, %f50, %f38;
	mov.f32 	%f52, 0f3F317218;
	fma.rn.f32 	%f161, %f37, %f52, %f51;

BB33_5:
	add.f32 	%f53, %f1, 0fBF000000;
	mul.f32 	%f54, %f161, 0f3F000000;
	mul.rn.f32 	%f55, %f54, %f53;
	sub.f32 	%f56, %f55, %f1;
	add.rn.f32 	%f57, %f55, %f2;
	add.f32 	%f58, %f56, %f57;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f163, %f1, %f58, %p8;
	bra.uni 	BB33_15;

BB33_6:
	add.f32 	%f61, %f1, 0fC0400000;
	mov.f32 	%f62, 0fC640F6F8;
	mov.f32 	%f63, 0fC43B38FB;
	fma.rn.f32 	%f64, %f63, %f61, %f62;
	mov.f32 	%f65, 0fC7206560;
	fma.rn.f32 	%f66, %f64, %f61, %f65;
	mov.f32 	%f67, 0fC73CB6AA;
	fma.rn.f32 	%f68, %f66, %f61, %f67;
	mov.f32 	%f69, 0fC80BAE5A;
	fma.rn.f32 	%f70, %f68, %f61, %f69;
	add.f32 	%f71, %f61, 0fC381A020;
	mov.f32 	%f72, 0fC62864B8;
	fma.rn.f32 	%f73, %f71, %f61, %f72;
	mov.f32 	%f74, 0fC7B50686;
	fma.rn.f32 	%f75, %f73, %f61, %f74;
	mov.f32 	%f76, 0fC8498465;
	fma.rn.f32 	%f60, %f75, %f61, %f76;
	// inline asm
	rcp.approx.ftz.f32 %f59,%f60;
	// inline asm
	fma.rn.f32 	%f163, %f70, %f59, %f61;
	bra.uni 	BB33_15;

BB33_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB33_9;

	add.f32 	%f77, %f1, 0fC0000000;
	mov.f32 	%f78, 0fB967A002;
	mov.f32 	%f79, 0f385007FA;
	fma.rn.f32 	%f80, %f79, %f77, %f78;
	mov.f32 	%f81, 0f3A0DE6FC;
	fma.rn.f32 	%f82, %f80, %f77, %f81;
	mov.f32 	%f83, 0fBA9DE0E2;
	fma.rn.f32 	%f84, %f82, %f77, %f83;
	mov.f32 	%f85, 0f3B3D05B7;
	fma.rn.f32 	%f86, %f84, %f77, %f85;
	mov.f32 	%f87, 0fBBF1EB10;
	fma.rn.f32 	%f88, %f86, %f77, %f87;
	mov.f32 	%f89, 0f3CA89A28;
	fma.rn.f32 	%f90, %f88, %f77, %f89;
	mov.f32 	%f91, 0fBD89F01A;
	fma.rn.f32 	%f92, %f90, %f77, %f91;
	mov.f32 	%f93, 0f3EA51A66;
	fma.rn.f32 	%f94, %f92, %f77, %f93;
	mov.f32 	%f95, 0f3ED87730;
	fma.rn.f32 	%f96, %f94, %f77, %f95;
	mul.f32 	%f163, %f96, %f77;
	bra.uni 	BB33_15;

BB33_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB33_11;

	mov.f32 	%f97, 0f3F800000;
	sub.f32 	%f98, %f97, %f1;
	mov.f32 	%f99, 0f3DD47577;
	mov.f32 	%f100, 0f3D3BEF76;
	fma.rn.f32 	%f101, %f100, %f98, %f99;
	mov.f32 	%f102, 0f3DFB8079;
	fma.rn.f32 	%f103, %f101, %f98, %f102;
	mov.f32 	%f104, 0f3E0295B5;
	fma.rn.f32 	%f105, %f103, %f98, %f104;
	mov.f32 	%f106, 0f3E12A765;
	fma.rn.f32 	%f107, %f105, %f98, %f106;
	mov.f32 	%f108, 0f3E2D6867;
	fma.rn.f32 	%f109, %f107, %f98, %f108;
	mov.f32 	%f110, 0f3E5462BF;
	fma.rn.f32 	%f111, %f109, %f98, %f110;
	mov.f32 	%f112, 0f3E8A8A72;
	fma.rn.f32 	%f113, %f111, %f98, %f112;
	mov.f32 	%f114, 0f3ECD26A4;
	fma.rn.f32 	%f115, %f113, %f98, %f114;
	mov.f32 	%f116, 0f3F528D32;
	fma.rn.f32 	%f117, %f115, %f98, %f116;
	mov.f32 	%f118, 0f3F13C468;
	fma.rn.f32 	%f119, %f117, %f98, %f118;
	mul.f32 	%f163, %f119, %f98;
	bra.uni 	BB33_15;

BB33_11:
	mov.f32 	%f120, 0fBBB34878;
	mov.f32 	%f121, 0f3B6B1C86;
	fma.rn.f32 	%f122, %f121, %f1, %f120;
	mov.f32 	%f123, 0fBD36CAEF;
	fma.rn.f32 	%f124, %f122, %f1, %f123;
	mov.f32 	%f125, 0f3E2B5555;
	fma.rn.f32 	%f126, %f124, %f1, %f125;
	mov.f32 	%f127, 0fBD2C96C7;
	fma.rn.f32 	%f128, %f126, %f1, %f127;
	mov.f32 	%f129, 0fBF27E6EB;
	fma.rn.f32 	%f130, %f128, %f1, %f129;
	mov.f32 	%f131, 0f3F13C463;
	fma.rn.f32 	%f132, %f130, %f1, %f131;
	mul.f32 	%f133, %f132, %f1;
	fma.rn.f32 	%f10, %f133, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB33_13;

	lg2.approx.f32 	%f162, %f10;
	bra.uni 	BB33_14;

BB33_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f136, %f10, 0f4B800000;
	selp.f32	%f137, %f136, %f10, %p14;
	selp.f32	%f138, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r5, %f137;
	and.b32  	%r6, %r5, 8388607;
	or.b32  	%r7, %r6, 1065353216;
	mov.b32 	 %f139, %r7;
	shr.u32 	%r8, %r5, 23;
	cvt.rn.f32.u32	%f140, %r8;
	add.f32 	%f141, %f138, %f140;
	setp.gt.f32	%p15, %f139, 0f3FAE147B;
	mul.f32 	%f142, %f139, 0f3F000000;
	add.f32 	%f143, %f141, 0f3F800000;
	selp.f32	%f144, %f142, %f139, %p15;
	selp.f32	%f145, %f143, %f141, %p15;
	add.f32 	%f135, %f144, 0f3F800000;
	add.f32 	%f146, %f144, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f134,%f135;
	// inline asm
	neg.f32 	%f147, %f146;
	mul.f32 	%f148, %f146, %f147;
	mul.rn.f32 	%f149, %f134, %f148;
	add.rn.f32 	%f150, %f146, %f149;
	mul.f32 	%f151, %f150, %f150;
	mov.f32 	%f152, 0f3C4C6A36;
	mov.f32 	%f153, 0f3B1E94E6;
	fma.rn.f32 	%f154, %f153, %f151, %f152;
	mov.f32 	%f155, 0f3DAAAB1A;
	fma.rn.f32 	%f156, %f154, %f151, %f155;
	mul.f32 	%f157, %f156, %f151;
	fma.rn.f32 	%f158, %f157, %f150, %f149;
	add.f32 	%f159, %f158, %f146;
	mov.f32 	%f160, 0f3F317218;
	fma.rn.f32 	%f162, %f145, %f160, %f159;

BB33_14:
	neg.f32 	%f163, %f162;

BB33_15:
	cvt.rzi.u32.f32	%r9, %f163;
	and.b32  	%r10, %r9, 255;
	st.param.b32	[func_retval0+0], %r10;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___lgammad(
	.param .b64 ___lgammad_param_0
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<37>;
	.reg .s64 	%rd<3>;
	.reg .f64 	%fd<125>;


	ld.param.f64 	%fd22, [___lgammad_param_0];
	abs.f64 	%fd1, %fd22;
	setp.gtu.f64	%p1, %fd1, 0d7FF0000000000000;
	@%p1 bra 	BB34_25;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_lgamma_pos, 
	(
	param0
	);
	ld.param.f64	%fd124, [retval0+0];
	}
	// Callseq End 1
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd22;
	}
	setp.gt.s32	%p2, %r12, -1;
	@%p2 bra 	BB34_26;

	cvt.rzi.f64.f64	%fd23, %fd1;
	setp.neu.f64	%p3, %fd1, %fd23;
	@%p3 bra 	BB34_4;

	mov.f64 	%fd124, 0d7FF0000000000000;
	bra.uni 	BB34_26;

BB34_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd1;
	}
	setp.lt.s32	%p4, %r1, 1006632960;
	mov.f64 	%fd120, %fd1;
	@%p4 bra 	BB34_12;

	add.s32 	%r13, %r1, %r1;
	setp.lt.u32	%p5, %r13, -2038431743;
	mov.f64 	%fd121, %fd1;
	@%p5 bra 	BB34_7;

	mov.f64 	%fd24, 0d0000000000000000;
	mul.rn.f64 	%fd3, %fd1, %fd24;
	mov.f64 	%fd121, %fd3;

BB34_7:
	mov.f64 	%fd4, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd4;
	}
	add.s32 	%r15, %r14, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd4;
	}
	mov.b64 	%fd25, {%r16, %r15};
	cvt.rni.f64.f64	%fd26, %fd25;
	cvt.rzi.s64.f64	%rd1, %fd26;
	cvt.u32.u64	%r17, %rd1;
	neg.f64 	%fd27, %fd26;
	mov.f64 	%fd28, 0d3FE0000000000000;
	fma.rn.f64 	%fd29, %fd27, %fd28, %fd4;
	mul.f64 	%fd30, %fd29, 0d3CA1A62633145C07;
	mov.f64 	%fd31, 0d400921FB54442D18;
	fma.rn.f64 	%fd32, %fd29, %fd31, %fd30;
	mul.rn.f64 	%fd33, %fd32, %fd32;
	mov.f64 	%fd34, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd35, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd36, %fd35, %fd33, %fd34;
	mov.f64 	%fd37, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd38, %fd36, %fd33, %fd37;
	mov.f64 	%fd39, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd40, %fd38, %fd33, %fd39;
	mov.f64 	%fd41, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd42, %fd40, %fd33, %fd41;
	mov.f64 	%fd43, 0d3FA5555555555551;
	fma.rn.f64 	%fd44, %fd42, %fd33, %fd43;
	mov.f64 	%fd45, 0dBFE0000000000000;
	fma.rn.f64 	%fd46, %fd44, %fd33, %fd45;
	mov.f64 	%fd47, 0d3FF0000000000000;
	fma.rn.f64 	%fd48, %fd46, %fd33, %fd47;
	mov.f64 	%fd49, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd50, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd51, %fd50, %fd33, %fd49;
	mov.f64 	%fd52, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd53, %fd51, %fd33, %fd52;
	mov.f64 	%fd54, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd55, %fd53, %fd33, %fd54;
	mov.f64 	%fd56, 0d3F81111111110818;
	fma.rn.f64 	%fd57, %fd55, %fd33, %fd56;
	mov.f64 	%fd58, 0dBFC5555555555554;
	fma.rn.f64 	%fd59, %fd57, %fd33, %fd58;
	mov.f64 	%fd60, 0d0000000000000000;
	fma.rn.f64 	%fd61, %fd59, %fd33, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd32, %fd32;
	and.b64  	%rd2, %rd1, 1;
	setp.eq.b64	%p6, %rd2, 1;
	not.pred 	%p7, %p6;
	selp.f64	%fd118, %fd62, %fd48, %p7;
	and.b32  	%r18, %r17, 2;
	setp.eq.s32	%p8, %r18, 0;
	@%p8 bra 	BB34_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd118;
	}
	xor.b32  	%r20, %r19, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd118;
	}
	mov.b64 	%fd118, {%r21, %r20};

BB34_9:
	cvt.rzi.f64.f64	%fd63, %fd4;
	setp.neu.f64	%p9, %fd4, %fd63;
	@%p9 bra 	BB34_11;

	mul.rn.f64 	%fd118, %fd4, %fd60;

BB34_11:
	abs.f64 	%fd65, %fd118;
	mul.f64 	%fd66, %fd65, %fd1;
	div.rn.f64 	%fd120, %fd31, %fd66;

BB34_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd120;
	}
	setp.lt.s32	%p10, %r33, 2146435072;
	setp.gt.f64	%p11, %fd120, 0d0000000000000000;
	and.pred  	%p12, %p11, %p10;
	@%p12 bra 	BB34_18;

	abs.f64 	%fd68, %fd120;
	setp.gtu.f64	%p13, %fd68, 0d7FF0000000000000;
	@%p13 bra 	BB34_17;

	setp.neu.f64	%p14, %fd120, 0d0000000000000000;
	@%p14 bra 	BB34_16;

	mov.f64 	%fd123, 0dFFF0000000000000;
	bra.uni 	BB34_24;

BB34_16:
	setp.eq.f64	%p15, %fd120, 0d7FF0000000000000;
	selp.f64	%fd123, %fd120, 0dFFF8000000000000, %p15;
	bra.uni 	BB34_24;

BB34_17:
	add.f64 	%fd123, %fd120, %fd120;
	bra.uni 	BB34_24;

BB34_18:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd120;
	}
	setp.lt.s32	%p16, %r33, 1048576;
	@%p16 bra 	BB34_20;

	mov.u32 	%r35, -1023;
	bra.uni 	BB34_21;

BB34_20:
	mul.f64 	%fd70, %fd120, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd70;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd70;
	}
	mov.u32 	%r35, -1077;

BB34_21:
	shr.u32 	%r24, %r33, 20;
	add.s32 	%r36, %r35, %r24;
	and.b32  	%r25, %r33, -2146435073;
	or.b32  	%r26, %r25, 1072693248;
	mov.b64 	%fd122, {%r34, %r26};
	setp.lt.s32	%p17, %r26, 1073127583;
	@%p17 bra 	BB34_23;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd122;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd122;
	}
	add.s32 	%r29, %r28, -1048576;
	mov.b64 	%fd122, {%r27, %r29};
	add.s32 	%r36, %r36, 1;

BB34_23:
	add.f64 	%fd72, %fd122, 0d3FF0000000000000;
	mov.f64 	%fd73, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd71,%fd72;
	// inline asm
	neg.f64 	%fd74, %fd72;
	fma.rn.f64 	%fd75, %fd74, %fd71, %fd73;
	fma.rn.f64 	%fd76, %fd75, %fd75, %fd75;
	fma.rn.f64 	%fd77, %fd76, %fd71, %fd71;
	add.f64 	%fd78, %fd122, 0dBFF0000000000000;
	mul.f64 	%fd79, %fd78, %fd77;
	fma.rn.f64 	%fd80, %fd78, %fd77, %fd79;
	mul.f64 	%fd81, %fd80, %fd80;
	mov.f64 	%fd82, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd83, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd84, %fd83, %fd81, %fd82;
	mov.f64 	%fd85, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd86, %fd84, %fd81, %fd85;
	mov.f64 	%fd87, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd88, %fd86, %fd81, %fd87;
	mov.f64 	%fd89, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd90, %fd88, %fd81, %fd89;
	mov.f64 	%fd91, 0d3F624924923BE72D;
	fma.rn.f64 	%fd92, %fd90, %fd81, %fd91;
	mov.f64 	%fd93, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd94, %fd92, %fd81, %fd93;
	mov.f64 	%fd95, 0d3FB5555555555554;
	fma.rn.f64 	%fd96, %fd94, %fd81, %fd95;
	sub.f64 	%fd97, %fd78, %fd80;
	add.f64 	%fd98, %fd97, %fd97;
	neg.f64 	%fd99, %fd80;
	fma.rn.f64 	%fd100, %fd99, %fd78, %fd98;
	mul.f64 	%fd101, %fd77, %fd100;
	mul.f64 	%fd102, %fd96, %fd81;
	fma.rn.f64 	%fd103, %fd102, %fd80, %fd101;
	xor.b32  	%r30, %r36, -2147483648;
	mov.u32 	%r31, -2147483648;
	mov.u32 	%r32, 1127219200;
	mov.b64 	%fd104, {%r30, %r32};
	mov.b64 	%fd105, {%r31, %r32};
	sub.f64 	%fd106, %fd104, %fd105;
	mov.f64 	%fd107, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd108, %fd106, %fd107, %fd80;
	neg.f64 	%fd109, %fd106;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	sub.f64 	%fd111, %fd110, %fd80;
	sub.f64 	%fd112, %fd103, %fd111;
	mov.f64 	%fd113, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd114, %fd106, %fd113, %fd112;
	add.f64 	%fd123, %fd108, %fd114;

BB34_24:
	sub.f64 	%fd115, %fd123, %fd124;
	neg.f64 	%fd116, %fd123;
	selp.f64	%fd124, %fd116, %fd115, %p4;
	bra.uni 	BB34_26;

BB34_25:
	add.f64 	%fd124, %fd22, %fd22;

BB34_26:
	st.param.f64	[func_retval0+0], %fd124;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___minff(
	.param .b32 ___minff_param_0,
	.param .b32 ___minff_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___minff_param_0];
	ld.param.f32 	%f2, [___minff_param_1];
	min.f32 	%f3, %f1, %f2;
	st.param.f32	[func_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___minii(
	.param .b32 ___minii_param_0,
	.param .b32 ___minii_param_1
)
{
	.reg .s32 	%r<4>;
	.reg .f32 	%f<4>;


	ld.param.u32 	%r1, [___minii_param_0];
	ld.param.u32 	%r2, [___minii_param_1];
	cvt.rn.f32.s32	%f1, %r1;
	cvt.rn.f32.s32	%f2, %r2;
	min.f32 	%f3, %f1, %f2;
	cvt.rzi.s32.f32	%r3, %f3;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___minuu(
	.param .b32 ___minjj_param_0,
	.param .b32 ___minjj_param_1
)
{
	.reg .s32 	%r<4>;
	.reg .f32 	%f<4>;


	ld.param.u32 	%r1, [___minjj_param_0];
	ld.param.u32 	%r2, [___minjj_param_1];
	cvt.rn.f32.u32	%f1, %r1;
	cvt.rn.f32.u32	%f2, %r2;
	min.f32 	%f3, %f1, %f2;
	cvt.rzi.u32.f32	%r3, %f3;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___minjj(
	.param .b32 ___mincc_param_0,
	.param .b32 ___mincc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<4>;


	ld.param.s8 	%rs1, [___mincc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	ld.param.s8 	%rs2, [___mincc_param_1];
	cvt.rn.f32.s16	%f2, %rs2;
	min.f32 	%f3, %f1, %f2;
	cvt.rzi.s32.f32	%r1, %f3;
	cvt.s32.s8 	%r2, %r1;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___minvv(
	.param .b32 ___minhh_param_0,
	.param .b32 ___minhh_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<4>;


	ld.param.u8 	%rs1, [___minhh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	ld.param.u8 	%rs2, [___minhh_param_1];
	cvt.rn.f32.u16	%f2, %rs2;
	min.f32 	%f3, %f1, %f2;
	cvt.rzi.u32.f32	%r1, %f3;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___mindd(
	.param .b64 ___mindd_param_0,
	.param .b64 ___mindd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___mindd_param_0];
	ld.param.f64 	%fd2, [___mindd_param_1];
	min.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[func_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___maxff(
	.param .b32 ___maxff_param_0,
	.param .b32 ___maxff_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___maxff_param_0];
	ld.param.f32 	%f2, [___maxff_param_1];
	max.f32 	%f3, %f1, %f2;
	st.param.f32	[func_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___maxii(
	.param .b32 ___maxii_param_0,
	.param .b32 ___maxii_param_1
)
{
	.reg .s32 	%r<4>;
	.reg .f32 	%f<4>;


	ld.param.u32 	%r1, [___maxii_param_0];
	ld.param.u32 	%r2, [___maxii_param_1];
	cvt.rn.f32.s32	%f1, %r1;
	cvt.rn.f32.s32	%f2, %r2;
	max.f32 	%f3, %f1, %f2;
	cvt.rzi.s32.f32	%r3, %f3;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___maxuu(
	.param .b32 ___maxjj_param_0,
	.param .b32 ___maxjj_param_1
)
{
	.reg .s32 	%r<4>;
	.reg .f32 	%f<4>;


	ld.param.u32 	%r1, [___maxjj_param_0];
	ld.param.u32 	%r2, [___maxjj_param_1];
	cvt.rn.f32.u32	%f1, %r1;
	cvt.rn.f32.u32	%f2, %r2;
	max.f32 	%f3, %f1, %f2;
	cvt.rzi.u32.f32	%r3, %f3;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___maxjj(
	.param .b32 ___maxcc_param_0,
	.param .b32 ___maxcc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<4>;


	ld.param.s8 	%rs1, [___maxcc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	ld.param.s8 	%rs2, [___maxcc_param_1];
	cvt.rn.f32.s16	%f2, %rs2;
	max.f32 	%f3, %f1, %f2;
	cvt.rzi.s32.f32	%r1, %f3;
	cvt.s32.s8 	%r2, %r1;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___maxvv(
	.param .b32 ___maxhh_param_0,
	.param .b32 ___maxhh_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<4>;


	ld.param.u8 	%rs1, [___maxhh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	ld.param.u8 	%rs2, [___maxhh_param_1];
	cvt.rn.f32.u16	%f2, %rs2;
	max.f32 	%f3, %f1, %f2;
	cvt.rzi.u32.f32	%r1, %f3;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[func_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___maxdd(
	.param .b64 ___maxdd_param_0,
	.param .b64 ___maxdd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___maxdd_param_0];
	ld.param.f64 	%fd2, [___maxdd_param_1];
	max.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[func_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___modff(
	.param .b32 ___modff_param_0,
	.param .b32 ___modff_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<18>;
	.reg .f32 	%f<48>;


	ld.param.f32 	%f18, [___modff_param_0];
	ld.param.f32 	%f19, [___modff_param_1];
	abs.f32 	%f46, %f18;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	abs.f32 	%f2, %f19;
	setp.eq.f32	%p2, %f2, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB47_2;
	bra.uni 	BB47_1;

BB47_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB47_14;

BB47_2:
	setp.ltu.f32	%p4, %f46, %f2;
	@%p4 bra 	BB47_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f2;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p5, %f3, 0f00000000;
	setp.eq.f32	%p6, %f3, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB47_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB47_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB47_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f2, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB47_10;

BB47_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f2, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB47_10;

BB47_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f2, %f34;
	bra.uni 	BB47_10;

BB47_9:
	setp.leu.f32	%p12, %f3, 0f00000000;
	add.f32 	%f36, %f2, %f2;
	selp.f32	%f44, %f36, %f2, %p12;

BB47_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f2;
	@%p14 bra 	BB47_12;

BB47_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f2;
	@%p16 bra 	BB47_11;

BB47_12:
	mov.b32 	 %r14, %f18;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB47_14;

BB47_13:
	setp.gtu.f32	%p17, %f2, 0f7F800000;
	add.f32 	%f40, %f18, %f19;
	selp.f32	%f41, %f40, %f18, %p17;
	add.f32 	%f42, %f41, %f18;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB47_14:
	st.param.f32	[func_retval0+0], %f47;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___modii(
	.param .b32 ___modii_param_0,
	.param .b32 ___modii_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<21>;
	.reg .f32 	%f<48>;


	ld.param.u32 	%r3, [___modii_param_0];
	ld.param.u32 	%r4, [___modii_param_1];
	cvt.rn.f32.s32	%f1, %r3;
	abs.f32 	%f46, %f1;
	cvt.rn.f32.s32	%f3, %r4;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB48_2;
	bra.uni 	BB48_1;

BB48_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB48_14;

BB48_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB48_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r5, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r6, %f21;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r5, %r6;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB48_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB48_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB48_7;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f23, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f25, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB48_10;

BB48_7:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f30, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f32, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB48_10;

BB48_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB48_10;

BB48_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB48_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB48_12;

BB48_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB48_11;

BB48_12:
	mov.b32 	 %r16, %f1;
	and.b32  	%r17, %r16, -2147483648;
	mov.b32 	 %r18, %f46;
	or.b32  	%r19, %r18, %r17;
	mov.b32 	 %f47, %r19;
	bra.uni 	BB48_14;

BB48_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB48_14:
	cvt.rzi.s32.f32	%r20, %f47;
	st.param.b32	[func_retval0+0], %r20;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___moduu(
	.param .b32 ___modjj_param_0,
	.param .b32 ___modjj_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<21>;
	.reg .f32 	%f<48>;


	ld.param.u32 	%r3, [___modjj_param_0];
	ld.param.u32 	%r4, [___modjj_param_1];
	cvt.rn.f32.u32	%f1, %r3;
	abs.f32 	%f46, %f1;
	cvt.rn.f32.u32	%f3, %r4;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB49_2;
	bra.uni 	BB49_1;

BB49_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB49_14;

BB49_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB49_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r5, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r6, %f21;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r5, %r6;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB49_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB49_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB49_7;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f23, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f25, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB49_10;

BB49_7:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f30, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f32, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB49_10;

BB49_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB49_10;

BB49_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB49_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB49_12;

BB49_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB49_11;

BB49_12:
	mov.b32 	 %r16, %f1;
	and.b32  	%r17, %r16, -2147483648;
	mov.b32 	 %r18, %f46;
	or.b32  	%r19, %r18, %r17;
	mov.b32 	 %f47, %r19;
	bra.uni 	BB49_14;

BB49_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB49_14:
	cvt.rzi.u32.f32	%r20, %f47;
	st.param.b32	[func_retval0+0], %r20;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___modjj(
	.param .b32 ___modcc_param_0,
	.param .b32 ___modcc_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<48>;


	ld.param.s8 	%rs1, [___modcc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f46, %f1;
	ld.param.s8 	%rs2, [___modcc_param_1];
	cvt.rn.f32.s16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB50_2;
	bra.uni 	BB50_1;

BB50_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB50_14;

BB50_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB50_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB50_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB50_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB50_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB50_10;

BB50_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB50_10;

BB50_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB50_10;

BB50_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB50_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB50_12;

BB50_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB50_11;

BB50_12:
	mov.b32 	 %r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB50_14;

BB50_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB50_14:
	cvt.rzi.s32.f32	%r18, %f47;
	cvt.s32.s8 	%r19, %r18;
	st.param.b32	[func_retval0+0], %r19;
	ret;
}

.visible .func  (.param .b32 func_retval0) ___modvv(
	.param .b32 ___modhh_param_0,
	.param .b32 ___modhh_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<48>;


	ld.param.u8 	%rs1, [___modhh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f46, %f1;
	ld.param.u8 	%rs2, [___modhh_param_1];
	cvt.rn.f32.u16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB51_2;
	bra.uni 	BB51_1;

BB51_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB51_14;

BB51_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB51_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB51_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB51_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB51_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB51_10;

BB51_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB51_10;

BB51_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB51_10;

BB51_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB51_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB51_12;

BB51_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB51_11;

BB51_12:
	mov.b32 	 %r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB51_14;

BB51_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB51_14:
	cvt.rzi.u32.f32	%r18, %f47;
	and.b32  	%r19, %r18, 255;
	st.param.b32	[func_retval0+0], %r19;
	ret;
}

.visible .func  (.param .b64 func_retval0) ___moddd(
	.param .b64 ___moddd_param_0,
	.param .b64 ___moddd_param_1
)
{
	.reg .pred 	%p<20>;
	.reg .s32 	%r<23>;
	.reg .f64 	%fd<36>;


	ld.param.f64 	%fd35, [___moddd_param_0];
	ld.param.f64 	%fd20, [___moddd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd35;
	}
	and.b32  	%r8, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd20;
	}
	and.b32  	%r21, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd35;
	}
	mov.b64 	%fd34, {%r10, %r8};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd20;
	}
	mov.b64 	%fd2, {%r22, %r21};
	setp.gt.u32	%p1, %r8, 2146435071;
	setp.gt.u32	%p2, %r21, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB52_15;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB52_3;

	mov.f64 	%fd35, 0dFFF8000000000000;
	bra.uni 	BB52_18;

BB52_3:
	setp.ltu.f64	%p5, %fd34, %fd2;
	@%p5 bra 	BB52_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd2;
	}
	setp.lt.s32	%p6, %r11, 1048576;
	@%p6 bra 	BB52_6;

	mov.f64 	%fd30, 0d0000000000000000;
	bra.uni 	BB52_10;

BB52_6:
	setp.geu.f64	%p7, %fd2, %fd34;
	mov.f64 	%fd31, %fd2;
	@%p7 bra 	BB52_9;

	mov.f64 	%fd32, %fd2;

BB52_8:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p8, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd32;
	}
	setp.lt.s32	%p9, %r12, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p10 bra 	BB52_8;

BB52_9:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd30;
	}

BB52_10:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd34;
	}
	setp.lt.s32	%p11, %r13, 1048576;
	@%p11 bra 	BB52_12;

	and.b32  	%r14, %r21, 1048575;
	and.b32  	%r15, %r1, 2146435072;
	or.b32  	%r16, %r14, %r15;
	mov.b64 	%fd29, {%r22, %r16};

BB52_12:
	mul.f64 	%fd22, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd29, %fd34;
	selp.f64	%fd33, %fd22, %fd29, %p12;
	setp.ltu.f64	%p13, %fd33, %fd2;
	@%p13 bra 	BB52_14;

BB52_13:
	sub.f64 	%fd23, %fd34, %fd33;
	setp.ltu.f64	%p14, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd23, %p14;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd33, %fd2;
	@%p15 bra 	BB52_13;

BB52_14:
	and.b32  	%r17, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd34;
	}
	or.b32  	%r19, %r18, %r17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r20, %r19};
	bra.uni 	BB52_18;

BB52_15:
	setp.gtu.f64	%p16, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p17, %fd2, 0d7FF0000000000000;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	BB52_17;

	setp.eq.f64	%p19, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd35, %p19;
	bra.uni 	BB52_18;

BB52_17:
	add.f64 	%fd35, %fd35, %fd20;

BB52_18:
	st.param.f64	[func_retval0+0], %fd35;
	ret;
}

.visible .func  (.param .b32 func_retval0) _Z12___remainderff(
	.param .b32 _Z12___remainderff_param_0,
	.param .b32 _Z12___remainderff_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<23>;
	.reg .f32 	%f<54>;


	ld.param.f32 	%f21, [_Z12___remainderff_param_0];
	ld.param.f32 	%f22, [_Z12___remainderff_param_1];
	abs.f32 	%f51, %f21;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	abs.f32 	%f2, %f22;
	setp.gtu.f32	%p2, %f2, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB53_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f2, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB53_3;
	bra.uni 	BB53_2;

BB53_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB53_21;

BB53_3:
	setp.ge.f32	%p7, %f51, %f2;
	@%p7 bra 	BB53_5;

	mov.u32 	%r22, 0;
	bra.uni 	BB53_16;

BB53_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f2;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p8, %f3, 0f00000000;
	setp.eq.f32	%p9, %f3, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB53_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB53_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB53_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f2, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB53_12;

BB53_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f2, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB53_12;

BB53_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f2, %f37;
	bra.uni 	BB53_12;

BB53_11:
	setp.leu.f32	%p15, %f3, 0f00000000;
	add.f32 	%f39, %f2, %f2;
	selp.f32	%f44, %f39, %f2, %p15;

BB53_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f2;
	@%p17 bra 	BB53_13;
	bra.uni 	BB53_22;

BB53_13:
	mov.f32 	%f52, %f51;

BB53_14:
	mov.f32 	%f10, %f52;
	mov.f32 	%f11, %f45;
	sub.f32 	%f42, %f10, %f11;
	setp.ltu.f32	%p18, %f10, %f11;
	selp.f32	%f52, %f10, %f42, %p18;
	mul.f32 	%f45, %f11, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f2;
	@%p19 bra 	BB53_14;

	setp.ge.f32	%p20, %f10, %f11;
	selp.u32	%r22, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB53_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f15, %f50, %f50;
	setp.gt.f32	%p21, %f15, %f2;
	@%p21 bra 	BB53_18;

	setp.eq.f32	%p22, %f15, %f2;
	setp.ne.s32	%p23, %r22, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB53_19;
	bra.uni 	BB53_18;

BB53_18:
	sub.f32 	%f50, %f50, %f2;

BB53_19:
	mov.b32 	 %r18, %f21;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB53_21;

BB53_20:
	add.f32 	%f53, %f21, %f22;

BB53_21:
	st.param.f32	[func_retval0+0], %f53;
	ret;

BB53_22:
	mov.u32 	%r22, 0;
	bra.uni 	BB53_16;
}

.visible .func  (.param .b32 func_retval0) _Z12___remainderii(
	.param .b32 _Z12___remainderii_param_0,
	.param .b32 _Z12___remainderii_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<26>;
	.reg .f32 	%f<54>;


	ld.param.u32 	%r5, [_Z12___remainderii_param_0];
	ld.param.u32 	%r6, [_Z12___remainderii_param_1];
	cvt.rn.f32.s32	%f1, %r5;
	abs.f32 	%f51, %f1;
	cvt.rn.f32.s32	%f3, %r6;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB54_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB54_3;
	bra.uni 	BB54_2;

BB54_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB54_21;

BB54_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB54_5;

	mov.u32 	%r25, 0;
	bra.uni 	BB54_16;

BB54_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r7, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r8, %f24;
	sub.s32 	%r1, %r7, %r8;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r7, %r8;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB54_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB54_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB54_9;

	shr.s32 	%r9, %r1, 31;
	shr.u32 	%r10, %r9, 30;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 2;
	cvt.rn.f32.s32	%f26, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r13, %r12, -3, %r1;
	cvt.rn.f32.s32	%f28, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB54_12;

BB54_9:
	shr.u32 	%r14, %r1, 31;
	add.s32 	%r15, %r1, %r14;
	shr.s32 	%r16, %r15, 1;
	cvt.rn.f32.s32	%f33, %r16;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r17, %r1, %r16;
	cvt.rn.f32.s32	%f35, %r17;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB54_12;

BB54_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB54_12;

BB54_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB54_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB54_13;
	bra.uni 	BB54_22;

BB54_13:
	mov.f32 	%f52, %f51;

BB54_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB54_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r25, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB54_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB54_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r25, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB54_19;
	bra.uni 	BB54_18;

BB54_18:
	sub.f32 	%f50, %f50, %f4;

BB54_19:
	mov.b32 	 %r20, %f1;
	and.b32  	%r21, %r20, -2147483648;
	mov.b32 	 %r22, %f50;
	xor.b32  	%r23, %r22, %r21;
	mov.b32 	 %f53, %r23;
	bra.uni 	BB54_21;

BB54_20:
	add.f32 	%f53, %f1, %f3;

BB54_21:
	cvt.rzi.s32.f32	%r24, %f53;
	st.param.b32	[func_retval0+0], %r24;
	ret;

BB54_22:
	mov.u32 	%r25, 0;
	bra.uni 	BB54_16;
}

.visible .func  (.param .b32 func_retval0) _Z12___remainderuu(
	.param .b32 _Z12___remainderjj_param_0,
	.param .b32 _Z12___remainderjj_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<26>;
	.reg .f32 	%f<54>;


	ld.param.u32 	%r5, [_Z12___remainderjj_param_0];
	ld.param.u32 	%r6, [_Z12___remainderjj_param_1];
	cvt.rn.f32.u32	%f1, %r5;
	abs.f32 	%f51, %f1;
	cvt.rn.f32.u32	%f3, %r6;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB55_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB55_3;
	bra.uni 	BB55_2;

BB55_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB55_21;

BB55_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB55_5;

	mov.u32 	%r25, 0;
	bra.uni 	BB55_16;

BB55_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r7, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r8, %f24;
	sub.s32 	%r1, %r7, %r8;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r7, %r8;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB55_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB55_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB55_9;

	shr.s32 	%r9, %r1, 31;
	shr.u32 	%r10, %r9, 30;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 2;
	cvt.rn.f32.s32	%f26, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r13, %r12, -3, %r1;
	cvt.rn.f32.s32	%f28, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB55_12;

BB55_9:
	shr.u32 	%r14, %r1, 31;
	add.s32 	%r15, %r1, %r14;
	shr.s32 	%r16, %r15, 1;
	cvt.rn.f32.s32	%f33, %r16;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r17, %r1, %r16;
	cvt.rn.f32.s32	%f35, %r17;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB55_12;

BB55_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB55_12;

BB55_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB55_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB55_13;
	bra.uni 	BB55_22;

BB55_13:
	mov.f32 	%f52, %f51;

BB55_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB55_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r25, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB55_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB55_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r25, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB55_19;
	bra.uni 	BB55_18;

BB55_18:
	sub.f32 	%f50, %f50, %f4;

BB55_19:
	mov.b32 	 %r20, %f1;
	and.b32  	%r21, %r20, -2147483648;
	mov.b32 	 %r22, %f50;
	xor.b32  	%r23, %r22, %r21;
	mov.b32 	 %f53, %r23;
	bra.uni 	BB55_21;

BB55_20:
	add.f32 	%f53, %f1, %f3;

BB55_21:
	cvt.rzi.u32.f32	%r24, %f53;
	st.param.b32	[func_retval0+0], %r24;
	ret;

BB55_22:
	mov.u32 	%r25, 0;
	bra.uni 	BB55_16;
}

.visible .func  (.param .b32 func_retval0) _Z12___remainderjj(
	.param .b32 _Z12___remaindercc_param_0,
	.param .b32 _Z12___remaindercc_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<25>;
	.reg .f32 	%f<54>;


	ld.param.s8 	%rs1, [_Z12___remaindercc_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f51, %f1;
	ld.param.s8 	%rs2, [_Z12___remaindercc_param_1];
	cvt.rn.f32.s16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB56_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB56_3;
	bra.uni 	BB56_2;

BB56_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB56_21;

BB56_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB56_5;

	mov.u32 	%r24, 0;
	bra.uni 	BB56_16;

BB56_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB56_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB56_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB56_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB56_12;

BB56_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB56_12;

BB56_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB56_12;

BB56_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB56_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB56_13;
	bra.uni 	BB56_22;

BB56_13:
	mov.f32 	%f52, %f51;

BB56_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB56_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r24, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB56_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB56_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r24, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB56_19;
	bra.uni 	BB56_18;

BB56_18:
	sub.f32 	%f50, %f50, %f4;

BB56_19:
	mov.b32 	 %r18, %f1;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB56_21;

BB56_20:
	add.f32 	%f53, %f1, %f3;

BB56_21:
	cvt.rzi.s32.f32	%r22, %f53;
	cvt.s32.s8 	%r23, %r22;
	st.param.b32	[func_retval0+0], %r23;
	ret;

BB56_22:
	mov.u32 	%r24, 0;
	bra.uni 	BB56_16;
}

.visible .func  (.param .b32 func_retval0) _Z12___remaindervv(
	.param .b32 _Z12___remainderhh_param_0,
	.param .b32 _Z12___remainderhh_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<25>;
	.reg .f32 	%f<54>;


	ld.param.u8 	%rs1, [_Z12___remainderhh_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f51, %f1;
	ld.param.u8 	%rs2, [_Z12___remainderhh_param_1];
	cvt.rn.f32.u16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB57_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB57_3;
	bra.uni 	BB57_2;

BB57_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB57_21;

BB57_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB57_5;

	mov.u32 	%r24, 0;
	bra.uni 	BB57_16;

BB57_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB57_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB57_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB57_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB57_12;

BB57_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB57_12;

BB57_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB57_12;

BB57_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB57_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB57_13;
	bra.uni 	BB57_22;

BB57_13:
	mov.f32 	%f52, %f51;

BB57_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB57_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r24, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB57_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB57_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r24, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB57_19;
	bra.uni 	BB57_18;

BB57_18:
	sub.f32 	%f50, %f50, %f4;

BB57_19:
	mov.b32 	 %r18, %f1;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB57_21;

BB57_20:
	add.f32 	%f53, %f1, %f3;

BB57_21:
	cvt.rzi.u32.f32	%r22, %f53;
	and.b32  	%r23, %r22, 255;
	st.param.b32	[func_retval0+0], %r23;
	ret;

BB57_22:
	mov.u32 	%r24, 0;
	bra.uni 	BB57_16;
}

.visible .func  (.param .b64 func_retval0) _Z12___remainderdd(
	.param .b64 _Z12___remainderdd_param_0,
	.param .b64 _Z12___remainderdd_param_1
)
{
	.reg .pred 	%p<24>;
	.reg .s32 	%r<35>;
	.reg .f64 	%fd<39>;


	ld.param.f64 	%fd22, [_Z12___remainderdd_param_0];
	ld.param.f64 	%fd23, [_Z12___remainderdd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd22;
	}
	and.b32  	%r12, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd23;
	}
	and.b32  	%r31, %r13, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd22;
	}
	mov.b64 	%fd37, {%r14, %r12};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd23;
	}
	mov.b64 	%fd2, {%r32, %r31};
	setp.gt.u32	%p1, %r12, 2146435071;
	setp.gt.u32	%p2, %r31, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB58_22;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB58_3;

	mov.f64 	%fd38, 0dFFF8000000000000;
	bra.uni 	BB58_25;

BB58_3:
	setp.ge.f64	%p5, %fd37, %fd2;
	@%p5 bra 	BB58_5;

	mov.u32 	%r34, 0;
	bra.uni 	BB58_18;

BB58_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.s32	%p6, %r15, 1048576;
	@%p6 bra 	BB58_7;

	mov.f64 	%fd33, 0d0000000000000000;
	bra.uni 	BB58_11;

BB58_7:
	setp.geu.f64	%p7, %fd2, %fd37;
	mov.f64 	%fd34, %fd2;
	@%p7 bra 	BB58_10;

	mov.f64 	%fd35, %fd2;

BB58_9:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p8, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd35;
	}
	setp.lt.s32	%p9, %r16, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p10 bra 	BB58_9;

BB58_10:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd33;
	}

BB58_11:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd37;
	}
	setp.lt.s32	%p11, %r17, 1048576;
	@%p11 bra 	BB58_13;

	and.b32  	%r18, %r31, 1048575;
	and.b32  	%r19, %r1, 2146435072;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd32, {%r32, %r20};

BB58_13:
	mul.f64 	%fd25, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd32, %fd37;
	selp.f64	%fd36, %fd25, %fd32, %p12;
	setp.ge.f64	%p13, %fd36, %fd2;
	@%p13 bra 	BB58_15;

	mov.u32 	%r34, 0;
	bra.uni 	BB58_18;

BB58_15:
	mov.u32 	%r33, -1;

BB58_16:
	setp.ltu.f64	%p14, %fd37, %fd36;
	selp.u32	%r22, 1, 0, %p14;
	shl.b32 	%r23, %r33, 1;
	add.s32 	%r33, %r22, %r23;
	sub.f64 	%fd26, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd26, %p14;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd36, %fd2;
	@%p15 bra 	BB58_16;

	not.b32 	%r24, %r33;
	and.b32  	%r34, %r24, 1;

BB58_18:
	add.f64 	%fd15, %fd37, %fd37;
	setp.gt.f64	%p16, %fd15, %fd2;
	@%p16 bra 	BB58_20;

	setp.eq.f64	%p17, %fd15, %fd2;
	setp.ne.s32	%p18, %r34, 0;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB58_21;
	bra.uni 	BB58_20;

BB58_20:
	sub.f64 	%fd37, %fd37, %fd2;

BB58_21:
	and.b32  	%r27, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd37;
	}
	xor.b32  	%r29, %r28, %r27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd37;
	}
	mov.b64 	%fd38, {%r30, %r29};
	bra.uni 	BB58_25;

BB58_22:
	setp.gtu.f64	%p20, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p21, %fd2, 0d7FF0000000000000;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	BB58_24;

	setp.eq.f64	%p23, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd22, %p23;
	bra.uni 	BB58_25;

BB58_24:
	add.f64 	%fd38, %fd22, %fd23;

BB58_25:
	st.param.f64	[func_retval0+0], %fd38;
	ret;
}

.visible .func  (.param .align 8 .b8 func_retval0[8]) ___mincc(
	.param .align 8 .b8 ___mincc_param_0[8],
	.param .align 8 .b8 ___mincc_param_1[8]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;


	ld.param.f32 	%f1, [___mincc_param_0+4];
	ld.param.f32 	%f2, [___mincc_param_0];
	ld.param.f32 	%f3, [___mincc_param_1+4];
	ld.param.f32 	%f4, [___mincc_param_1];
	mul.f32 	%f5, %f2, %f2;
	mul.f32 	%f6, %f4, %f4;
	setp.lt.f32	%p1, %f5, %f6;
	selp.f32	%f7, %f1, %f3, %p1;
	selp.f32	%f8, %f2, %f4, %p1;
	st.param.f32	[func_retval0+0], %f8;
	st.param.f32	[func_retval0+4], %f7;
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) ___minzz(
	.param .align 16 .b8 ___minzz_param_0[16],
	.param .align 16 .b8 ___minzz_param_1[16]
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<9>;


	ld.param.f64 	%fd1, [___minzz_param_0+8];
	ld.param.f64 	%fd2, [___minzz_param_0];
	ld.param.f64 	%fd3, [___minzz_param_1+8];
	ld.param.f64 	%fd4, [___minzz_param_1];
	mul.f64 	%fd5, %fd2, %fd2;
	mul.f64 	%fd6, %fd4, %fd4;
	setp.lt.f64	%p1, %fd5, %fd6;
	selp.f64	%fd7, %fd1, %fd3, %p1;
	selp.f64	%fd8, %fd2, %fd4, %p1;
	st.param.f64	[func_retval0+0], %fd8;
	st.param.f64	[func_retval0+8], %fd7;
	ret;
}

.visible .func  (.param .align 8 .b8 func_retval0[8]) ___maxcc(
	.param .align 8 .b8 ___maxcc_param_0[8],
	.param .align 8 .b8 ___maxcc_param_1[8]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;


	ld.param.f32 	%f1, [___maxcc_param_0+4];
	ld.param.f32 	%f2, [___maxcc_param_0];
	ld.param.f32 	%f3, [___maxcc_param_1+4];
	ld.param.f32 	%f4, [___maxcc_param_1];
	mul.f32 	%f5, %f2, %f2;
	mul.f32 	%f6, %f4, %f4;
	setp.gt.f32	%p1, %f5, %f6;
	selp.f32	%f7, %f1, %f3, %p1;
	selp.f32	%f8, %f2, %f4, %p1;
	st.param.f32	[func_retval0+0], %f8;
	st.param.f32	[func_retval0+4], %f7;
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) ___maxzz(
	.param .align 16 .b8 ___maxzz_param_0[16],
	.param .align 16 .b8 ___maxzz_param_1[16]
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<9>;


	ld.param.f64 	%fd1, [___maxzz_param_0+8];
	ld.param.f64 	%fd2, [___maxzz_param_0];
	ld.param.f64 	%fd3, [___maxzz_param_1+8];
	ld.param.f64 	%fd4, [___maxzz_param_1];
	mul.f64 	%fd5, %fd2, %fd2;
	mul.f64 	%fd6, %fd4, %fd4;
	setp.gt.f64	%p1, %fd5, %fd6;
	selp.f64	%fd7, %fd1, %fd3, %p1;
	selp.f64	%fd8, %fd2, %fd4, %p1;
	st.param.f64	[func_retval0+0], %fd8;
	st.param.f64	[func_retval0+8], %fd7;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .s32 	%r<48>;
	.reg .f32 	%f<3>;
	.reg .f64 	%fd<141>;


	ld.param.f64 	%fd14, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd15, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd14;
	}
	shr.u32 	%r45, %r44, 20;
	setp.ne.s32	%p1, %r45, 0;
	@%p1 bra 	BB63_2;

	mul.f64 	%fd16, %fd14, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd16;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd16;
	}
	shr.u32 	%r18, %r44, 20;
	add.s32 	%r45, %r18, -54;

BB63_2:
	add.s32 	%r46, %r45, -1023;
	and.b32  	%r19, %r44, -2146435073;
	or.b32  	%r20, %r19, 1072693248;
	mov.b64 	%fd138, {%r43, %r20};
	setp.lt.u32	%p2, %r20, 1073127583;
	@%p2 bra 	BB63_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd138;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd138;
	}
	add.s32 	%r23, %r22, -1048576;
	mov.b64 	%fd138, {%r21, %r23};
	add.s32 	%r46, %r45, -1022;

BB63_4:
	add.f64 	%fd18, %fd138, 0d3FF0000000000000;
	mov.f64 	%fd19, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd17,%fd18;
	// inline asm
	neg.f64 	%fd20, %fd18;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd19;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd17, %fd17;
	add.f64 	%fd24, %fd138, 0dBFF0000000000000;
	mul.f64 	%fd25, %fd24, %fd23;
	fma.rn.f64 	%fd26, %fd24, %fd23, %fd25;
	mul.f64 	%fd27, %fd26, %fd26;
	mov.f64 	%fd28, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd29, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3F6249249242B910;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3F89999999999DFB;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	sub.f64 	%fd41, %fd24, %fd26;
	add.f64 	%fd42, %fd41, %fd41;
	neg.f64 	%fd43, %fd26;
	fma.rn.f64 	%fd44, %fd43, %fd24, %fd42;
	mul.f64 	%fd45, %fd23, %fd44;
	fma.rn.f64 	%fd46, %fd40, %fd27, 0d3FB5555555555555;
	mov.f64 	%fd47, 0d3FB5555555555555;
	sub.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd40, %fd27, %fd48;
	add.f64 	%fd50, %fd49, 0d0000000000000000;
	add.f64 	%fd51, %fd50, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd52, %fd46, %fd51;
	sub.f64 	%fd53, %fd46, %fd52;
	add.f64 	%fd54, %fd53, %fd51;
	mul.rn.f64 	%fd55, %fd26, %fd26;
	neg.f64 	%fd56, %fd55;
	fma.rn.f64 	%fd57, %fd26, %fd26, %fd56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd45;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd45;
	}
	add.s32 	%r26, %r25, 1048576;
	mov.b64 	%fd58, {%r24, %r26};
	fma.rn.f64 	%fd59, %fd26, %fd58, %fd57;
	mul.rn.f64 	%fd60, %fd55, %fd26;
	neg.f64 	%fd61, %fd60;
	fma.rn.f64 	%fd62, %fd55, %fd26, %fd61;
	fma.rn.f64 	%fd63, %fd55, %fd45, %fd62;
	fma.rn.f64 	%fd64, %fd59, %fd26, %fd63;
	mul.rn.f64 	%fd65, %fd52, %fd60;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd60, %fd66;
	fma.rn.f64 	%fd68, %fd52, %fd64, %fd67;
	fma.rn.f64 	%fd69, %fd54, %fd60, %fd68;
	add.f64 	%fd70, %fd65, %fd69;
	sub.f64 	%fd71, %fd65, %fd70;
	add.f64 	%fd72, %fd71, %fd69;
	add.f64 	%fd73, %fd26, %fd70;
	sub.f64 	%fd74, %fd26, %fd73;
	add.f64 	%fd75, %fd74, %fd70;
	add.f64 	%fd76, %fd75, %fd72;
	add.f64 	%fd77, %fd76, %fd45;
	add.f64 	%fd78, %fd73, %fd77;
	sub.f64 	%fd79, %fd73, %fd78;
	add.f64 	%fd80, %fd79, %fd77;
	xor.b32  	%r27, %r46, -2147483648;
	mov.u32 	%r28, -2147483648;
	mov.u32 	%r29, 1127219200;
	mov.b64 	%fd81, {%r27, %r29};
	mov.b64 	%fd82, {%r28, %r29};
	sub.f64 	%fd83, %fd81, %fd82;
	mov.f64 	%fd84, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd85, %fd83, %fd84, %fd78;
	neg.f64 	%fd86, %fd83;
	fma.rn.f64 	%fd87, %fd86, %fd84, %fd85;
	sub.f64 	%fd88, %fd87, %fd78;
	sub.f64 	%fd89, %fd80, %fd88;
	mov.f64 	%fd90, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd91, %fd83, %fd90, %fd89;
	add.f64 	%fd92, %fd85, %fd91;
	sub.f64 	%fd93, %fd85, %fd92;
	add.f64 	%fd94, %fd93, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd15;
	}
	add.s32 	%r31, %r30, %r30;
	and.b32  	%r32, %r30, -15728641;
	setp.gt.u32	%p3, %r31, -33554433;
	selp.b32	%r33, %r32, %r30, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd15;
	}
	mov.b64 	%fd95, {%r34, %r33};
	mul.rn.f64 	%fd96, %fd92, %fd95;
	neg.f64 	%fd97, %fd96;
	fma.rn.f64 	%fd98, %fd92, %fd95, %fd97;
	fma.rn.f64 	%fd99, %fd94, %fd95, %fd98;
	add.f64 	%fd4, %fd96, %fd99;
	sub.f64 	%fd100, %fd96, %fd4;
	add.f64 	%fd5, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd4;
	}
	mov.b32 	 %f1, %r13;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p4, %f2, 0f40874911;
	@%p4 bra 	BB63_6;

	setp.lt.s32	%p5, %r13, 0;
	selp.f64	%fd101, 0d0000000000000000, 0d7FF0000000000000, %p5;
	abs.f64 	%fd102, %fd4;
	setp.gtu.f64	%p6, %fd102, 0d7FF0000000000000;
	add.f64 	%fd103, %fd4, %fd4;
	selp.f64	%fd140, %fd103, %fd101, %p6;
	bra.uni 	BB63_10;

BB63_6:
	mov.f64 	%fd104, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd105, %fd4, %fd104;
	mov.f64 	%fd106, 0d4338000000000000;
	add.rn.f64 	%fd107, %fd105, %fd106;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd107;
	}
	mov.f64 	%fd108, 0dC338000000000000;
	add.rn.f64 	%fd109, %fd107, %fd108;
	mov.f64 	%fd110, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd111, %fd109, %fd110, %fd4;
	mov.f64 	%fd112, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd113, %fd109, %fd112, %fd111;
	mov.f64 	%fd114, 0d3E928AF3FCA213EA;
	mov.f64 	%fd115, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd116, %fd115, %fd113, %fd114;
	mov.f64 	%fd117, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd118, %fd116, %fd113, %fd117;
	mov.f64 	%fd119, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd120, %fd118, %fd113, %fd119;
	mov.f64 	%fd121, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd122, %fd120, %fd113, %fd121;
	mov.f64 	%fd123, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd124, %fd122, %fd113, %fd123;
	mov.f64 	%fd125, 0d3F81111111122322;
	fma.rn.f64 	%fd126, %fd124, %fd113, %fd125;
	mov.f64 	%fd127, 0d3FA55555555502A1;
	fma.rn.f64 	%fd128, %fd126, %fd113, %fd127;
	mov.f64 	%fd129, 0d3FC5555555555511;
	fma.rn.f64 	%fd130, %fd128, %fd113, %fd129;
	mov.f64 	%fd131, 0d3FE000000000000B;
	fma.rn.f64 	%fd132, %fd130, %fd113, %fd131;
	fma.rn.f64 	%fd134, %fd132, %fd113, %fd19;
	fma.rn.f64 	%fd139, %fd134, %fd113, %fd19;
	abs.s32 	%r35, %r14;
	setp.lt.s32	%p7, %r35, 1023;
	@%p7 bra 	BB63_8;

	add.s32 	%r36, %r14, 2046;
	shl.b32 	%r37, %r36, 19;
	and.b32  	%r38, %r37, -1048576;
	shl.b32 	%r39, %r36, 20;
	sub.s32 	%r47, %r39, %r38;
	mov.u32 	%r40, 0;
	mov.b64 	%fd135, {%r40, %r38};
	mul.f64 	%fd139, %fd139, %fd135;
	bra.uni 	BB63_9;

BB63_8:
	shl.b32 	%r41, %r14, 20;
	add.s32 	%r47, %r41, 1072693248;

BB63_9:
	mov.u32 	%r42, 0;
	mov.b64 	%fd136, {%r42, %r47};
	mul.f64 	%fd140, %fd139, %fd136;

BB63_10:
	abs.f64 	%fd137, %fd140;
	setp.eq.f64	%p8, %fd137, 0d7FF0000000000000;
	@%p8 bra 	BB63_12;

	fma.rn.f64 	%fd140, %fd140, %fd5, %fd140;

BB63_12:
	st.param.f64	[func_retval0+0], %fd140;
	ret;
}

.func  (.param .b64 func_retval0) __internal_lgamma_pos(
	.param .b64 __internal_lgamma_pos_param_0
)
{
	.reg .pred 	%p<22>;
	.reg .s32 	%r<54>;
	.reg .f64 	%fd<298>;


	ld.param.f64 	%fd24, [__internal_lgamma_pos_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd24;
	}
	setp.gt.s32	%p1, %r50, 1074266111;
	@%p1 bra 	BB64_17;

	setp.gt.s32	%p2, %r50, 1073217535;
	@%p2 bra 	BB64_16;

	setp.gt.s32	%p3, %r50, 1072064101;
	@%p3 bra 	BB64_15;

	mov.f64 	%fd25, 0d3EA7B77CEB0625E8;
	mov.f64 	%fd26, 0dBE7844988BFE6590;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0dBE998C69C8710CC4;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0dBEF6527A5A11CF6E;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F20EC2950B1B5DE;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0dBF2C4D80C24BA278;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0dBF5315B4E8CC0D09;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3F7D917F15D50020;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0dBF83B4ABB41CB6FA;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0dBFA59AF1275B7120;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3FC5512321A168A0;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	mov.f64 	%fd46, 0dBFA5815E8FDCE74C;
	fma.rn.f64 	%fd47, %fd45, %fd24, %fd46;
	mov.f64 	%fd48, 0dBFE4FCF4026ADD1A;
	fma.rn.f64 	%fd49, %fd47, %fd24, %fd48;
	mov.f64 	%fd50, 0d3FE2788CFC6FB5C8;
	fma.rn.f64 	%fd51, %fd49, %fd24, %fd50;
	mul.f64 	%fd52, %fd51, %fd24;
	fma.rn.f64 	%fd1, %fd52, %fd24, %fd24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd1;
	}
	setp.gt.f64	%p4, %fd1, 0d0000000000000000;
	setp.lt.s32	%p5, %r46, 2146435072;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB64_9;

	abs.f64 	%fd53, %fd1;
	setp.gtu.f64	%p7, %fd53, 0d7FF0000000000000;
	@%p7 bra 	BB64_8;

	setp.neu.f64	%p8, %fd1, 0d0000000000000000;
	@%p8 bra 	BB64_7;

	mov.f64 	%fd54, 0dFFF0000000000000;
	neg.f64 	%fd297, %fd54;
	bra.uni 	BB64_32;

BB64_7:
	setp.eq.f64	%p9, %fd1, 0d7FF0000000000000;
	selp.f64	%fd2, %fd1, 0dFFF8000000000000, %p9;
	neg.f64 	%fd297, %fd2;
	bra.uni 	BB64_32;

BB64_8:
	add.f64 	%fd3, %fd1, %fd1;
	neg.f64 	%fd297, %fd3;
	bra.uni 	BB64_32;

BB64_9:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd1;
	}
	setp.lt.s32	%p10, %r46, 1048576;
	@%p10 bra 	BB64_11;

	mov.u32 	%r48, -1023;
	bra.uni 	BB64_12;

BB64_11:
	mul.f64 	%fd55, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd55;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd55;
	}
	mov.u32 	%r48, -1077;

BB64_12:
	shr.u32 	%r23, %r46, 20;
	add.s32 	%r49, %r48, %r23;
	and.b32  	%r24, %r46, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd294, {%r47, %r25};
	setp.lt.s32	%p11, %r25, 1073127583;
	@%p11 bra 	BB64_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd294;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd294;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd294, {%r26, %r28};
	add.s32 	%r49, %r49, 1;

BB64_14:
	add.f64 	%fd57, %fd294, 0d3FF0000000000000;
	mov.f64 	%fd58, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd56,%fd57;
	// inline asm
	neg.f64 	%fd59, %fd57;
	fma.rn.f64 	%fd60, %fd59, %fd56, %fd58;
	fma.rn.f64 	%fd61, %fd60, %fd60, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd56, %fd56;
	add.f64 	%fd63, %fd294, 0dBFF0000000000000;
	mul.f64 	%fd64, %fd63, %fd62;
	fma.rn.f64 	%fd65, %fd63, %fd62, %fd64;
	mul.f64 	%fd66, %fd65, %fd65;
	mov.f64 	%fd67, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd68, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	mov.f64 	%fd70, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd71, %fd69, %fd66, %fd70;
	mov.f64 	%fd72, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd73, %fd71, %fd66, %fd72;
	mov.f64 	%fd74, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd75, %fd73, %fd66, %fd74;
	mov.f64 	%fd76, 0d3F624924923BE72D;
	fma.rn.f64 	%fd77, %fd75, %fd66, %fd76;
	mov.f64 	%fd78, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd79, %fd77, %fd66, %fd78;
	mov.f64 	%fd80, 0d3FB5555555555554;
	fma.rn.f64 	%fd81, %fd79, %fd66, %fd80;
	sub.f64 	%fd82, %fd63, %fd65;
	add.f64 	%fd83, %fd82, %fd82;
	neg.f64 	%fd84, %fd65;
	fma.rn.f64 	%fd85, %fd84, %fd63, %fd83;
	mul.f64 	%fd86, %fd62, %fd85;
	mul.f64 	%fd87, %fd81, %fd66;
	fma.rn.f64 	%fd88, %fd87, %fd65, %fd86;
	xor.b32  	%r29, %r49, -2147483648;
	mov.u32 	%r30, -2147483648;
	mov.u32 	%r31, 1127219200;
	mov.b64 	%fd89, {%r29, %r31};
	mov.b64 	%fd90, {%r30, %r31};
	sub.f64 	%fd91, %fd89, %fd90;
	mov.f64 	%fd92, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd93, %fd91, %fd92, %fd65;
	neg.f64 	%fd94, %fd91;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	sub.f64 	%fd96, %fd95, %fd65;
	sub.f64 	%fd97, %fd88, %fd96;
	mov.f64 	%fd98, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd99, %fd91, %fd98, %fd97;
	add.f64 	%fd7, %fd93, %fd99;
	neg.f64 	%fd297, %fd7;
	bra.uni 	BB64_32;

BB64_15:
	mov.f64 	%fd100, 0d3FF0000000000000;
	sub.f64 	%fd101, %fd100, %fd24;
	mov.f64 	%fd102, 0d3FA3EB504359EB88;
	mov.f64 	%fd103, 0d3F881F6D2A4C4310;
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0d3FAE35D8DEB06317;
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3FAED469A8B6ECCE;
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0d3FACC1B1C357BEFE;
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3FAD7154DB67F79F;
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0d3FAFCC622CF2F7BB;
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FB11747A4D1CC43;
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0d3FB24CE16A21B8AC;
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FB3B1C21A7BCB00;
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0d3FB556723452ED57;
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	mov.f64 	%fd123, 0d3FB748C00891544F;
	fma.rn.f64 	%fd124, %fd122, %fd101, %fd123;
	mov.f64 	%fd125, 0d3FB9A0207808CF40;
	fma.rn.f64 	%fd126, %fd124, %fd101, %fd125;
	mov.f64 	%fd127, 0d3FBC80673B8AE26B;
	fma.rn.f64 	%fd128, %fd126, %fd101, %fd127;
	mov.f64 	%fd129, 0d3FC010B364B7E555;
	fma.rn.f64 	%fd130, %fd128, %fd101, %fd129;
	mov.f64 	%fd131, 0d3FC2703A1D239658;
	fma.rn.f64 	%fd132, %fd130, %fd101, %fd131;
	mov.f64 	%fd133, 0d3FC5B40CB1137E6E;
	fma.rn.f64 	%fd134, %fd132, %fd101, %fd133;
	mov.f64 	%fd135, 0d3FCA8B9C17AC4F03;
	fma.rn.f64 	%fd136, %fd134, %fd101, %fd135;
	mov.f64 	%fd137, 0d3FD151322AC7CB52;
	fma.rn.f64 	%fd138, %fd136, %fd101, %fd137;
	mov.f64 	%fd139, 0d3FD9A4D55BEAB1D4;
	fma.rn.f64 	%fd140, %fd138, %fd101, %fd139;
	mov.f64 	%fd141, 0d3FEA51A6625307D6;
	fma.rn.f64 	%fd142, %fd140, %fd101, %fd141;
	mov.f64 	%fd143, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd144, %fd142, %fd101, %fd143;
	mul.f64 	%fd297, %fd144, %fd101;
	bra.uni 	BB64_32;

BB64_16:
	add.f64 	%fd145, %fd24, 0dC000000000000000;
	mov.f64 	%fd146, 0dBE71FA71D78C0EE2;
	mov.f64 	%fd147, 0d3E452636124338B3;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	mov.f64 	%fd149, 0d3E8D111F31E61306;
	fma.rn.f64 	%fd150, %fd148, %fd145, %fd149;
	mov.f64 	%fd151, 0dBEA0502BBE1B2706;
	fma.rn.f64 	%fd152, %fd150, %fd145, %fd151;
	mov.f64 	%fd153, 0d3EB06850B2970292;
	fma.rn.f64 	%fd154, %fd152, %fd145, %fd153;
	mov.f64 	%fd155, 0dBEC108474875033D;
	fma.rn.f64 	%fd156, %fd154, %fd145, %fd155;
	mov.f64 	%fd157, 0d3ED24ACCC62909DC;
	fma.rn.f64 	%fd158, %fd156, %fd145, %fd157;
	mov.f64 	%fd159, 0dBEE3CB25209E63BE;
	fma.rn.f64 	%fd160, %fd158, %fd145, %fd159;
	mov.f64 	%fd161, 0d3EF581CBBC8CDC7B;
	fma.rn.f64 	%fd162, %fd160, %fd145, %fd161;
	mov.f64 	%fd163, 0dBF078E04B85C7597;
	fma.rn.f64 	%fd164, %fd162, %fd145, %fd163;
	mov.f64 	%fd165, 0d3F1A12730CF45051;
	fma.rn.f64 	%fd166, %fd164, %fd145, %fd165;
	mov.f64 	%fd167, 0dBF2D3FD354062012;
	fma.rn.f64 	%fd168, %fd166, %fd145, %fd167;
	mov.f64 	%fd169, 0d3F40B36B0B4DE323;
	fma.rn.f64 	%fd170, %fd168, %fd145, %fd169;
	mov.f64 	%fd171, 0dBF538AC5C6D0317A;
	fma.rn.f64 	%fd172, %fd170, %fd145, %fd171;
	mov.f64 	%fd173, 0d3F67ADD6EAAB19FC;
	fma.rn.f64 	%fd174, %fd172, %fd145, %fd173;
	mov.f64 	%fd175, 0dBF7E404FC20E4D5B;
	fma.rn.f64 	%fd176, %fd174, %fd145, %fd175;
	mov.f64 	%fd177, 0d3F951322AC7DA390;
	fma.rn.f64 	%fd178, %fd176, %fd145, %fd177;
	mov.f64 	%fd179, 0dBFB13E001A5578A3;
	fma.rn.f64 	%fd180, %fd178, %fd145, %fd179;
	mov.f64 	%fd181, 0d3FD4A34CC4A60FA3;
	fma.rn.f64 	%fd182, %fd180, %fd145, %fd181;
	mov.f64 	%fd183, 0d3FDB0EE6072093CF;
	fma.rn.f64 	%fd184, %fd182, %fd145, %fd183;
	mul.f64 	%fd297, %fd184, %fd145;
	bra.uni 	BB64_32;

BB64_17:
	setp.gt.s32	%p12, %r50, 1075838975;
	@%p12 bra 	BB64_19;

	add.f64 	%fd185, %fd24, 0dC008000000000000;
	mov.f64 	%fd186, 0dC1122B7730207EF3;
	mov.f64 	%fd187, 0dC0AF7040BB18FB05;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0dC1585A0DB81DE7D0;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0dC18A992B8BA94677;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0dC1AAC5CB6957CC20;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0dC1BC0E2B308774BE;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0dC1C6BA13DCAE7F67;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0dC1CCF33B9C3D120C;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	add.f64 	%fd201, %fd185, 0dC08FF62E0BE189FE;
	mov.f64 	%fd202, 0dC10074FACE10C93F;
	fma.rn.f64 	%fd203, %fd201, %fd185, %fd202;
	mov.f64 	%fd204, 0dC151B662F8D75791;
	fma.rn.f64 	%fd205, %fd203, %fd185, %fd204;
	mov.f64 	%fd206, 0dC18EE64AB4D207F7;
	fma.rn.f64 	%fd207, %fd205, %fd185, %fd206;
	mov.f64 	%fd208, 0dC1B9051687C9951A;
	fma.rn.f64 	%fd209, %fd207, %fd185, %fd208;
	mov.f64 	%fd210, 0dC1D2B866BF0B853D;
	fma.rn.f64 	%fd211, %fd209, %fd185, %fd210;
	mov.f64 	%fd212, 0dC1D4E2130E9DC133;
	fma.rn.f64 	%fd213, %fd211, %fd185, %fd212;
	div.rn.f64 	%fd214, %fd200, %fd213;
	add.f64 	%fd297, %fd214, %fd185;
	bra.uni 	BB64_32;

BB64_19:
	// inline asm
	rcp.approx.ftz.f64 %fd215,%fd24;
	// inline asm
	neg.f64 	%fd13, %fd24;
	mov.f64 	%fd217, 0d3FF0000000000000;
	fma.rn.f64 	%fd218, %fd13, %fd215, %fd217;
	fma.rn.f64 	%fd219, %fd218, %fd218, %fd218;
	fma.rn.f64 	%fd220, %fd219, %fd215, %fd215;
	mul.f64 	%fd221, %fd220, %fd220;
	mov.f64 	%fd222, 0d3F4B68B992738FBF;
	mov.f64 	%fd223, 0dBF5AC321034783F9;
	fma.rn.f64 	%fd224, %fd223, %fd221, %fd222;
	mov.f64 	%fd225, 0dBF4380D01E4F7B8C;
	fma.rn.f64 	%fd226, %fd224, %fd221, %fd225;
	mov.f64 	%fd227, 0d3F4A019FA29F7264;
	fma.rn.f64 	%fd228, %fd226, %fd221, %fd227;
	mov.f64 	%fd229, 0dBF66C16C16B2ACEC;
	fma.rn.f64 	%fd230, %fd228, %fd221, %fd229;
	mov.f64 	%fd231, 0d3FB5555555555545;
	fma.rn.f64 	%fd232, %fd230, %fd221, %fd231;
	mov.f64 	%fd233, 0d3FED67F1C864BEAE;
	fma.rn.f64 	%fd14, %fd232, %fd220, %fd233;
	setp.lt.s32	%p13, %r50, 2146435072;
	setp.gt.f64	%p14, %fd24, 0d0000000000000000;
	and.pred  	%p15, %p14, %p13;
	@%p15 bra 	BB64_25;

	abs.f64 	%fd234, %fd24;
	setp.gtu.f64	%p16, %fd234, 0d7FF0000000000000;
	@%p16 bra 	BB64_24;

	setp.neu.f64	%p17, %fd24, 0d0000000000000000;
	@%p17 bra 	BB64_23;

	mov.f64 	%fd296, 0dFFF0000000000000;
	bra.uni 	BB64_31;

BB64_23:
	setp.eq.f64	%p18, %fd24, 0d7FF0000000000000;
	selp.f64	%fd296, %fd24, 0dFFF8000000000000, %p18;
	bra.uni 	BB64_31;

BB64_24:
	add.f64 	%fd296, %fd24, %fd24;
	bra.uni 	BB64_31;

BB64_25:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd24;
	}
	setp.lt.s32	%p19, %r50, 1048576;
	@%p19 bra 	BB64_27;

	mov.u32 	%r52, -1023;
	bra.uni 	BB64_28;

BB64_27:
	mul.f64 	%fd236, %fd24, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd236;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd236;
	}
	mov.u32 	%r52, -1077;

BB64_28:
	shr.u32 	%r34, %r50, 20;
	add.s32 	%r53, %r52, %r34;
	and.b32  	%r35, %r50, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd295, {%r51, %r36};
	setp.lt.s32	%p20, %r36, 1073127583;
	@%p20 bra 	BB64_30;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd295;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd295;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd295, {%r37, %r39};
	add.s32 	%r53, %r53, 1;

BB64_30:
	add.f64 	%fd238, %fd295, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd237,%fd238;
	// inline asm
	neg.f64 	%fd240, %fd238;
	fma.rn.f64 	%fd241, %fd240, %fd237, %fd217;
	fma.rn.f64 	%fd242, %fd241, %fd241, %fd241;
	fma.rn.f64 	%fd243, %fd242, %fd237, %fd237;
	add.f64 	%fd244, %fd295, 0dBFF0000000000000;
	mul.f64 	%fd245, %fd244, %fd243;
	fma.rn.f64 	%fd246, %fd244, %fd243, %fd245;
	mul.f64 	%fd247, %fd246, %fd246;
	mov.f64 	%fd248, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd249, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd250, %fd249, %fd247, %fd248;
	mov.f64 	%fd251, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd252, %fd250, %fd247, %fd251;
	mov.f64 	%fd253, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd254, %fd252, %fd247, %fd253;
	mov.f64 	%fd255, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd256, %fd254, %fd247, %fd255;
	mov.f64 	%fd257, 0d3F624924923BE72D;
	fma.rn.f64 	%fd258, %fd256, %fd247, %fd257;
	mov.f64 	%fd259, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd260, %fd258, %fd247, %fd259;
	mov.f64 	%fd261, 0d3FB5555555555554;
	fma.rn.f64 	%fd262, %fd260, %fd247, %fd261;
	sub.f64 	%fd263, %fd244, %fd246;
	add.f64 	%fd264, %fd263, %fd263;
	neg.f64 	%fd265, %fd246;
	fma.rn.f64 	%fd266, %fd265, %fd244, %fd264;
	mul.f64 	%fd267, %fd243, %fd266;
	mul.f64 	%fd268, %fd262, %fd247;
	fma.rn.f64 	%fd269, %fd268, %fd246, %fd267;
	xor.b32  	%r40, %r53, -2147483648;
	mov.u32 	%r41, -2147483648;
	mov.u32 	%r42, 1127219200;
	mov.b64 	%fd270, {%r40, %r42};
	mov.b64 	%fd271, {%r41, %r42};
	sub.f64 	%fd272, %fd270, %fd271;
	mov.f64 	%fd273, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd246;
	neg.f64 	%fd275, %fd272;
	fma.rn.f64 	%fd276, %fd275, %fd273, %fd274;
	sub.f64 	%fd277, %fd276, %fd246;
	sub.f64 	%fd278, %fd269, %fd277;
	mov.f64 	%fd279, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd280, %fd272, %fd279, %fd278;
	add.f64 	%fd296, %fd274, %fd280;

BB64_31:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd296;
	}
	add.s32 	%r44, %r43, -1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd296;
	}
	mov.b64 	%fd281, {%r45, %r44};
	add.f64 	%fd282, %fd24, 0dBFE0000000000000;
	fma.rn.f64 	%fd283, %fd281, %fd282, %fd14;
	fma.rn.f64 	%fd284, %fd281, %fd282, %fd13;
	add.f64 	%fd285, %fd284, %fd283;
	setp.eq.f64	%p21, %fd24, 0d7FF0000000000000;
	selp.f64	%fd297, %fd24, %fd285, %p21;

BB64_32:
	st.param.f64	[func_retval0+0], %fd297;
	ret;
}


