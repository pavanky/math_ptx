//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Jul 31 22:29:38 2014 (1406860178)
// Cuda compilation tools, release 6.5, V6.5.14
//

.version 4.1
.target sm_35
.address_size 64

.func  (.param .b64 funj_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.func  (.param .b64 funj_retval0) __internal_lgamma_pos
(
	.param .b64 __internal_lgamma_pos_param_0
)
;

.weak .func  (.param .b32 funj_retval0) cudaMalloj(
	.param .b64 cudaMalloj_param_0,
	.param .b64 cudaMalloj_param_1
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 funj_retval0) cudaFuncGetAttributes(
	.param .b64 cudaFuncGetAttributes_param_0,
	.param .b64 cudaFuncGetAttributes_param_1
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 funj_retval0) cudaDeviceGetAttribute(
	.param .b64 cudaDeviceGetAttribute_param_0,
	.param .b32 cudaDeviceGetAttribute_param_1,
	.param .b32 cudaDeviceGetAttribute_param_2
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 funj_retval0) cudaGetDevice(
	.param .b64 cudaGetDevice_param_0
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 funj_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessor(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_3
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floors(
	.param .b32 ___floors_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___floors_param_0];
	cvt.rmi.f32.f32	%f2, %f1;
	st.param.f32	[funj_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___floord(
	.param .b64 ___floord_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___floord_param_0];
	cvt.rmi.f64.f64	%fd2, %fd1;
	st.param.f64	[funj_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floori(
	.param .b32 ___floori_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___floori_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___flooru(
	.param .b32 ___flooru_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___flooru_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floorj(
	.param .b32 ___floorj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.s8 	%rs1, [___floorj_param_0];
	cvt.s32.s16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floorv(
	.param .b32 ___floorv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___floorv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceils(
	.param .b32 ___ceils_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___ceils_param_0];
	cvt.rpi.f32.f32	%f2, %f1;
	st.param.f32	[funj_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___ceild(
	.param .b64 ___ceild_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___ceild_param_0];
	cvt.rpi.f64.f64	%fd2, %fd1;
	st.param.f64	[funj_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceili(
	.param .b32 ___ceili_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___ceili_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceilu(
	.param .b32 ___ceilu_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___ceilu_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceilj(
	.param .b32 ___ceilj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.s8 	%rs1, [___ceilj_param_0];
	cvt.s32.s16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceilv(
	.param .b32 ___ceilv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___ceilv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___rounds(
	.param .b32 ___rounds_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<4>;
	.reg .f32 	%f<10>;


	ld.param.f32 	%f4, [___rounds_param_0];
	abs.f32 	%f5, %f4;
	mov.b32 	 %r1, %f4;
	and.b32  	%r2, %r1, -2147483648;
	or.b32  	%r3, %r2, 1056964608;
	mov.b32 	 %f6, %r3;
	add.f32 	%f7, %f6, %f4;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f4, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB17_2;

	cvt.rzi.f32.f32	%f9, %f4;

BB17_2:
	st.param.f32	[funj_retval0+0], %f9;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___roundd(
	.param .b64 ___roundd_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f64 	%fd<9>;


	ld.param.f64 	%fd8, [___roundd_param_0];
	abs.f64 	%fd1, %fd8;
	setp.ge.f64	%p1, %fd1, 0d4330000000000000;
	@%p1 bra 	BB18_2;

	add.f64 	%fd5, %fd1, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd6, %fd5;
	setp.lt.f64	%p2, %fd1, 0d3FE0000000000000;
	selp.f64	%fd7, 0d0000000000000000, %fd6, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1, %temp}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd8;
	}
	and.b32  	%r4, %r3, -2147483648;
	or.b32  	%r5, %r2, %r4;
	mov.b64 	%fd8, {%r1, %r5};

BB18_2:
	st.param.f64	[funj_retval0+0], %fd8;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundi(
	.param .b32 ___roundi_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___roundi_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundu(
	.param .b32 ___roundu_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___roundu_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundj(
	.param .b32 ___roundj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.s8 	%rs1, [___roundj_param_0];
	cvt.s32.s16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundv(
	.param .b32 ___roundv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___roundv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___abss(
	.param .b32 ___abss_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___abss_param_0];
	abs.f32 	%f2, %f1;
	st.param.f32	[funj_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___absd(
	.param .b64 ___absd_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___absd_param_0];
	abs.f64 	%fd2, %fd1;
	st.param.f64	[funj_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absi(
	.param .b32 ___absi_param_0
)
{
	.reg .s32 	%r<3>;


	ld.param.u32 	%r1, [___absi_param_0];
	abs.s32 	%r2, %r1;
	st.param.b32	[funj_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absj(
	.param .b32 ___absj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<4>;


	ld.param.s8 	%rs1, [___absj_param_0];
	cvt.s32.s16	%r1, %rs1;
	abs.s32 	%r2, %r1;
	cvt.s32.s8 	%r3, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absu(
	.param .b32 ___absu_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___absu_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absv(
	.param .b32 ___absv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___absv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammas(
	.param .b32 ___tgammas_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<99>;


	ld.param.f32 	%f24, [___tgammas_param_0];
	setp.ltu.f32	%p1, %f24, 0f00000000;
	@%p1 bra 	BB29_5;

	setp.gt.f32	%p2, %f24, 0f42100000;
	selp.f32	%f1, 0f42100000, %f24, %p2;
	setp.gt.f32	%p3, %f1, 0f42081EB8;
	add.f32 	%f2, %f1, 0fBF800000;
	selp.f32	%f86, %f2, %f1, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB29_2;
	bra.uni 	BB29_4;

BB29_2:
	mov.f32 	%f88, %f87;

BB29_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB29_3;

BB29_4:
	setp.ltu.f32	%p6, %f1, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f1;
	setp.lt.f32	%p7, %f1, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f2;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB29_12;

BB29_5:
	cvt.rmi.f32.f32	%f49, %f24;
	setp.eq.f32	%p9, %f49, %f24;
	selp.f32	%f50, 0f7FFFFFFF, %f24, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f13, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f13, 0fC2081EB8;
	add.f32 	%f51, %f13, 0f40C00000;
	selp.f32	%f95, %f51, %f13, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB29_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB29_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB29_7;

BB29_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f13, 0fC2081EB8;
	@%p14 bra 	BB29_12;

	add.f32 	%f70, %f13, 0f3F800000;
	mul.f32 	%f71, %f13, %f70;
	add.f32 	%f72, %f13, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f13, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f13, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f13, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f24, 0fC2280000;
	@%p15 bra 	BB29_12;

	cvt.rzi.s32.f32	%r1, %f24;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32	%p16, %r2, 1;
	@%p16 bra 	BB29_12;

	mov.f32 	%f98, 0f80000000;

BB29_12:
	st.param.f32	[funj_retval0+0], %f98;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___tgammad(
	.param .b64 ___tgammad_param_0
)
{
	.reg .pred 	%p<30>;
	.reg .s32 	%r<72>;
	.reg .f32 	%f<3>;
	.reg .s64 	%rd<3>;
	.reg .f64 	%fd<429>;


	ld.param.f64 	%fd52, [___tgammad_param_0];
	setp.ltu.f64	%p1, %fd52, 0d0000000000000000;
	@%p1 bra 	BB30_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd52;
	}
	setp.lt.s32	%p2, %r67, 1076756480;
	@%p2 bra 	BB30_17;

	setp.lt.f64	%p3, %fd52, 0d406573FAE561F648;
	@%p3 bra 	BB30_4;

	mov.f64 	%fd428, 0d7FF0000000000000;
	bra.uni 	BB30_45;

BB30_4:
	// inline asm
	rcp.approx.ftz.f64 %fd54,%fd52;
	// inline asm
	neg.f64 	%fd56, %fd52;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd54, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd54, %fd54;
	mov.f64 	%fd61, 0d3F4B8239C670E690;
	mov.f64 	%fd62, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0dBF0B1D75D3346711;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0dBF436773BDB97B48;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3F1247604839C038;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0d3F49B0FF6874F2C4;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0dBF2E13CE465FA859;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0dBF65F7268EDAB4C8;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3F6C71C71C71C71C;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	mov.f64 	%fd78, 0d3FB5555555555555;
	fma.rn.f64 	%fd79, %fd77, %fd60, %fd78;
	fma.rn.f64 	%fd1, %fd79, %fd60, %fd57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd52;
	}
	shr.u32 	%r68, %r67, 20;
	setp.ne.s32	%p4, %r68, 0;
	@%p4 bra 	BB30_6;

	mul.f64 	%fd80, %fd52, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd80;
	}
	shr.u32 	%r24, %r67, 20;
	add.s32 	%r68, %r24, -54;

BB30_6:
	add.s32 	%r69, %r68, -1023;
	and.b32  	%r25, %r67, -2146435073;
	or.b32  	%r26, %r25, 1072693248;
	mov.b64 	%fd400, {%r66, %r26};
	setp.lt.u32	%p5, %r26, 1073127583;
	@%p5 bra 	BB30_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd400;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd400;
	}
	add.s32 	%r29, %r28, -1048576;
	mov.b64 	%fd400, {%r27, %r29};
	add.s32 	%r69, %r68, -1022;

BB30_8:
	add.f64 	%fd83, %fd52, 0dBFE0000000000000;
	add.f64 	%fd82, %fd400, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd81,%fd82;
	// inline asm
	neg.f64 	%fd85, %fd82;
	fma.rn.f64 	%fd86, %fd85, %fd81, %fd57;
	fma.rn.f64 	%fd87, %fd86, %fd86, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd81, %fd81;
	add.f64 	%fd89, %fd400, 0dBFF0000000000000;
	mul.f64 	%fd90, %fd89, %fd88;
	fma.rn.f64 	%fd91, %fd89, %fd88, %fd90;
	mul.f64 	%fd92, %fd91, %fd91;
	mov.f64 	%fd93, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd94, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F6249249242B910;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F89999999999DFB;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	sub.f64 	%fd106, %fd89, %fd91;
	add.f64 	%fd107, %fd106, %fd106;
	neg.f64 	%fd108, %fd91;
	fma.rn.f64 	%fd109, %fd108, %fd89, %fd107;
	mul.f64 	%fd110, %fd88, %fd109;
	fma.rn.f64 	%fd111, %fd105, %fd92, 0d3FB5555555555555;
	sub.f64 	%fd113, %fd78, %fd111;
	fma.rn.f64 	%fd114, %fd105, %fd92, %fd113;
	add.f64 	%fd115, %fd114, 0d0000000000000000;
	add.f64 	%fd116, %fd115, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd117, %fd111, %fd116;
	sub.f64 	%fd118, %fd111, %fd117;
	add.f64 	%fd119, %fd118, %fd116;
	mul.rn.f64 	%fd120, %fd91, %fd91;
	neg.f64 	%fd121, %fd120;
	fma.rn.f64 	%fd122, %fd91, %fd91, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd110;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd110;
	}
	add.s32 	%r32, %r31, 1048576;
	mov.b64 	%fd123, {%r30, %r32};
	fma.rn.f64 	%fd124, %fd91, %fd123, %fd122;
	mul.rn.f64 	%fd125, %fd120, %fd91;
	neg.f64 	%fd126, %fd125;
	fma.rn.f64 	%fd127, %fd120, %fd91, %fd126;
	fma.rn.f64 	%fd128, %fd120, %fd110, %fd127;
	fma.rn.f64 	%fd129, %fd124, %fd91, %fd128;
	mul.rn.f64 	%fd130, %fd117, %fd125;
	neg.f64 	%fd131, %fd130;
	fma.rn.f64 	%fd132, %fd117, %fd125, %fd131;
	fma.rn.f64 	%fd133, %fd117, %fd129, %fd132;
	fma.rn.f64 	%fd134, %fd119, %fd125, %fd133;
	add.f64 	%fd135, %fd130, %fd134;
	sub.f64 	%fd136, %fd130, %fd135;
	add.f64 	%fd137, %fd136, %fd134;
	add.f64 	%fd138, %fd91, %fd135;
	sub.f64 	%fd139, %fd91, %fd138;
	add.f64 	%fd140, %fd139, %fd135;
	add.f64 	%fd141, %fd140, %fd137;
	add.f64 	%fd142, %fd141, %fd110;
	add.f64 	%fd143, %fd138, %fd142;
	sub.f64 	%fd144, %fd138, %fd143;
	add.f64 	%fd145, %fd144, %fd142;
	xor.b32  	%r33, %r69, -2147483648;
	mov.u32 	%r34, -2147483648;
	mov.u32 	%r35, 1127219200;
	mov.b64 	%fd146, {%r33, %r35};
	mov.b64 	%fd147, {%r34, %r35};
	sub.f64 	%fd148, %fd146, %fd147;
	mov.f64 	%fd149, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd150, %fd148, %fd149, %fd143;
	neg.f64 	%fd151, %fd148;
	fma.rn.f64 	%fd152, %fd151, %fd149, %fd150;
	sub.f64 	%fd153, %fd152, %fd143;
	sub.f64 	%fd154, %fd145, %fd153;
	mov.f64 	%fd155, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd156, %fd148, %fd155, %fd154;
	add.f64 	%fd157, %fd150, %fd156;
	sub.f64 	%fd158, %fd150, %fd157;
	add.f64 	%fd159, %fd158, %fd156;
	mul.rn.f64 	%fd160, %fd157, %fd83;
	neg.f64 	%fd161, %fd160;
	fma.rn.f64 	%fd162, %fd157, %fd83, %fd161;
	fma.rn.f64 	%fd163, %fd159, %fd83, %fd162;
	add.f64 	%fd164, %fd160, %fd163;
	sub.f64 	%fd165, %fd160, %fd164;
	add.f64 	%fd166, %fd165, %fd163;
	sub.f64 	%fd167, %fd164, %fd52;
	sub.f64 	%fd168, %fd164, %fd167;
	sub.f64 	%fd169, %fd168, %fd52;
	add.f64 	%fd170, %fd169, 0d0000000000000000;
	add.f64 	%fd171, %fd170, %fd166;
	add.f64 	%fd5, %fd167, %fd171;
	sub.f64 	%fd172, %fd167, %fd5;
	add.f64 	%fd6, %fd172, %fd171;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd5;
	}
	mov.b32 	 %f1, %r13;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p6, %f2, 0f40874911;
	@%p6 bra 	BB30_10;

	setp.lt.s32	%p7, %r13, 0;
	selp.f64	%fd173, 0d0000000000000000, 0d7FF0000000000000, %p7;
	abs.f64 	%fd174, %fd5;
	setp.gtu.f64	%p8, %fd174, 0d7FF0000000000000;
	add.f64 	%fd175, %fd5, %fd5;
	selp.f64	%fd402, %fd175, %fd173, %p8;
	bra.uni 	BB30_14;

BB30_10:
	mov.f64 	%fd399, 0d3FF0000000000000;
	mov.f64 	%fd176, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd177, %fd5, %fd176;
	mov.f64 	%fd178, 0d4338000000000000;
	add.rn.f64 	%fd179, %fd177, %fd178;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd179;
	}
	mov.f64 	%fd180, 0dC338000000000000;
	add.rn.f64 	%fd181, %fd179, %fd180;
	mov.f64 	%fd182, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd183, %fd181, %fd182, %fd5;
	mov.f64 	%fd184, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd185, %fd181, %fd184, %fd183;
	mov.f64 	%fd186, 0d3E928AF3FCA213EA;
	mov.f64 	%fd187, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0d3F81111111122322;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0d3FA55555555502A1;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	mov.f64 	%fd201, 0d3FC5555555555511;
	fma.rn.f64 	%fd202, %fd200, %fd185, %fd201;
	mov.f64 	%fd203, 0d3FE000000000000B;
	fma.rn.f64 	%fd204, %fd202, %fd185, %fd203;
	fma.rn.f64 	%fd206, %fd204, %fd185, %fd399;
	fma.rn.f64 	%fd401, %fd206, %fd185, %fd399;
	abs.s32 	%r36, %r14;
	setp.lt.s32	%p9, %r36, 1023;
	@%p9 bra 	BB30_12;

	add.s32 	%r37, %r14, 2046;
	shl.b32 	%r38, %r37, 19;
	and.b32  	%r39, %r38, -1048576;
	shl.b32 	%r40, %r37, 20;
	sub.s32 	%r70, %r40, %r39;
	mov.u32 	%r41, 0;
	mov.b64 	%fd207, {%r41, %r39};
	mul.f64 	%fd401, %fd401, %fd207;
	bra.uni 	BB30_13;

BB30_12:
	shl.b32 	%r42, %r14, 20;
	add.s32 	%r70, %r42, 1072693248;

BB30_13:
	mov.u32 	%r43, 0;
	mov.b64 	%fd208, {%r43, %r70};
	mul.f64 	%fd402, %fd401, %fd208;

BB30_14:
	abs.f64 	%fd209, %fd402;
	setp.eq.f64	%p10, %fd209, 0d7FF0000000000000;
	@%p10 bra 	BB30_16;

	fma.rn.f64 	%fd402, %fd402, %fd6, %fd402;

BB30_16:
	mul.f64 	%fd210, %fd402, 0dBCAA6A0D6F814637;
	mov.f64 	%fd211, 0d40040D931FF62706;
	fma.rn.f64 	%fd212, %fd402, %fd211, %fd210;
	mul.f64 	%fd428, %fd212, %fd1;
	bra.uni 	BB30_45;

BB30_17:
	setp.gt.s32	%p11, %r67, 1073217535;
	add.f64 	%fd425, %fd52, 0dBFF0000000000000;
	mov.f64 	%fd403, 0d3FF0000000000000;
	@%p11 bra 	BB30_19;

	mov.f64 	%fd424, %fd52;
	bra.uni 	BB30_21;

BB30_19:
	mov.f64 	%fd427, %fd425;
	mov.f64 	%fd426, %fd52;

BB30_20:
	mov.f64 	%fd19, %fd426;
	mov.f64 	%fd426, %fd427;
	neg.f64 	%fd215, %fd403;
	fma.rn.f64 	%fd403, %fd403, %fd19, %fd215;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd426;
	}
	setp.gt.s32	%p12, %r44, 1073217535;
	add.f64 	%fd427, %fd426, 0dBFF0000000000000;
	mov.f64 	%fd425, %fd427;
	mov.f64 	%fd424, %fd426;
	@%p12 bra 	BB30_20;

BB30_21:
	setp.gt.s32	%p13, %r67, 1071644671;
	selp.f64	%fd216, %fd425, %fd424, %p13;
	mov.f64 	%fd217, 0dBE8B338C457183B6;
	mov.f64 	%fd218, 0dBDFE6BDF8CC487CD;
	fma.rn.f64 	%fd219, %fd218, %fd216, %fd217;
	mov.f64 	%fd220, 0d3EB31831766A0388;
	fma.rn.f64 	%fd221, %fd219, %fd216, %fd220;
	mov.f64 	%fd222, 0dBEB4FC07FC9F1563;
	fma.rn.f64 	%fd223, %fd221, %fd216, %fd222;
	mov.f64 	%fd224, 0dBEF51D59DCE6A679;
	fma.rn.f64 	%fd225, %fd223, %fd216, %fd224;
	mov.f64 	%fd226, 0d3F20C8A6351CB1F9;
	fma.rn.f64 	%fd227, %fd225, %fd216, %fd226;
	mov.f64 	%fd228, 0dBF2C364D9E00D4CA;
	fma.rn.f64 	%fd229, %fd227, %fd216, %fd228;
	mov.f64 	%fd230, 0dBF5317112046830B;
	fma.rn.f64 	%fd231, %fd229, %fd216, %fd230;
	mov.f64 	%fd232, 0d3F7D919C50FF9416;
	fma.rn.f64 	%fd233, %fd231, %fd216, %fd232;
	mov.f64 	%fd234, 0dBF83B4AF28728BB0;
	fma.rn.f64 	%fd235, %fd233, %fd216, %fd234;
	mov.f64 	%fd236, 0dBFA59AF103C171DC;
	fma.rn.f64 	%fd237, %fd235, %fd216, %fd236;
	mov.f64 	%fd238, 0d3FC5512320B45D97;
	fma.rn.f64 	%fd239, %fd237, %fd216, %fd238;
	mov.f64 	%fd240, 0dBFA5815E8FA27607;
	fma.rn.f64 	%fd241, %fd239, %fd216, %fd240;
	mov.f64 	%fd242, 0dBFE4FCF4026AFA4B;
	fma.rn.f64 	%fd243, %fd241, %fd216, %fd242;
	mov.f64 	%fd244, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd245, %fd243, %fd216, %fd244;
	mov.f64 	%fd246, 0d3FF0000000000000;
	fma.rn.f64 	%fd247, %fd245, %fd216, %fd246;
	mul.f64 	%fd248, %fd247, %fd52;
	setp.lt.s32	%p14, %r67, 1071644672;
	selp.f64	%fd249, %fd248, %fd247, %p14;
	div.rn.f64 	%fd428, %fd403, %fd249;
	bra.uni 	BB30_45;

BB30_22:
	setp.lt.f64	%p15, %fd52, 0d0000000000000000;
	@%p15 bra 	BB30_24;

	add.f64 	%fd428, %fd52, %fd52;
	bra.uni 	BB30_45;

BB30_24:
	cvt.rzi.f64.f64	%fd250, %fd52;
	setp.neu.f64	%p16, %fd250, %fd52;
	@%p16 bra 	BB30_26;

	mov.f64 	%fd428, 0dFFF8000000000000;
	bra.uni 	BB30_45;

BB30_26:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd52;
	}
	setp.lt.u32	%p17, %r18, -1070727168;
	@%p17 bra 	BB30_41;

	setp.lt.u32	%p18, %r18, -1066983424;
	@%p18 bra 	BB30_29;

	cvt.rmi.f64.f64	%fd251, %fd52;
	mul.f64 	%fd252, %fd251, 0d3FE0000000000000;
	cvt.rmi.f64.f64	%fd253, %fd252;
	fma.rn.f64 	%fd254, %fd253, 0dC000000000000000, %fd251;
	setp.eq.f64	%p19, %fd254, 0d3FF0000000000000;
	selp.f64	%fd428, 0d8000000000000000, 0d0000000000000000, %p19;
	bra.uni 	BB30_45;

BB30_29:
	add.s32 	%r45, %r18, %r18;
	setp.lt.u32	%p20, %r45, -2038431743;
	mov.f64 	%fd423, %fd52;
	@%p20 bra 	BB30_31;

	mov.f64 	%fd255, 0d0000000000000000;
	mul.rn.f64 	%fd28, %fd52, %fd255;
	mov.f64 	%fd423, %fd28;

BB30_31:
	mov.f64 	%fd29, %fd423;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd29;
	}
	add.s32 	%r47, %r46, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd29;
	}
	mov.b64 	%fd256, {%r48, %r47};
	cvt.rni.f64.f64	%fd257, %fd256;
	cvt.rzi.s64.f64	%rd1, %fd257;
	cvt.u32.u64	%r49, %rd1;
	neg.f64 	%fd258, %fd257;
	mov.f64 	%fd259, 0d3FE0000000000000;
	fma.rn.f64 	%fd260, %fd258, %fd259, %fd29;
	mul.f64 	%fd261, %fd260, 0d3CA1A62633145C07;
	mov.f64 	%fd262, 0d400921FB54442D18;
	fma.rn.f64 	%fd263, %fd260, %fd262, %fd261;
	mul.rn.f64 	%fd264, %fd263, %fd263;
	mov.f64 	%fd265, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd266, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd267, %fd266, %fd264, %fd265;
	mov.f64 	%fd268, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd269, %fd267, %fd264, %fd268;
	mov.f64 	%fd270, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd271, %fd269, %fd264, %fd270;
	mov.f64 	%fd272, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd273, %fd271, %fd264, %fd272;
	mov.f64 	%fd274, 0d3FA5555555555551;
	fma.rn.f64 	%fd275, %fd273, %fd264, %fd274;
	mov.f64 	%fd276, 0dBFE0000000000000;
	fma.rn.f64 	%fd277, %fd275, %fd264, %fd276;
	mov.f64 	%fd278, 0d3FF0000000000000;
	fma.rn.f64 	%fd279, %fd277, %fd264, %fd278;
	mov.f64 	%fd280, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd281, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd282, %fd281, %fd264, %fd280;
	mov.f64 	%fd283, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd284, %fd282, %fd264, %fd283;
	mov.f64 	%fd285, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd286, %fd284, %fd264, %fd285;
	mov.f64 	%fd287, 0d3F81111111110818;
	fma.rn.f64 	%fd288, %fd286, %fd264, %fd287;
	mov.f64 	%fd289, 0dBFC5555555555554;
	fma.rn.f64 	%fd290, %fd288, %fd264, %fd289;
	mov.f64 	%fd291, 0d0000000000000000;
	fma.rn.f64 	%fd292, %fd290, %fd264, %fd291;
	fma.rn.f64 	%fd293, %fd292, %fd263, %fd263;
	and.b64  	%rd2, %rd1, 1;
	setp.eq.b64	%p21, %rd2, 1;
	not.pred 	%p22, %p21;
	selp.f64	%fd404, %fd293, %fd279, %p22;
	and.b32  	%r50, %r49, 2;
	setp.eq.s32	%p23, %r50, 0;
	@%p23 bra 	BB30_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd404;
	}
	xor.b32  	%r52, %r51, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd404;
	}
	mov.b64 	%fd404, {%r53, %r52};

BB30_33:
	cvt.rzi.f64.f64	%fd294, %fd29;
	setp.neu.f64	%p24, %fd29, %fd294;
	@%p24 bra 	BB30_35;

	mul.rn.f64 	%fd404, %fd29, %fd291;

BB30_35:
	mov.f64 	%fd296, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd297, %fd52, %fd296;
	mov.f64 	%fd298, 0d4338000000000000;
	add.rn.f64 	%fd299, %fd297, %fd298;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd299;
	}
	mov.f64 	%fd300, 0dC338000000000000;
	add.rn.f64 	%fd301, %fd299, %fd300;
	mov.f64 	%fd302, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd303, %fd301, %fd302, %fd52;
	mov.f64 	%fd304, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd305, %fd301, %fd304, %fd303;
	mov.f64 	%fd306, 0d3E928AF3FCA213EA;
	mov.f64 	%fd307, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd308, %fd307, %fd305, %fd306;
	mov.f64 	%fd309, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd310, %fd308, %fd305, %fd309;
	mov.f64 	%fd311, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd312, %fd310, %fd305, %fd311;
	mov.f64 	%fd313, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd314, %fd312, %fd305, %fd313;
	mov.f64 	%fd315, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd316, %fd314, %fd305, %fd315;
	mov.f64 	%fd317, 0d3F81111111122322;
	fma.rn.f64 	%fd318, %fd316, %fd305, %fd317;
	mov.f64 	%fd319, 0d3FA55555555502A1;
	fma.rn.f64 	%fd320, %fd318, %fd305, %fd319;
	mov.f64 	%fd321, 0d3FC5555555555511;
	fma.rn.f64 	%fd322, %fd320, %fd305, %fd321;
	mov.f64 	%fd323, 0d3FE000000000000B;
	fma.rn.f64 	%fd324, %fd322, %fd305, %fd323;
	fma.rn.f64 	%fd326, %fd324, %fd305, %fd278;
	fma.rn.f64 	%fd405, %fd326, %fd305, %fd278;
	abs.s32 	%r54, %r19;
	setp.lt.s32	%p25, %r54, 1023;
	@%p25 bra 	BB30_37;

	add.s32 	%r55, %r19, 2046;
	shl.b32 	%r56, %r55, 19;
	and.b32  	%r57, %r56, -1048576;
	shl.b32 	%r58, %r55, 20;
	sub.s32 	%r71, %r58, %r57;
	mov.u32 	%r59, 0;
	mov.b64 	%fd327, {%r59, %r57};
	mul.f64 	%fd405, %fd405, %fd327;
	bra.uni 	BB30_38;

BB30_37:
	shl.b32 	%r60, %r19, 20;
	add.s32 	%r71, %r60, 1072693248;

BB30_38:
	mov.u32 	%r61, 0;
	mov.b64 	%fd328, {%r61, %r71};
	mul.f64 	%fd38, %fd405, %fd328;
	abs.f64 	%fd39, %fd52;
	add.f64 	%fd406, %fd39, 0dBFE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd39;
	}
	setp.lt.s32	%p26, %r23, 1080131584;
	@%p26 bra 	BB30_40;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r62, %temp}, %fd406;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r63}, %fd406;
	}
	add.s32 	%r64, %r63, -1048576;
	mov.b64 	%fd406, {%r62, %r64};

BB30_40:
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd406;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd331, [retval0+0];
	}
	// Callseq End 0
	mul.f64 	%fd332, %fd38, %fd331;
	setp.gt.s32	%p27, %r23, 1080131583;
	selp.f64	%fd333, %fd332, %fd38, %p27;
	// inline asm
	rcp.approx.ftz.f64 %fd329,%fd39;
	// inline asm
	neg.f64 	%fd334, %fd39;
	fma.rn.f64 	%fd336, %fd334, %fd329, %fd278;
	fma.rn.f64 	%fd337, %fd336, %fd336, %fd336;
	fma.rn.f64 	%fd338, %fd337, %fd329, %fd329;
	mov.f64 	%fd339, 0d3F4B8239C670E690;
	fma.rn.f64 	%fd341, %fd291, %fd338, %fd339;
	mov.f64 	%fd342, 0dBF0B1D75D3346711;
	fma.rn.f64 	%fd343, %fd341, %fd338, %fd342;
	mov.f64 	%fd344, 0dBF436773BDB97B48;
	fma.rn.f64 	%fd345, %fd343, %fd338, %fd344;
	mov.f64 	%fd346, 0d3F1247604839C038;
	fma.rn.f64 	%fd347, %fd345, %fd338, %fd346;
	mov.f64 	%fd348, 0d3F49B0FF6874F2C4;
	fma.rn.f64 	%fd349, %fd347, %fd338, %fd348;
	mov.f64 	%fd350, 0dBF2E13CE465FA859;
	fma.rn.f64 	%fd351, %fd349, %fd338, %fd350;
	mov.f64 	%fd352, 0dBF65F7268EDAB4C8;
	fma.rn.f64 	%fd353, %fd351, %fd338, %fd352;
	mov.f64 	%fd354, 0d3F6C71C71C71C71C;
	fma.rn.f64 	%fd355, %fd353, %fd338, %fd354;
	mov.f64 	%fd356, 0d3FB5555555555555;
	fma.rn.f64 	%fd357, %fd355, %fd338, %fd356;
	fma.rn.f64 	%fd358, %fd357, %fd338, %fd278;
	mul.f64 	%fd359, %fd333, %fd358;
	mul.f64 	%fd360, %fd359, %fd39;
	mul.f64 	%fd361, %fd360, %fd404;
	rcp.rn.f64 	%fd362, %fd361;
	mul.f64 	%fd363, %fd362, 0dBC9A6A0D6F814637;
	mov.f64 	%fd364, 0d3FF40D931FF62706;
	fma.rn.f64 	%fd365, %fd362, %fd364, %fd363;
	div.rn.f64 	%fd428, %fd365, %fd331;
	bra.uni 	BB30_45;

BB30_41:
	setp.lt.u32	%p28, %r18, -1075838976;
	mov.f64 	%fd419, %fd52;
	mov.f64 	%fd420, %fd52;
	@%p28 bra 	BB30_44;

	mov.f64 	%fd421, %fd52;
	mov.f64 	%fd422, %fd52;

BB30_43:
	fma.rn.f64 	%fd422, %fd422, %fd421, %fd422;
	add.f64 	%fd421, %fd421, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd421;
	}
	setp.gt.u32	%p29, %r65, -1075838977;
	mov.f64 	%fd420, %fd422;
	mov.f64 	%fd419, %fd421;
	@%p29 bra 	BB30_43;

BB30_44:
	mov.f64 	%fd366, 0dBE8B338C457183B6;
	mov.f64 	%fd367, 0dBDFE6BDF8CC487CD;
	fma.rn.f64 	%fd368, %fd367, %fd419, %fd366;
	mov.f64 	%fd369, 0d3EB31831766A0388;
	fma.rn.f64 	%fd370, %fd368, %fd419, %fd369;
	mov.f64 	%fd371, 0dBEB4FC07FC9F1563;
	fma.rn.f64 	%fd372, %fd370, %fd419, %fd371;
	mov.f64 	%fd373, 0dBEF51D59DCE6A679;
	fma.rn.f64 	%fd374, %fd372, %fd419, %fd373;
	mov.f64 	%fd375, 0d3F20C8A6351CB1F9;
	fma.rn.f64 	%fd376, %fd374, %fd419, %fd375;
	mov.f64 	%fd377, 0dBF2C364D9E00D4CA;
	fma.rn.f64 	%fd378, %fd376, %fd419, %fd377;
	mov.f64 	%fd379, 0dBF5317112046830B;
	fma.rn.f64 	%fd380, %fd378, %fd419, %fd379;
	mov.f64 	%fd381, 0d3F7D919C50FF9416;
	fma.rn.f64 	%fd382, %fd380, %fd419, %fd381;
	mov.f64 	%fd383, 0dBF83B4AF28728BB0;
	fma.rn.f64 	%fd384, %fd382, %fd419, %fd383;
	mov.f64 	%fd385, 0dBFA59AF103C171DC;
	fma.rn.f64 	%fd386, %fd384, %fd419, %fd385;
	mov.f64 	%fd387, 0d3FC5512320B45D97;
	fma.rn.f64 	%fd388, %fd386, %fd419, %fd387;
	mov.f64 	%fd389, 0dBFA5815E8FA27607;
	fma.rn.f64 	%fd390, %fd388, %fd419, %fd389;
	mov.f64 	%fd391, 0dBFE4FCF4026AFA4B;
	fma.rn.f64 	%fd392, %fd390, %fd419, %fd391;
	mov.f64 	%fd393, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd394, %fd392, %fd419, %fd393;
	mov.f64 	%fd395, 0d3FF0000000000000;
	fma.rn.f64 	%fd396, %fd394, %fd419, %fd395;
	mul.f64 	%fd397, %fd420, %fd396;
	rcp.rn.f64 	%fd428, %fd397;

BB30_45:
	st.param.f64	[funj_retval0+0], %fd428;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammai(
	.param .b32 ___tgammai_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.u32 	%r1, [___tgammai_param_0];
	cvt.rn.f32.s32	%f1, %r1;
	setp.ltu.f32	%p1, %f1, 0f00000000;
	@%p1 bra 	BB31_5;

	setp.gt.f32	%p2, %f1, 0f42100000;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB31_2;
	bra.uni 	BB31_4;

BB31_2:
	mov.f32 	%f88, %f87;

BB31_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB31_3;

BB31_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB31_12;

BB31_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB31_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB31_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB31_7;

BB31_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB31_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f1, 0fC2280000;
	@%p15 bra 	BB31_12;

	cvt.rzi.s32.f32	%r2, %f1;
	and.b32  	%r3, %r2, 1;
	setp.eq.b32	%p16, %r3, 1;
	@%p16 bra 	BB31_12;

	mov.f32 	%f98, 0f80000000;

BB31_12:
	cvt.rzi.s32.f32	%r4, %f98;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammau(
	.param .b32 ___tgammau_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.u32 	%r1, [___tgammau_param_0];
	cvt.rn.f32.u32	%f1, %r1;
	setp.ltu.f32	%p1, %f1, 0f00000000;
	@%p1 bra 	BB32_5;

	setp.gt.f32	%p2, %f1, 0f42100000;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB32_2;
	bra.uni 	BB32_4;

BB32_2:
	mov.f32 	%f88, %f87;

BB32_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB32_3;

BB32_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB32_12;

BB32_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB32_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB32_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB32_7;

BB32_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB32_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f1, 0fC2280000;
	@%p15 bra 	BB32_12;

	cvt.rzi.s32.f32	%r2, %f1;
	and.b32  	%r3, %r2, 1;
	setp.eq.b32	%p16, %r3, 1;
	@%p16 bra 	BB32_12;

	mov.f32 	%f98, 0f80000000;

BB32_12:
	cvt.rzi.u32.f32	%r4, %f98;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammaj(
	.param .b32 ___tgammaj_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.s8 	%rs1, [___tgammaj_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	setp.lt.s16	%p1, %rs1, 0;
	@%p1 bra 	BB33_5;

	setp.gt.s16	%p2, %rs1, 36;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB33_2;
	bra.uni 	BB33_4;

BB33_2:
	mov.f32 	%f88, %f87;

BB33_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB33_3;

BB33_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB33_12;

BB33_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB33_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB33_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB33_7;

BB33_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB33_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.gt.s16	%p15, %rs1, -43;
	@%p15 bra 	BB33_12;

	cvt.rzi.s32.f32	%r1, %f1;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32	%p16, %r2, 1;
	@%p16 bra 	BB33_12;

	mov.f32 	%f98, 0f80000000;

BB33_12:
	cvt.rzi.s32.f32	%r3, %f98;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammav(
	.param .b32 ___tgammav_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<45>;


	ld.param.u8 	%rs1, [___tgammav_param_0];
	cvt.rn.f32.u16	%f12, %rs1;
	setp.gt.u16	%p1, %rs1, 36;
	selp.f32	%f1, 0f42100000, %f12, %p1;
	setp.gt.f32	%p2, %f1, 0f42081EB8;
	add.f32 	%f2, %f1, 0fBF800000;
	selp.f32	%f42, %f2, %f1, %p2;
	setp.gt.f32	%p3, %f42, 0f3FC00000;
	add.f32 	%f43, %f42, 0fBF800000;
	mov.f32 	%f38, 0f3F800000;
	@%p3 bra 	BB34_1;
	bra.uni 	BB34_3;

BB34_1:
	mov.f32 	%f44, %f43;

BB34_2:
	mov.f32 	%f42, %f44;
	mul.f32 	%f38, %f38, %f42;
	add.f32 	%f44, %f42, 0fBF800000;
	setp.gt.f32	%p4, %f42, 0f3FC00000;
	mov.f32 	%f43, %f44;
	@%p4 bra 	BB34_2;

BB34_3:
	setp.ltu.f32	%p5, %f1, 0f3F000000;
	selp.f32	%f15, %f42, %f43, %p5;
	mov.f32 	%f16, 0f3BE86AA4;
	mov.f32 	%f17, 0fBA8AA19E;
	fma.rn.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0fBC1E2998;
	fma.rn.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBD2CBE4A;
	fma.rn.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f3E2A8A17;
	fma.rn.f32 	%f24, %f22, %f15, %f23;
	mov.f32 	%f25, 0fBD2C0CBB;
	fma.rn.f32 	%f26, %f24, %f15, %f25;
	mov.f32 	%f27, 0fBF27E7A3;
	fma.rn.f32 	%f28, %f26, %f15, %f27;
	mov.f32 	%f29, 0f3F13C468;
	fma.rn.f32 	%f30, %f28, %f15, %f29;
	mov.f32 	%f31, 0f3F800000;
	fma.rn.f32 	%f32, %f30, %f15, %f31;
	mul.f32 	%f33, %f32, %f1;
	setp.lt.f32	%p6, %f1, 0f3F000000;
	selp.f32	%f34, %f33, %f32, %p6;
	div.approx.f32 	%f35, %f38, %f34;
	mul.f32 	%f36, %f35, %f2;
	selp.f32	%f37, %f36, %f35, %p2;
	cvt.rzi.u32.f32	%r1, %f37;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[funj_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammas(
	.param .b32 ___lgammas_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<269>;


	ld.param.f32 	%f39, [___lgammas_param_0];
	abs.f32 	%f1, %f39;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB35_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB35_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f1;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB35_4;

	lg2.approx.f32 	%f261, %f1;
	bra.uni 	BB35_5;

BB35_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f51, %f1, 0f4B800000;
	selp.f32	%f52, %f51, %f1, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r3, %f52;
	and.b32  	%r4, %r3, 8388607;
	or.b32  	%r5, %r4, 1065353216;
	mov.b32 	 %f54, %r5;
	shr.u32 	%r6, %r3, 23;
	cvt.rn.f32.u32	%f55, %r6;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB35_5:
	add.f32 	%f76, %f1, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f1;
	add.rn.f32 	%f80, %f78, %f2;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f268, %f1, %f81, %p8;
	bra.uni 	BB35_15;

BB35_6:
	add.f32 	%f84, %f1, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB35_15;

BB35_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB35_9;

	add.f32 	%f100, %f1, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB35_15;

BB35_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB35_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f1;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB35_15;

BB35_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f1, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f1, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f1, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f1, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f1, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f1, %f154;
	mul.f32 	%f156, %f155, %f1;
	fma.rn.f32 	%f10, %f156, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB35_13;

	lg2.approx.f32 	%f262, %f10;
	bra.uni 	BB35_14;

BB35_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f159, %f10, 0f4B800000;
	selp.f32	%f160, %f159, %f10, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r7, %f160;
	and.b32  	%r8, %r7, 8388607;
	or.b32  	%r9, %r8, 1065353216;
	mov.b32 	 %f162, %r9;
	shr.u32 	%r10, %r7, 23;
	cvt.rn.f32.u32	%f163, %r10;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB35_14:
	neg.f32 	%f268, %f262;

BB35_15:
	setp.ge.f32	%p16, %f39, 0f00000000;
	@%p16 bra 	BB35_37;

	cvt.rmi.f32.f32	%f184, %f1;
	setp.neu.f32	%p17, %f1, %f184;
	@%p17 bra 	BB35_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB35_37;

BB35_18:
	setp.lt.f32	%p18, %f1, 0f1FEC1E4A;
	@%p18 bra 	BB35_33;

	add.f32 	%f185, %f1, %f1;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f1;
	mul.f32 	%f16, %f189, 0f40490FDB;
	mul.rn.f32 	%f17, %f16, %f16;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB35_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f17, %f190;
	bra.uni 	BB35_22;

BB35_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f17, %f192;

BB35_22:
	@%p19 bra 	BB35_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f17, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f17, %f196;
	bra.uni 	BB35_25;

BB35_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f17, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f17, %f199;

BB35_25:
	fma.rn.f32 	%f265, %f264, %f16, %f16;
	@%p19 bra 	BB35_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f17, %f200;

BB35_27:
	and.b32  	%r11, %r1, 2;
	setp.eq.s32	%p22, %r11, 0;
	@%p22 bra 	BB35_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB35_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f29, %f203, %f1;
	setp.gt.f32	%p23, %f29, 0f00000000;
	setp.lt.f32	%p24, %f29, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB35_31;

	lg2.approx.f32 	%f266, %f29;
	bra.uni 	BB35_32;

BB35_31:
	setp.lt.f32	%p26, %f29, 0f00800000;
	mul.f32 	%f206, %f29, 0f4B800000;
	selp.f32	%f207, %f206, %f29, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r12, %f207;
	and.b32  	%r13, %r12, 8388607;
	or.b32  	%r14, %r13, 1065353216;
	mov.b32 	 %f209, %r14;
	shr.u32 	%r15, %r12, 23;
	cvt.rn.f32.u32	%f210, %r15;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB35_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB35_37;

BB35_33:
	setp.gt.f32	%p28, %f1, 0f00000000;
	setp.lt.f32	%p29, %f1, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB35_35;

	lg2.approx.f32 	%f267, %f1;
	bra.uni 	BB35_36;

BB35_35:
	setp.lt.f32	%p31, %f1, 0f00800000;
	mul.f32 	%f235, %f1, 0f4B800000;
	selp.f32	%f236, %f235, %f1, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r16, %f236;
	and.b32  	%r17, %r16, 8388607;
	or.b32  	%r18, %r17, 1065353216;
	mov.b32 	 %f238, %r18;
	shr.u32 	%r19, %r16, 23;
	cvt.rn.f32.u32	%f239, %r19;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB35_36:
	neg.f32 	%f268, %f267;

BB35_37:
	st.param.f32	[funj_retval0+0], %f268;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___lgammad(
	.param .b64 ___lgammad_param_0
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<37>;
	.reg .s64 	%rd<3>;
	.reg .f64 	%fd<125>;


	ld.param.f64 	%fd22, [___lgammad_param_0];
	abs.f64 	%fd1, %fd22;
	setp.gtu.f64	%p1, %fd1, 0d7FF0000000000000;
	@%p1 bra 	BB36_25;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_lgamma_pos, 
	(
	param0
	);
	ld.param.f64	%fd124, [retval0+0];
	}
	// Callseq End 1
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd22;
	}
	setp.gt.s32	%p2, %r12, -1;
	@%p2 bra 	BB36_26;

	cvt.rzi.f64.f64	%fd23, %fd1;
	setp.neu.f64	%p3, %fd1, %fd23;
	@%p3 bra 	BB36_4;

	mov.f64 	%fd124, 0d7FF0000000000000;
	bra.uni 	BB36_26;

BB36_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd1;
	}
	setp.lt.s32	%p4, %r1, 1006632960;
	mov.f64 	%fd120, %fd1;
	@%p4 bra 	BB36_12;

	add.s32 	%r13, %r1, %r1;
	setp.lt.u32	%p5, %r13, -2038431743;
	mov.f64 	%fd121, %fd1;
	@%p5 bra 	BB36_7;

	mov.f64 	%fd24, 0d0000000000000000;
	mul.rn.f64 	%fd3, %fd1, %fd24;
	mov.f64 	%fd121, %fd3;

BB36_7:
	mov.f64 	%fd4, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd4;
	}
	add.s32 	%r15, %r14, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd4;
	}
	mov.b64 	%fd25, {%r16, %r15};
	cvt.rni.f64.f64	%fd26, %fd25;
	cvt.rzi.s64.f64	%rd1, %fd26;
	cvt.u32.u64	%r17, %rd1;
	neg.f64 	%fd27, %fd26;
	mov.f64 	%fd28, 0d3FE0000000000000;
	fma.rn.f64 	%fd29, %fd27, %fd28, %fd4;
	mul.f64 	%fd30, %fd29, 0d3CA1A62633145C07;
	mov.f64 	%fd31, 0d400921FB54442D18;
	fma.rn.f64 	%fd32, %fd29, %fd31, %fd30;
	mul.rn.f64 	%fd33, %fd32, %fd32;
	mov.f64 	%fd34, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd35, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd36, %fd35, %fd33, %fd34;
	mov.f64 	%fd37, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd38, %fd36, %fd33, %fd37;
	mov.f64 	%fd39, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd40, %fd38, %fd33, %fd39;
	mov.f64 	%fd41, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd42, %fd40, %fd33, %fd41;
	mov.f64 	%fd43, 0d3FA5555555555551;
	fma.rn.f64 	%fd44, %fd42, %fd33, %fd43;
	mov.f64 	%fd45, 0dBFE0000000000000;
	fma.rn.f64 	%fd46, %fd44, %fd33, %fd45;
	mov.f64 	%fd47, 0d3FF0000000000000;
	fma.rn.f64 	%fd48, %fd46, %fd33, %fd47;
	mov.f64 	%fd49, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd50, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd51, %fd50, %fd33, %fd49;
	mov.f64 	%fd52, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd53, %fd51, %fd33, %fd52;
	mov.f64 	%fd54, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd55, %fd53, %fd33, %fd54;
	mov.f64 	%fd56, 0d3F81111111110818;
	fma.rn.f64 	%fd57, %fd55, %fd33, %fd56;
	mov.f64 	%fd58, 0dBFC5555555555554;
	fma.rn.f64 	%fd59, %fd57, %fd33, %fd58;
	mov.f64 	%fd60, 0d0000000000000000;
	fma.rn.f64 	%fd61, %fd59, %fd33, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd32, %fd32;
	and.b64  	%rd2, %rd1, 1;
	setp.eq.b64	%p6, %rd2, 1;
	not.pred 	%p7, %p6;
	selp.f64	%fd118, %fd62, %fd48, %p7;
	and.b32  	%r18, %r17, 2;
	setp.eq.s32	%p8, %r18, 0;
	@%p8 bra 	BB36_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd118;
	}
	xor.b32  	%r20, %r19, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd118;
	}
	mov.b64 	%fd118, {%r21, %r20};

BB36_9:
	cvt.rzi.f64.f64	%fd63, %fd4;
	setp.neu.f64	%p9, %fd4, %fd63;
	@%p9 bra 	BB36_11;

	mul.rn.f64 	%fd118, %fd4, %fd60;

BB36_11:
	abs.f64 	%fd65, %fd118;
	mul.f64 	%fd66, %fd65, %fd1;
	div.rn.f64 	%fd120, %fd31, %fd66;

BB36_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd120;
	}
	setp.lt.s32	%p10, %r33, 2146435072;
	setp.gt.f64	%p11, %fd120, 0d0000000000000000;
	and.pred  	%p12, %p11, %p10;
	@%p12 bra 	BB36_18;

	abs.f64 	%fd68, %fd120;
	setp.gtu.f64	%p13, %fd68, 0d7FF0000000000000;
	@%p13 bra 	BB36_17;

	setp.neu.f64	%p14, %fd120, 0d0000000000000000;
	@%p14 bra 	BB36_16;

	mov.f64 	%fd123, 0dFFF0000000000000;
	bra.uni 	BB36_24;

BB36_16:
	setp.eq.f64	%p15, %fd120, 0d7FF0000000000000;
	selp.f64	%fd123, %fd120, 0dFFF8000000000000, %p15;
	bra.uni 	BB36_24;

BB36_17:
	add.f64 	%fd123, %fd120, %fd120;
	bra.uni 	BB36_24;

BB36_18:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd120;
	}
	setp.lt.s32	%p16, %r33, 1048576;
	@%p16 bra 	BB36_20;

	mov.u32 	%r35, -1023;
	bra.uni 	BB36_21;

BB36_20:
	mul.f64 	%fd70, %fd120, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd70;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd70;
	}
	mov.u32 	%r35, -1077;

BB36_21:
	shr.u32 	%r24, %r33, 20;
	add.s32 	%r36, %r35, %r24;
	and.b32  	%r25, %r33, -2146435073;
	or.b32  	%r26, %r25, 1072693248;
	mov.b64 	%fd122, {%r34, %r26};
	setp.lt.s32	%p17, %r26, 1073127583;
	@%p17 bra 	BB36_23;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd122;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd122;
	}
	add.s32 	%r29, %r28, -1048576;
	mov.b64 	%fd122, {%r27, %r29};
	add.s32 	%r36, %r36, 1;

BB36_23:
	add.f64 	%fd72, %fd122, 0d3FF0000000000000;
	mov.f64 	%fd73, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd71,%fd72;
	// inline asm
	neg.f64 	%fd74, %fd72;
	fma.rn.f64 	%fd75, %fd74, %fd71, %fd73;
	fma.rn.f64 	%fd76, %fd75, %fd75, %fd75;
	fma.rn.f64 	%fd77, %fd76, %fd71, %fd71;
	add.f64 	%fd78, %fd122, 0dBFF0000000000000;
	mul.f64 	%fd79, %fd78, %fd77;
	fma.rn.f64 	%fd80, %fd78, %fd77, %fd79;
	mul.f64 	%fd81, %fd80, %fd80;
	mov.f64 	%fd82, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd83, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd84, %fd83, %fd81, %fd82;
	mov.f64 	%fd85, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd86, %fd84, %fd81, %fd85;
	mov.f64 	%fd87, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd88, %fd86, %fd81, %fd87;
	mov.f64 	%fd89, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd90, %fd88, %fd81, %fd89;
	mov.f64 	%fd91, 0d3F624924923BE72D;
	fma.rn.f64 	%fd92, %fd90, %fd81, %fd91;
	mov.f64 	%fd93, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd94, %fd92, %fd81, %fd93;
	mov.f64 	%fd95, 0d3FB5555555555554;
	fma.rn.f64 	%fd96, %fd94, %fd81, %fd95;
	sub.f64 	%fd97, %fd78, %fd80;
	add.f64 	%fd98, %fd97, %fd97;
	neg.f64 	%fd99, %fd80;
	fma.rn.f64 	%fd100, %fd99, %fd78, %fd98;
	mul.f64 	%fd101, %fd77, %fd100;
	mul.f64 	%fd102, %fd96, %fd81;
	fma.rn.f64 	%fd103, %fd102, %fd80, %fd101;
	xor.b32  	%r30, %r36, -2147483648;
	mov.u32 	%r31, -2147483648;
	mov.u32 	%r32, 1127219200;
	mov.b64 	%fd104, {%r30, %r32};
	mov.b64 	%fd105, {%r31, %r32};
	sub.f64 	%fd106, %fd104, %fd105;
	mov.f64 	%fd107, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd108, %fd106, %fd107, %fd80;
	neg.f64 	%fd109, %fd106;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	sub.f64 	%fd111, %fd110, %fd80;
	sub.f64 	%fd112, %fd103, %fd111;
	mov.f64 	%fd113, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd114, %fd106, %fd113, %fd112;
	add.f64 	%fd123, %fd108, %fd114;

BB36_24:
	sub.f64 	%fd115, %fd123, %fd124;
	neg.f64 	%fd116, %fd123;
	selp.f64	%fd124, %fd116, %fd115, %p4;
	bra.uni 	BB36_26;

BB36_25:
	add.f64 	%fd124, %fd22, %fd22;

BB36_26:
	st.param.f64	[funj_retval0+0], %fd124;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammai(
	.param .b32 ___lgammai_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.u32 	%r3, [___lgammai_param_0];
	cvt.rn.f32.s32	%f1, %r3;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p1, %f2, 0f40400000;
	@%p1 bra 	BB37_7;

	setp.ltu.f32	%p2, %f2, 0f40F9999A;
	@%p2 bra 	BB37_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f2;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f3, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f2, 0f7F800000;
	setp.gt.f32	%p4, %f2, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB37_4;

	lg2.approx.f32 	%f261, %f2;
	bra.uni 	BB37_5;

BB37_4:
	setp.lt.f32	%p6, %f2, 0f00800000;
	mul.f32 	%f51, %f2, 0f4B800000;
	selp.f32	%f52, %f51, %f2, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r4, %f52;
	and.b32  	%r5, %r4, 8388607;
	or.b32  	%r6, %r5, 1065353216;
	mov.b32 	 %f54, %r6;
	shr.u32 	%r7, %r4, 23;
	cvt.rn.f32.u32	%f55, %r7;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB37_5:
	add.f32 	%f76, %f2, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f2;
	add.rn.f32 	%f80, %f78, %f3;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f2, 0f7F800000;
	selp.f32	%f268, %f2, %f81, %p8;
	bra.uni 	BB37_15;

BB37_6:
	add.f32 	%f84, %f2, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB37_15;

BB37_7:
	setp.ltu.f32	%p9, %f2, 0f3FC00000;
	@%p9 bra 	BB37_9;

	add.f32 	%f100, %f2, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB37_15;

BB37_9:
	setp.ltu.f32	%p10, %f2, 0f3F333333;
	@%p10 bra 	BB37_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f2;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB37_15;

BB37_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f2, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f2, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f2, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f2, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f2, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f2, %f154;
	mul.f32 	%f156, %f155, %f2;
	fma.rn.f32 	%f11, %f156, %f2, %f2;
	setp.gt.f32	%p11, %f11, 0f00000000;
	setp.lt.f32	%p12, %f11, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB37_13;

	lg2.approx.f32 	%f262, %f11;
	bra.uni 	BB37_14;

BB37_13:
	setp.lt.f32	%p14, %f11, 0f00800000;
	mul.f32 	%f159, %f11, 0f4B800000;
	selp.f32	%f160, %f159, %f11, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r8, %f160;
	and.b32  	%r9, %r8, 8388607;
	or.b32  	%r10, %r9, 1065353216;
	mov.b32 	 %f162, %r10;
	shr.u32 	%r11, %r8, 23;
	cvt.rn.f32.u32	%f163, %r11;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB37_14:
	neg.f32 	%f268, %f262;

BB37_15:
	setp.ge.f32	%p16, %f1, 0f00000000;
	@%p16 bra 	BB37_37;

	cvt.rmi.f32.f32	%f184, %f2;
	setp.neu.f32	%p17, %f2, %f184;
	@%p17 bra 	BB37_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB37_37;

BB37_18:
	setp.lt.f32	%p18, %f2, 0f1FEC1E4A;
	@%p18 bra 	BB37_33;

	add.f32 	%f185, %f2, %f2;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f2;
	mul.f32 	%f17, %f189, 0f40490FDB;
	mul.rn.f32 	%f18, %f17, %f17;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB37_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f18, %f190;
	bra.uni 	BB37_22;

BB37_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f18, %f192;

BB37_22:
	@%p19 bra 	BB37_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f18, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f18, %f196;
	bra.uni 	BB37_25;

BB37_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f18, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f18, %f199;

BB37_25:
	fma.rn.f32 	%f265, %f264, %f17, %f17;
	@%p19 bra 	BB37_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f18, %f200;

BB37_27:
	and.b32  	%r12, %r1, 2;
	setp.eq.s32	%p22, %r12, 0;
	@%p22 bra 	BB37_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB37_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f30, %f203, %f2;
	setp.gt.f32	%p23, %f30, 0f00000000;
	setp.lt.f32	%p24, %f30, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB37_31;

	lg2.approx.f32 	%f266, %f30;
	bra.uni 	BB37_32;

BB37_31:
	setp.lt.f32	%p26, %f30, 0f00800000;
	mul.f32 	%f206, %f30, 0f4B800000;
	selp.f32	%f207, %f206, %f30, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r13, %f207;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	 %f209, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32	%f210, %r16;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB37_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB37_37;

BB37_33:
	setp.gt.f32	%p28, %f2, 0f00000000;
	setp.lt.f32	%p29, %f2, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB37_35;

	lg2.approx.f32 	%f267, %f2;
	bra.uni 	BB37_36;

BB37_35:
	setp.lt.f32	%p31, %f2, 0f00800000;
	mul.f32 	%f235, %f2, 0f4B800000;
	selp.f32	%f236, %f235, %f2, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r17, %f236;
	and.b32  	%r18, %r17, 8388607;
	or.b32  	%r19, %r18, 1065353216;
	mov.b32 	 %f238, %r19;
	shr.u32 	%r20, %r17, 23;
	cvt.rn.f32.u32	%f239, %r20;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB37_36:
	neg.f32 	%f268, %f267;

BB37_37:
	cvt.rzi.s32.f32	%r21, %f268;
	st.param.b32	[funj_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammau(
	.param .b32 ___lgammau_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.u32 	%r3, [___lgammau_param_0];
	cvt.rn.f32.u32	%f1, %r3;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p1, %f2, 0f40400000;
	@%p1 bra 	BB38_7;

	setp.ltu.f32	%p2, %f2, 0f40F9999A;
	@%p2 bra 	BB38_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f2;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f3, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f2, 0f7F800000;
	setp.gt.f32	%p4, %f2, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB38_4;

	lg2.approx.f32 	%f261, %f2;
	bra.uni 	BB38_5;

BB38_4:
	setp.lt.f32	%p6, %f2, 0f00800000;
	mul.f32 	%f51, %f2, 0f4B800000;
	selp.f32	%f52, %f51, %f2, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r4, %f52;
	and.b32  	%r5, %r4, 8388607;
	or.b32  	%r6, %r5, 1065353216;
	mov.b32 	 %f54, %r6;
	shr.u32 	%r7, %r4, 23;
	cvt.rn.f32.u32	%f55, %r7;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB38_5:
	add.f32 	%f76, %f2, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f2;
	add.rn.f32 	%f80, %f78, %f3;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f2, 0f7F800000;
	selp.f32	%f268, %f2, %f81, %p8;
	bra.uni 	BB38_15;

BB38_6:
	add.f32 	%f84, %f2, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB38_15;

BB38_7:
	setp.ltu.f32	%p9, %f2, 0f3FC00000;
	@%p9 bra 	BB38_9;

	add.f32 	%f100, %f2, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB38_15;

BB38_9:
	setp.ltu.f32	%p10, %f2, 0f3F333333;
	@%p10 bra 	BB38_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f2;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB38_15;

BB38_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f2, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f2, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f2, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f2, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f2, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f2, %f154;
	mul.f32 	%f156, %f155, %f2;
	fma.rn.f32 	%f11, %f156, %f2, %f2;
	setp.gt.f32	%p11, %f11, 0f00000000;
	setp.lt.f32	%p12, %f11, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB38_13;

	lg2.approx.f32 	%f262, %f11;
	bra.uni 	BB38_14;

BB38_13:
	setp.lt.f32	%p14, %f11, 0f00800000;
	mul.f32 	%f159, %f11, 0f4B800000;
	selp.f32	%f160, %f159, %f11, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r8, %f160;
	and.b32  	%r9, %r8, 8388607;
	or.b32  	%r10, %r9, 1065353216;
	mov.b32 	 %f162, %r10;
	shr.u32 	%r11, %r8, 23;
	cvt.rn.f32.u32	%f163, %r11;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB38_14:
	neg.f32 	%f268, %f262;

BB38_15:
	setp.ge.f32	%p16, %f1, 0f00000000;
	@%p16 bra 	BB38_37;

	cvt.rmi.f32.f32	%f184, %f2;
	setp.neu.f32	%p17, %f2, %f184;
	@%p17 bra 	BB38_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB38_37;

BB38_18:
	setp.lt.f32	%p18, %f2, 0f1FEC1E4A;
	@%p18 bra 	BB38_33;

	add.f32 	%f185, %f2, %f2;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f2;
	mul.f32 	%f17, %f189, 0f40490FDB;
	mul.rn.f32 	%f18, %f17, %f17;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB38_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f18, %f190;
	bra.uni 	BB38_22;

BB38_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f18, %f192;

BB38_22:
	@%p19 bra 	BB38_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f18, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f18, %f196;
	bra.uni 	BB38_25;

BB38_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f18, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f18, %f199;

BB38_25:
	fma.rn.f32 	%f265, %f264, %f17, %f17;
	@%p19 bra 	BB38_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f18, %f200;

BB38_27:
	and.b32  	%r12, %r1, 2;
	setp.eq.s32	%p22, %r12, 0;
	@%p22 bra 	BB38_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB38_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f30, %f203, %f2;
	setp.gt.f32	%p23, %f30, 0f00000000;
	setp.lt.f32	%p24, %f30, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB38_31;

	lg2.approx.f32 	%f266, %f30;
	bra.uni 	BB38_32;

BB38_31:
	setp.lt.f32	%p26, %f30, 0f00800000;
	mul.f32 	%f206, %f30, 0f4B800000;
	selp.f32	%f207, %f206, %f30, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r13, %f207;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	 %f209, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32	%f210, %r16;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB38_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB38_37;

BB38_33:
	setp.gt.f32	%p28, %f2, 0f00000000;
	setp.lt.f32	%p29, %f2, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB38_35;

	lg2.approx.f32 	%f267, %f2;
	bra.uni 	BB38_36;

BB38_35:
	setp.lt.f32	%p31, %f2, 0f00800000;
	mul.f32 	%f235, %f2, 0f4B800000;
	selp.f32	%f236, %f235, %f2, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r17, %f236;
	and.b32  	%r18, %r17, 8388607;
	or.b32  	%r19, %r18, 1065353216;
	mov.b32 	 %f238, %r19;
	shr.u32 	%r20, %r17, 23;
	cvt.rn.f32.u32	%f239, %r20;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB38_36:
	neg.f32 	%f268, %f267;

BB38_37:
	cvt.rzi.u32.f32	%r21, %f268;
	st.param.b32	[funj_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammaj(
	.param .b32 ___lgammaj_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.s8 	%rs1, [___lgammaj_param_0];
	cvt.rn.f32.s16	%f39, %rs1;
	abs.f32 	%f1, %f39;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB39_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB39_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f1;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB39_4;

	lg2.approx.f32 	%f261, %f1;
	bra.uni 	BB39_5;

BB39_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f51, %f1, 0f4B800000;
	selp.f32	%f52, %f51, %f1, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r3, %f52;
	and.b32  	%r4, %r3, 8388607;
	or.b32  	%r5, %r4, 1065353216;
	mov.b32 	 %f54, %r5;
	shr.u32 	%r6, %r3, 23;
	cvt.rn.f32.u32	%f55, %r6;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB39_5:
	add.f32 	%f76, %f1, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f1;
	add.rn.f32 	%f80, %f78, %f2;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f268, %f1, %f81, %p8;
	bra.uni 	BB39_15;

BB39_6:
	add.f32 	%f84, %f1, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB39_15;

BB39_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB39_9;

	add.f32 	%f100, %f1, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB39_15;

BB39_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB39_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f1;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB39_15;

BB39_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f1, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f1, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f1, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f1, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f1, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f1, %f154;
	mul.f32 	%f156, %f155, %f1;
	fma.rn.f32 	%f10, %f156, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB39_13;

	lg2.approx.f32 	%f262, %f10;
	bra.uni 	BB39_14;

BB39_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f159, %f10, 0f4B800000;
	selp.f32	%f160, %f159, %f10, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r7, %f160;
	and.b32  	%r8, %r7, 8388607;
	or.b32  	%r9, %r8, 1065353216;
	mov.b32 	 %f162, %r9;
	shr.u32 	%r10, %r7, 23;
	cvt.rn.f32.u32	%f163, %r10;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB39_14:
	neg.f32 	%f268, %f262;

BB39_15:
	setp.gt.s16	%p16, %rs1, -1;
	@%p16 bra 	BB39_37;

	cvt.rmi.f32.f32	%f184, %f1;
	setp.neu.f32	%p17, %f1, %f184;
	@%p17 bra 	BB39_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB39_37;

BB39_18:
	setp.lt.f32	%p18, %f1, 0f1FEC1E4A;
	@%p18 bra 	BB39_33;

	add.f32 	%f185, %f1, %f1;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f1;
	mul.f32 	%f16, %f189, 0f40490FDB;
	mul.rn.f32 	%f17, %f16, %f16;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB39_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f17, %f190;
	bra.uni 	BB39_22;

BB39_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f17, %f192;

BB39_22:
	@%p19 bra 	BB39_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f17, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f17, %f196;
	bra.uni 	BB39_25;

BB39_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f17, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f17, %f199;

BB39_25:
	fma.rn.f32 	%f265, %f264, %f16, %f16;
	@%p19 bra 	BB39_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f17, %f200;

BB39_27:
	and.b32  	%r11, %r1, 2;
	setp.eq.s32	%p22, %r11, 0;
	@%p22 bra 	BB39_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB39_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f29, %f203, %f1;
	setp.gt.f32	%p23, %f29, 0f00000000;
	setp.lt.f32	%p24, %f29, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB39_31;

	lg2.approx.f32 	%f266, %f29;
	bra.uni 	BB39_32;

BB39_31:
	setp.lt.f32	%p26, %f29, 0f00800000;
	mul.f32 	%f206, %f29, 0f4B800000;
	selp.f32	%f207, %f206, %f29, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r12, %f207;
	and.b32  	%r13, %r12, 8388607;
	or.b32  	%r14, %r13, 1065353216;
	mov.b32 	 %f209, %r14;
	shr.u32 	%r15, %r12, 23;
	cvt.rn.f32.u32	%f210, %r15;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB39_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB39_37;

BB39_33:
	setp.gt.f32	%p28, %f1, 0f00000000;
	setp.lt.f32	%p29, %f1, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB39_35;

	lg2.approx.f32 	%f267, %f1;
	bra.uni 	BB39_36;

BB39_35:
	setp.lt.f32	%p31, %f1, 0f00800000;
	mul.f32 	%f235, %f1, 0f4B800000;
	selp.f32	%f236, %f235, %f1, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r16, %f236;
	and.b32  	%r17, %r16, 8388607;
	or.b32  	%r18, %r17, 1065353216;
	mov.b32 	 %f238, %r18;
	shr.u32 	%r19, %r16, 23;
	cvt.rn.f32.u32	%f239, %r19;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB39_36:
	neg.f32 	%f268, %f267;

BB39_37:
	cvt.rzi.s32.f32	%r20, %f268;
	cvt.s32.s8 	%r21, %r20;
	st.param.b32	[funj_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammav(
	.param .b32 ___lgammav_param_0
)
{
	.reg .pred 	%p<16>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<11>;
	.reg .f32 	%f<164>;


	ld.param.u8 	%rs1, [___lgammav_param_0];
	cvt.rn.f32.u16	%f16, %rs1;
	abs.f32 	%f1, %f16;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB40_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB40_6;

	// inline asm
	rcp.approx.ftz.f32 %f17,%f1;
	// inline asm
	mul.f32 	%f19, %f17, %f17;
	mov.f32 	%f20, 0fBB360953;
	mov.f32 	%f21, 0f3A4BE755;
	fma.rn.f32 	%f22, %f21, %f19, %f20;
	mov.f32 	%f23, 0f3DAAAAA3;
	fma.rn.f32 	%f24, %f22, %f19, %f23;
	mov.f32 	%f25, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f24, %f17, %f25;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB40_4;

	lg2.approx.f32 	%f161, %f1;
	bra.uni 	BB40_5;

BB40_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f28, %f1, 0f4B800000;
	selp.f32	%f29, %f28, %f1, %p6;
	selp.f32	%f30, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r1, %f29;
	and.b32  	%r2, %r1, 8388607;
	or.b32  	%r3, %r2, 1065353216;
	mov.b32 	 %f31, %r3;
	shr.u32 	%r4, %r1, 23;
	cvt.rn.f32.u32	%f32, %r4;
	add.f32 	%f33, %f30, %f32;
	setp.gt.f32	%p7, %f31, 0f3FAE147B;
	mul.f32 	%f34, %f31, 0f3F000000;
	add.f32 	%f35, %f33, 0f3F800000;
	selp.f32	%f36, %f34, %f31, %p7;
	selp.f32	%f37, %f35, %f33, %p7;
	add.f32 	%f27, %f36, 0f3F800000;
	add.f32 	%f38, %f36, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f26,%f27;
	// inline asm
	neg.f32 	%f39, %f38;
	mul.f32 	%f40, %f38, %f39;
	mul.rn.f32 	%f41, %f26, %f40;
	add.rn.f32 	%f42, %f38, %f41;
	mul.f32 	%f43, %f42, %f42;
	mov.f32 	%f44, 0f3C4C6A36;
	mov.f32 	%f45, 0f3B1E94E6;
	fma.rn.f32 	%f46, %f45, %f43, %f44;
	mov.f32 	%f47, 0f3DAAAB1A;
	fma.rn.f32 	%f48, %f46, %f43, %f47;
	mul.f32 	%f49, %f48, %f43;
	fma.rn.f32 	%f50, %f49, %f42, %f41;
	add.f32 	%f51, %f50, %f38;
	mov.f32 	%f52, 0f3F317218;
	fma.rn.f32 	%f161, %f37, %f52, %f51;

BB40_5:
	add.f32 	%f53, %f1, 0fBF000000;
	mul.f32 	%f54, %f161, 0f3F000000;
	mul.rn.f32 	%f55, %f54, %f53;
	sub.f32 	%f56, %f55, %f1;
	add.rn.f32 	%f57, %f55, %f2;
	add.f32 	%f58, %f56, %f57;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f163, %f1, %f58, %p8;
	bra.uni 	BB40_15;

BB40_6:
	add.f32 	%f61, %f1, 0fC0400000;
	mov.f32 	%f62, 0fC640F6F8;
	mov.f32 	%f63, 0fC43B38FB;
	fma.rn.f32 	%f64, %f63, %f61, %f62;
	mov.f32 	%f65, 0fC7206560;
	fma.rn.f32 	%f66, %f64, %f61, %f65;
	mov.f32 	%f67, 0fC73CB6AA;
	fma.rn.f32 	%f68, %f66, %f61, %f67;
	mov.f32 	%f69, 0fC80BAE5A;
	fma.rn.f32 	%f70, %f68, %f61, %f69;
	add.f32 	%f71, %f61, 0fC381A020;
	mov.f32 	%f72, 0fC62864B8;
	fma.rn.f32 	%f73, %f71, %f61, %f72;
	mov.f32 	%f74, 0fC7B50686;
	fma.rn.f32 	%f75, %f73, %f61, %f74;
	mov.f32 	%f76, 0fC8498465;
	fma.rn.f32 	%f60, %f75, %f61, %f76;
	// inline asm
	rcp.approx.ftz.f32 %f59,%f60;
	// inline asm
	fma.rn.f32 	%f163, %f70, %f59, %f61;
	bra.uni 	BB40_15;

BB40_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB40_9;

	add.f32 	%f77, %f1, 0fC0000000;
	mov.f32 	%f78, 0fB967A002;
	mov.f32 	%f79, 0f385007FA;
	fma.rn.f32 	%f80, %f79, %f77, %f78;
	mov.f32 	%f81, 0f3A0DE6FC;
	fma.rn.f32 	%f82, %f80, %f77, %f81;
	mov.f32 	%f83, 0fBA9DE0E2;
	fma.rn.f32 	%f84, %f82, %f77, %f83;
	mov.f32 	%f85, 0f3B3D05B7;
	fma.rn.f32 	%f86, %f84, %f77, %f85;
	mov.f32 	%f87, 0fBBF1EB10;
	fma.rn.f32 	%f88, %f86, %f77, %f87;
	mov.f32 	%f89, 0f3CA89A28;
	fma.rn.f32 	%f90, %f88, %f77, %f89;
	mov.f32 	%f91, 0fBD89F01A;
	fma.rn.f32 	%f92, %f90, %f77, %f91;
	mov.f32 	%f93, 0f3EA51A66;
	fma.rn.f32 	%f94, %f92, %f77, %f93;
	mov.f32 	%f95, 0f3ED87730;
	fma.rn.f32 	%f96, %f94, %f77, %f95;
	mul.f32 	%f163, %f96, %f77;
	bra.uni 	BB40_15;

BB40_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB40_11;

	mov.f32 	%f97, 0f3F800000;
	sub.f32 	%f98, %f97, %f1;
	mov.f32 	%f99, 0f3DD47577;
	mov.f32 	%f100, 0f3D3BEF76;
	fma.rn.f32 	%f101, %f100, %f98, %f99;
	mov.f32 	%f102, 0f3DFB8079;
	fma.rn.f32 	%f103, %f101, %f98, %f102;
	mov.f32 	%f104, 0f3E0295B5;
	fma.rn.f32 	%f105, %f103, %f98, %f104;
	mov.f32 	%f106, 0f3E12A765;
	fma.rn.f32 	%f107, %f105, %f98, %f106;
	mov.f32 	%f108, 0f3E2D6867;
	fma.rn.f32 	%f109, %f107, %f98, %f108;
	mov.f32 	%f110, 0f3E5462BF;
	fma.rn.f32 	%f111, %f109, %f98, %f110;
	mov.f32 	%f112, 0f3E8A8A72;
	fma.rn.f32 	%f113, %f111, %f98, %f112;
	mov.f32 	%f114, 0f3ECD26A4;
	fma.rn.f32 	%f115, %f113, %f98, %f114;
	mov.f32 	%f116, 0f3F528D32;
	fma.rn.f32 	%f117, %f115, %f98, %f116;
	mov.f32 	%f118, 0f3F13C468;
	fma.rn.f32 	%f119, %f117, %f98, %f118;
	mul.f32 	%f163, %f119, %f98;
	bra.uni 	BB40_15;

BB40_11:
	mov.f32 	%f120, 0fBBB34878;
	mov.f32 	%f121, 0f3B6B1C86;
	fma.rn.f32 	%f122, %f121, %f1, %f120;
	mov.f32 	%f123, 0fBD36CAEF;
	fma.rn.f32 	%f124, %f122, %f1, %f123;
	mov.f32 	%f125, 0f3E2B5555;
	fma.rn.f32 	%f126, %f124, %f1, %f125;
	mov.f32 	%f127, 0fBD2C96C7;
	fma.rn.f32 	%f128, %f126, %f1, %f127;
	mov.f32 	%f129, 0fBF27E6EB;
	fma.rn.f32 	%f130, %f128, %f1, %f129;
	mov.f32 	%f131, 0f3F13C463;
	fma.rn.f32 	%f132, %f130, %f1, %f131;
	mul.f32 	%f133, %f132, %f1;
	fma.rn.f32 	%f10, %f133, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB40_13;

	lg2.approx.f32 	%f162, %f10;
	bra.uni 	BB40_14;

BB40_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f136, %f10, 0f4B800000;
	selp.f32	%f137, %f136, %f10, %p14;
	selp.f32	%f138, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r5, %f137;
	and.b32  	%r6, %r5, 8388607;
	or.b32  	%r7, %r6, 1065353216;
	mov.b32 	 %f139, %r7;
	shr.u32 	%r8, %r5, 23;
	cvt.rn.f32.u32	%f140, %r8;
	add.f32 	%f141, %f138, %f140;
	setp.gt.f32	%p15, %f139, 0f3FAE147B;
	mul.f32 	%f142, %f139, 0f3F000000;
	add.f32 	%f143, %f141, 0f3F800000;
	selp.f32	%f144, %f142, %f139, %p15;
	selp.f32	%f145, %f143, %f141, %p15;
	add.f32 	%f135, %f144, 0f3F800000;
	add.f32 	%f146, %f144, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f134,%f135;
	// inline asm
	neg.f32 	%f147, %f146;
	mul.f32 	%f148, %f146, %f147;
	mul.rn.f32 	%f149, %f134, %f148;
	add.rn.f32 	%f150, %f146, %f149;
	mul.f32 	%f151, %f150, %f150;
	mov.f32 	%f152, 0f3C4C6A36;
	mov.f32 	%f153, 0f3B1E94E6;
	fma.rn.f32 	%f154, %f153, %f151, %f152;
	mov.f32 	%f155, 0f3DAAAB1A;
	fma.rn.f32 	%f156, %f154, %f151, %f155;
	mul.f32 	%f157, %f156, %f151;
	fma.rn.f32 	%f158, %f157, %f150, %f149;
	add.f32 	%f159, %f158, %f146;
	mov.f32 	%f160, 0f3F317218;
	fma.rn.f32 	%f162, %f145, %f160, %f159;

BB40_14:
	neg.f32 	%f163, %f162;

BB40_15:
	cvt.rzi.u32.f32	%r9, %f163;
	and.b32  	%r10, %r9, 255;
	st.param.b32	[funj_retval0+0], %r10;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absc(
	.param .align 8 .b8 ___absc_param_0[8]
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<13>;


	ld.param.f32 	%f1, [___absc_param_0+4];
	ld.param.f32 	%f2, [___absc_param_0];
	abs.f32 	%f3, %f2;
	abs.f32 	%f4, %f1;
	setp.gt.f32	%p1, %f3, %f4;
	selp.f32	%f5, %f3, %f4, %p1;
	selp.f32	%f6, %f4, %f3, %p1;
	div.rn.f32 	%f7, %f6, %f5;
	fma.rn.f32 	%f8, %f7, %f7, 0f3F800000;
	sqrt.rn.f32 	%f9, %f8;
	mul.f32 	%f10, %f5, %f9;
	setp.eq.f32	%p2, %f5, 0f00000000;
	setp.gt.f32	%p3, %f5, 0f7F7FFFFF;
	or.pred  	%p4, %p2, %p3;
	setp.gt.f32	%p5, %f6, 0f7F7FFFFF;
	or.pred  	%p6, %p4, %p5;
	add.f32 	%f11, %f5, %f6;
	selp.f32	%f12, %f11, %f10, %p6;
	st.param.f32	[funj_retval0+0], %f12;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___absz(
	.param .align 16 .b8 ___absz_param_0[16]
)
{
	.reg .pred 	%p<7>;
	.reg .f64 	%fd<13>;


	ld.param.f64 	%fd1, [___absz_param_0+8];
	ld.param.f64 	%fd2, [___absz_param_0];
	abs.f64 	%fd3, %fd2;
	abs.f64 	%fd4, %fd1;
	setp.gt.f64	%p1, %fd3, %fd4;
	selp.f64	%fd5, %fd3, %fd4, %p1;
	selp.f64	%fd6, %fd4, %fd3, %p1;
	div.rn.f64 	%fd7, %fd6, %fd5;
	fma.rn.f64 	%fd8, %fd7, %fd7, 0d3FF0000000000000;
	sqrt.rn.f64 	%fd9, %fd8;
	mul.f64 	%fd10, %fd5, %fd9;
	setp.eq.f64	%p2, %fd5, 0d0000000000000000;
	setp.gt.f64	%p3, %fd5, 0d7FEFFFFFFFFFFFFF;
	or.pred  	%p4, %p2, %p3;
	setp.gt.f64	%p5, %fd6, 0d7FEFFFFFFFFFFFFF;
	or.pred  	%p6, %p4, %p5;
	add.f64 	%fd11, %fd5, %fd6;
	selp.f64	%fd12, %fd11, %fd10, %p6;
	st.param.f64	[funj_retval0+0], %fd12;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3remdd(
	.param .b64 _Z3remdd_param_0,
	.param .b64 _Z3remdd_param_1
)
{
	.reg .pred 	%p<24>;
	.reg .s32 	%r<35>;
	.reg .f64 	%fd<39>;


	ld.param.f64 	%fd22, [_Z3remdd_param_0];
	ld.param.f64 	%fd23, [_Z3remdd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd22;
	}
	and.b32  	%r12, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd23;
	}
	and.b32  	%r31, %r13, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd22;
	}
	mov.b64 	%fd37, {%r14, %r12};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd23;
	}
	mov.b64 	%fd2, {%r32, %r31};
	setp.gt.u32	%p1, %r12, 2146435071;
	setp.gt.u32	%p2, %r31, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB43_22;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB43_3;

	mov.f64 	%fd38, 0dFFF8000000000000;
	bra.uni 	BB43_25;

BB43_3:
	setp.ge.f64	%p5, %fd37, %fd2;
	@%p5 bra 	BB43_5;

	mov.u32 	%r34, 0;
	bra.uni 	BB43_18;

BB43_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.s32	%p6, %r15, 1048576;
	@%p6 bra 	BB43_7;

	mov.f64 	%fd33, 0d0000000000000000;
	bra.uni 	BB43_11;

BB43_7:
	setp.geu.f64	%p7, %fd2, %fd37;
	mov.f64 	%fd34, %fd2;
	@%p7 bra 	BB43_10;

	mov.f64 	%fd35, %fd2;

BB43_9:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p8, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd35;
	}
	setp.lt.s32	%p9, %r16, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p10 bra 	BB43_9;

BB43_10:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd33;
	}

BB43_11:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd37;
	}
	setp.lt.s32	%p11, %r17, 1048576;
	@%p11 bra 	BB43_13;

	and.b32  	%r18, %r31, 1048575;
	and.b32  	%r19, %r1, 2146435072;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd32, {%r32, %r20};

BB43_13:
	mul.f64 	%fd25, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd32, %fd37;
	selp.f64	%fd36, %fd25, %fd32, %p12;
	setp.ge.f64	%p13, %fd36, %fd2;
	@%p13 bra 	BB43_15;

	mov.u32 	%r34, 0;
	bra.uni 	BB43_18;

BB43_15:
	mov.u32 	%r33, -1;

BB43_16:
	setp.ltu.f64	%p14, %fd37, %fd36;
	selp.u32	%r22, 1, 0, %p14;
	shl.b32 	%r23, %r33, 1;
	add.s32 	%r33, %r22, %r23;
	sub.f64 	%fd26, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd26, %p14;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd36, %fd2;
	@%p15 bra 	BB43_16;

	not.b32 	%r24, %r33;
	and.b32  	%r34, %r24, 1;

BB43_18:
	add.f64 	%fd15, %fd37, %fd37;
	setp.gt.f64	%p16, %fd15, %fd2;
	@%p16 bra 	BB43_20;

	setp.eq.f64	%p17, %fd15, %fd2;
	setp.ne.s32	%p18, %r34, 0;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB43_21;
	bra.uni 	BB43_20;

BB43_20:
	sub.f64 	%fd37, %fd37, %fd2;

BB43_21:
	and.b32  	%r27, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd37;
	}
	xor.b32  	%r29, %r28, %r27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd37;
	}
	mov.b64 	%fd38, {%r30, %r29};
	bra.uni 	BB43_25;

BB43_22:
	setp.gtu.f64	%p20, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p21, %fd2, 0d7FF0000000000000;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	BB43_24;

	setp.eq.f64	%p23, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd22, %p23;
	bra.uni 	BB43_25;

BB43_24:
	add.f64 	%fd38, %fd22, %fd23;

BB43_25:
	st.param.f64	[funj_retval0+0], %fd38;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3moddd(
	.param .b64 _Z3moddd_param_0,
	.param .b64 _Z3moddd_param_1
)
{
	.reg .pred 	%p<20>;
	.reg .s32 	%r<23>;
	.reg .f64 	%fd<36>;


	ld.param.f64 	%fd35, [_Z3moddd_param_0];
	ld.param.f64 	%fd20, [_Z3moddd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd35;
	}
	and.b32  	%r8, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd20;
	}
	and.b32  	%r21, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd35;
	}
	mov.b64 	%fd34, {%r10, %r8};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd20;
	}
	mov.b64 	%fd2, {%r22, %r21};
	setp.gt.u32	%p1, %r8, 2146435071;
	setp.gt.u32	%p2, %r21, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB44_15;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB44_3;

	mov.f64 	%fd35, 0dFFF8000000000000;
	bra.uni 	BB44_18;

BB44_3:
	setp.ltu.f64	%p5, %fd34, %fd2;
	@%p5 bra 	BB44_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd2;
	}
	setp.lt.s32	%p6, %r11, 1048576;
	@%p6 bra 	BB44_6;

	mov.f64 	%fd30, 0d0000000000000000;
	bra.uni 	BB44_10;

BB44_6:
	setp.geu.f64	%p7, %fd2, %fd34;
	mov.f64 	%fd31, %fd2;
	@%p7 bra 	BB44_9;

	mov.f64 	%fd32, %fd2;

BB44_8:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p8, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd32;
	}
	setp.lt.s32	%p9, %r12, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p10 bra 	BB44_8;

BB44_9:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd30;
	}

BB44_10:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd34;
	}
	setp.lt.s32	%p11, %r13, 1048576;
	@%p11 bra 	BB44_12;

	and.b32  	%r14, %r21, 1048575;
	and.b32  	%r15, %r1, 2146435072;
	or.b32  	%r16, %r14, %r15;
	mov.b64 	%fd29, {%r22, %r16};

BB44_12:
	mul.f64 	%fd22, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd29, %fd34;
	selp.f64	%fd33, %fd22, %fd29, %p12;
	setp.ltu.f64	%p13, %fd33, %fd2;
	@%p13 bra 	BB44_14;

BB44_13:
	sub.f64 	%fd23, %fd34, %fd33;
	setp.ltu.f64	%p14, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd23, %p14;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd33, %fd2;
	@%p15 bra 	BB44_13;

BB44_14:
	and.b32  	%r17, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd34;
	}
	or.b32  	%r19, %r18, %r17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r20, %r19};
	bra.uni 	BB44_18;

BB44_15:
	setp.gtu.f64	%p16, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p17, %fd2, 0d7FF0000000000000;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	BB44_17;

	setp.eq.f64	%p19, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd35, %p19;
	bra.uni 	BB44_18;

BB44_17:
	add.f64 	%fd35, %fd35, %fd20;

BB44_18:
	st.param.f64	[funj_retval0+0], %fd35;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minss(
	.param .b32 ___minss_param_0,
	.param .b32 ___minss_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___minss_param_0];
	ld.param.f32 	%f2, [___minss_param_1];
	min.f32 	%f3, %f1, %f2;
	st.param.f32	[funj_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minii(
	.param .b32 ___minii_param_0,
	.param .b32 ___minii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___minii_param_0];
	ld.param.u32 	%r2, [___minii_param_1];
	min.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minuu(
	.param .b32 ___minuu_param_0,
	.param .b32 ___minuu_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___minuu_param_0];
	ld.param.u32 	%r2, [___minuu_param_1];
	min.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minjj(
	.param .b32 ___minjj_param_0,
	.param .b32 ___minjj_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___minjj_param_0];
	ld.param.s8 	%rs2, [___minjj_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	min.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minvv(
	.param .b32 ___minvv_param_0,
	.param .b32 ___minvv_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___minvv_param_0];
	ld.param.u8 	%rs2, [___minvv_param_1];
	cvt.u32.u16	%r1, %rs2;
	cvt.u32.u16	%r2, %rs1;
	min.s32 	%r3, %r2, %r1;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___mindd(
	.param .b64 ___mindd_param_0,
	.param .b64 ___mindd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___mindd_param_0];
	ld.param.f64 	%fd2, [___mindd_param_1];
	min.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[funj_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxss(
	.param .b32 ___maxss_param_0,
	.param .b32 ___maxss_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___maxss_param_0];
	ld.param.f32 	%f2, [___maxss_param_1];
	max.f32 	%f3, %f1, %f2;
	st.param.f32	[funj_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxii(
	.param .b32 ___maxii_param_0,
	.param .b32 ___maxii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___maxii_param_0];
	ld.param.u32 	%r2, [___maxii_param_1];
	max.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxuu(
	.param .b32 ___maxuu_param_0,
	.param .b32 ___maxuu_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___maxuu_param_0];
	ld.param.u32 	%r2, [___maxuu_param_1];
	max.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxjj(
	.param .b32 ___maxjj_param_0,
	.param .b32 ___maxjj_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___maxjj_param_0];
	ld.param.s8 	%rs2, [___maxjj_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	max.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxvv(
	.param .b32 ___maxvv_param_0,
	.param .b32 ___maxvv_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___maxvv_param_0];
	ld.param.u8 	%rs2, [___maxvv_param_1];
	cvt.u32.u16	%r1, %rs2;
	cvt.u32.u16	%r2, %rs1;
	max.s32 	%r3, %r2, %r1;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___maxdd(
	.param .b64 ___maxdd_param_0,
	.param .b64 ___maxdd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___maxdd_param_0];
	ld.param.f64 	%fd2, [___maxdd_param_1];
	max.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[funj_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modss(
	.param .b32 ___modss_param_0,
	.param .b32 ___modss_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<18>;
	.reg .f32 	%f<48>;


	ld.param.f32 	%f18, [___modss_param_0];
	ld.param.f32 	%f19, [___modss_param_1];
	abs.f32 	%f46, %f18;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	abs.f32 	%f2, %f19;
	setp.eq.f32	%p2, %f2, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB57_2;
	bra.uni 	BB57_1;

BB57_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB57_14;

BB57_2:
	setp.ltu.f32	%p4, %f46, %f2;
	@%p4 bra 	BB57_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f2;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p5, %f3, 0f00000000;
	setp.eq.f32	%p6, %f3, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB57_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB57_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB57_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f2, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB57_10;

BB57_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f2, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB57_10;

BB57_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f2, %f34;
	bra.uni 	BB57_10;

BB57_9:
	setp.leu.f32	%p12, %f3, 0f00000000;
	add.f32 	%f36, %f2, %f2;
	selp.f32	%f44, %f36, %f2, %p12;

BB57_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f2;
	@%p14 bra 	BB57_12;

BB57_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f2;
	@%p16 bra 	BB57_11;

BB57_12:
	mov.b32 	 %r14, %f18;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB57_14;

BB57_13:
	setp.gtu.f32	%p17, %f2, 0f7F800000;
	add.f32 	%f40, %f18, %f19;
	selp.f32	%f41, %f40, %f18, %p17;
	add.f32 	%f42, %f41, %f18;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB57_14:
	st.param.f32	[funj_retval0+0], %f47;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIfET_S0_S0_(
	.param .b32 _Z3modIfET_S0_S0__param_0,
	.param .b32 _Z3modIfET_S0_S0__param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<18>;
	.reg .f32 	%f<48>;


	ld.param.f32 	%f18, [_Z3modIfET_S0_S0__param_0];
	ld.param.f32 	%f19, [_Z3modIfET_S0_S0__param_1];
	abs.f32 	%f46, %f18;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	abs.f32 	%f2, %f19;
	setp.eq.f32	%p2, %f2, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB58_2;
	bra.uni 	BB58_1;

BB58_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB58_14;

BB58_2:
	setp.ltu.f32	%p4, %f46, %f2;
	@%p4 bra 	BB58_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f2;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p5, %f3, 0f00000000;
	setp.eq.f32	%p6, %f3, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB58_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB58_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB58_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f2, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB58_10;

BB58_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f2, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB58_10;

BB58_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f2, %f34;
	bra.uni 	BB58_10;

BB58_9:
	setp.leu.f32	%p12, %f3, 0f00000000;
	add.f32 	%f36, %f2, %f2;
	selp.f32	%f44, %f36, %f2, %p12;

BB58_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f2;
	@%p14 bra 	BB58_12;

BB58_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f2;
	@%p16 bra 	BB58_11;

BB58_12:
	mov.b32 	 %r14, %f18;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB58_14;

BB58_13:
	setp.gtu.f32	%p17, %f2, 0f7F800000;
	add.f32 	%f40, %f18, %f19;
	selp.f32	%f41, %f40, %f18, %p17;
	add.f32 	%f42, %f41, %f18;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB58_14:
	st.param.f32	[funj_retval0+0], %f47;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modii(
	.param .b32 ___modii_param_0,
	.param .b32 ___modii_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<21>;
	.reg .f32 	%f<48>;


	ld.param.u32 	%r3, [___modii_param_0];
	ld.param.u32 	%r4, [___modii_param_1];
	cvt.rn.f32.s32	%f1, %r3;
	abs.f32 	%f46, %f1;
	cvt.rn.f32.s32	%f3, %r4;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB59_2;
	bra.uni 	BB59_1;

BB59_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB59_14;

BB59_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB59_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r5, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r6, %f21;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r5, %r6;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB59_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB59_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB59_7;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f23, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f25, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB59_10;

BB59_7:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f30, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f32, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB59_10;

BB59_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB59_10;

BB59_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB59_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB59_12;

BB59_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB59_11;

BB59_12:
	mov.b32 	 %r16, %f1;
	and.b32  	%r17, %r16, -2147483648;
	mov.b32 	 %r18, %f46;
	or.b32  	%r19, %r18, %r17;
	mov.b32 	 %f47, %r19;
	bra.uni 	BB59_14;

BB59_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB59_14:
	cvt.rzi.s32.f32	%r20, %f47;
	st.param.b32	[funj_retval0+0], %r20;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIiET_S0_S0_(
	.param .b32 _Z3modIiET_S0_S0__param_0,
	.param .b32 _Z3modIiET_S0_S0__param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<21>;
	.reg .f32 	%f<48>;


	ld.param.u32 	%r3, [_Z3modIiET_S0_S0__param_0];
	ld.param.u32 	%r4, [_Z3modIiET_S0_S0__param_1];
	cvt.rn.f32.s32	%f1, %r3;
	abs.f32 	%f46, %f1;
	cvt.rn.f32.s32	%f3, %r4;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB60_2;
	bra.uni 	BB60_1;

BB60_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB60_14;

BB60_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB60_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r5, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r6, %f21;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r5, %r6;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB60_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB60_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB60_7;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f23, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f25, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB60_10;

BB60_7:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f30, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f32, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB60_10;

BB60_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB60_10;

BB60_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB60_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB60_12;

BB60_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB60_11;

BB60_12:
	mov.b32 	 %r16, %f1;
	and.b32  	%r17, %r16, -2147483648;
	mov.b32 	 %r18, %f46;
	or.b32  	%r19, %r18, %r17;
	mov.b32 	 %f47, %r19;
	bra.uni 	BB60_14;

BB60_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB60_14:
	cvt.rzi.s32.f32	%r20, %f47;
	st.param.b32	[funj_retval0+0], %r20;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___moduu(
	.param .b32 ___moduu_param_0,
	.param .b32 ___moduu_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<21>;
	.reg .f32 	%f<48>;


	ld.param.u32 	%r3, [___moduu_param_0];
	ld.param.u32 	%r4, [___moduu_param_1];
	cvt.rn.f32.u32	%f1, %r3;
	abs.f32 	%f46, %f1;
	cvt.rn.f32.u32	%f3, %r4;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB61_2;
	bra.uni 	BB61_1;

BB61_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB61_14;

BB61_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB61_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r5, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r6, %f21;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r5, %r6;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB61_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB61_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB61_7;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f23, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f25, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB61_10;

BB61_7:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f30, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f32, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB61_10;

BB61_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB61_10;

BB61_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB61_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB61_12;

BB61_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB61_11;

BB61_12:
	mov.b32 	 %r16, %f1;
	and.b32  	%r17, %r16, -2147483648;
	mov.b32 	 %r18, %f46;
	or.b32  	%r19, %r18, %r17;
	mov.b32 	 %f47, %r19;
	bra.uni 	BB61_14;

BB61_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB61_14:
	cvt.rzi.u32.f32	%r20, %f47;
	st.param.b32	[funj_retval0+0], %r20;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIjET_S0_S0_(
	.param .b32 _Z3modIjET_S0_S0__param_0,
	.param .b32 _Z3modIjET_S0_S0__param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<21>;
	.reg .f32 	%f<48>;


	ld.param.u32 	%r3, [_Z3modIjET_S0_S0__param_0];
	ld.param.u32 	%r4, [_Z3modIjET_S0_S0__param_1];
	cvt.rn.f32.u32	%f1, %r3;
	abs.f32 	%f46, %f1;
	cvt.rn.f32.u32	%f3, %r4;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB62_2;
	bra.uni 	BB62_1;

BB62_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB62_14;

BB62_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB62_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r5, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r6, %f21;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r5, %r6;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB62_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB62_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB62_7;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f23, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f25, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB62_10;

BB62_7:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f30, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f32, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB62_10;

BB62_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB62_10;

BB62_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB62_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB62_12;

BB62_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB62_11;

BB62_12:
	mov.b32 	 %r16, %f1;
	and.b32  	%r17, %r16, -2147483648;
	mov.b32 	 %r18, %f46;
	or.b32  	%r19, %r18, %r17;
	mov.b32 	 %f47, %r19;
	bra.uni 	BB62_14;

BB62_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB62_14:
	cvt.rzi.u32.f32	%r20, %f47;
	st.param.b32	[funj_retval0+0], %r20;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modjj(
	.param .b32 ___modjj_param_0,
	.param .b32 ___modjj_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<48>;


	ld.param.s8 	%rs1, [___modjj_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f46, %f1;
	ld.param.s8 	%rs2, [___modjj_param_1];
	cvt.rn.f32.s16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB63_2;
	bra.uni 	BB63_1;

BB63_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB63_14;

BB63_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB63_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB63_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB63_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB63_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB63_10;

BB63_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB63_10;

BB63_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB63_10;

BB63_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB63_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB63_12;

BB63_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB63_11;

BB63_12:
	mov.b32 	 %r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB63_14;

BB63_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB63_14:
	cvt.rzi.s32.f32	%r18, %f47;
	cvt.s32.s8 	%r19, %r18;
	st.param.b32	[funj_retval0+0], %r19;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIcET_S0_S0_(
	.param .b32 _Z3modIcET_S0_S0__param_0,
	.param .b32 _Z3modIcET_S0_S0__param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<48>;


	ld.param.s8 	%rs1, [_Z3modIcET_S0_S0__param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f46, %f1;
	ld.param.s8 	%rs2, [_Z3modIcET_S0_S0__param_1];
	cvt.rn.f32.s16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB64_2;
	bra.uni 	BB64_1;

BB64_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB64_14;

BB64_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB64_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB64_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB64_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB64_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB64_10;

BB64_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB64_10;

BB64_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB64_10;

BB64_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB64_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB64_12;

BB64_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB64_11;

BB64_12:
	mov.b32 	 %r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB64_14;

BB64_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB64_14:
	cvt.rzi.s32.f32	%r18, %f47;
	cvt.s32.s8 	%r19, %r18;
	st.param.b32	[funj_retval0+0], %r19;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modvv(
	.param .b32 ___modvv_param_0,
	.param .b32 ___modvv_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<48>;


	ld.param.u8 	%rs1, [___modvv_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f46, %f1;
	ld.param.u8 	%rs2, [___modvv_param_1];
	cvt.rn.f32.u16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB65_2;
	bra.uni 	BB65_1;

BB65_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB65_14;

BB65_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB65_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB65_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB65_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB65_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB65_10;

BB65_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB65_10;

BB65_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB65_10;

BB65_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB65_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB65_12;

BB65_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB65_11;

BB65_12:
	mov.b32 	 %r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB65_14;

BB65_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB65_14:
	cvt.rzi.u32.f32	%r18, %f47;
	and.b32  	%r19, %r18, 255;
	st.param.b32	[funj_retval0+0], %r19;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIhET_S0_S0_(
	.param .b32 _Z3modIhET_S0_S0__param_0,
	.param .b32 _Z3modIhET_S0_S0__param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<48>;


	ld.param.u8 	%rs1, [_Z3modIhET_S0_S0__param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f46, %f1;
	ld.param.u8 	%rs2, [_Z3modIhET_S0_S0__param_1];
	cvt.rn.f32.u16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	setp.eq.f32	%p2, %f4, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB66_2;
	bra.uni 	BB66_1;

BB66_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB66_14;

BB66_2:
	setp.ltu.f32	%p4, %f46, %f4;
	@%p4 bra 	BB66_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f4;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p5, %f5, 0f00000000;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB66_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB66_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB66_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f4, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB66_10;

BB66_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f4, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB66_10;

BB66_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f4, %f34;
	bra.uni 	BB66_10;

BB66_9:
	setp.leu.f32	%p12, %f5, 0f00000000;
	add.f32 	%f36, %f4, %f4;
	selp.f32	%f44, %f36, %f4, %p12;

BB66_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f4;
	@%p14 bra 	BB66_12;

BB66_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f4;
	@%p16 bra 	BB66_11;

BB66_12:
	mov.b32 	 %r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB66_14;

BB66_13:
	setp.gtu.f32	%p17, %f4, 0f7F800000;
	add.f32 	%f40, %f1, %f3;
	selp.f32	%f41, %f40, %f1, %p17;
	add.f32 	%f42, %f41, %f1;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB66_14:
	cvt.rzi.u32.f32	%r18, %f47;
	and.b32  	%r19, %r18, 255;
	st.param.b32	[funj_retval0+0], %r19;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___moddd(
	.param .b64 ___moddd_param_0,
	.param .b64 ___moddd_param_1
)
{
	.reg .pred 	%p<20>;
	.reg .s32 	%r<23>;
	.reg .f64 	%fd<36>;


	ld.param.f64 	%fd35, [___moddd_param_0];
	ld.param.f64 	%fd20, [___moddd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd35;
	}
	and.b32  	%r8, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd20;
	}
	and.b32  	%r21, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd35;
	}
	mov.b64 	%fd34, {%r10, %r8};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd20;
	}
	mov.b64 	%fd2, {%r22, %r21};
	setp.gt.u32	%p1, %r8, 2146435071;
	setp.gt.u32	%p2, %r21, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB67_15;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB67_3;

	mov.f64 	%fd35, 0dFFF8000000000000;
	bra.uni 	BB67_18;

BB67_3:
	setp.ltu.f64	%p5, %fd34, %fd2;
	@%p5 bra 	BB67_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd2;
	}
	setp.lt.s32	%p6, %r11, 1048576;
	@%p6 bra 	BB67_6;

	mov.f64 	%fd30, 0d0000000000000000;
	bra.uni 	BB67_10;

BB67_6:
	setp.geu.f64	%p7, %fd2, %fd34;
	mov.f64 	%fd31, %fd2;
	@%p7 bra 	BB67_9;

	mov.f64 	%fd32, %fd2;

BB67_8:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p8, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd32;
	}
	setp.lt.s32	%p9, %r12, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p10 bra 	BB67_8;

BB67_9:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd30;
	}

BB67_10:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd34;
	}
	setp.lt.s32	%p11, %r13, 1048576;
	@%p11 bra 	BB67_12;

	and.b32  	%r14, %r21, 1048575;
	and.b32  	%r15, %r1, 2146435072;
	or.b32  	%r16, %r14, %r15;
	mov.b64 	%fd29, {%r22, %r16};

BB67_12:
	mul.f64 	%fd22, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd29, %fd34;
	selp.f64	%fd33, %fd22, %fd29, %p12;
	setp.ltu.f64	%p13, %fd33, %fd2;
	@%p13 bra 	BB67_14;

BB67_13:
	sub.f64 	%fd23, %fd34, %fd33;
	setp.ltu.f64	%p14, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd23, %p14;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd33, %fd2;
	@%p15 bra 	BB67_13;

BB67_14:
	and.b32  	%r17, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd34;
	}
	or.b32  	%r19, %r18, %r17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r20, %r19};
	bra.uni 	BB67_18;

BB67_15:
	setp.gtu.f64	%p16, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p17, %fd2, 0d7FF0000000000000;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	BB67_17;

	setp.eq.f64	%p19, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd35, %p19;
	bra.uni 	BB67_18;

BB67_17:
	add.f64 	%fd35, %fd35, %fd20;

BB67_18:
	st.param.f64	[funj_retval0+0], %fd35;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___remss(
	.param .b32 ___remss_param_0,
	.param .b32 ___remss_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<23>;
	.reg .f32 	%f<54>;


	ld.param.f32 	%f21, [___remss_param_0];
	ld.param.f32 	%f22, [___remss_param_1];
	abs.f32 	%f51, %f21;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	abs.f32 	%f2, %f22;
	setp.gtu.f32	%p2, %f2, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB68_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f2, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB68_3;
	bra.uni 	BB68_2;

BB68_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB68_21;

BB68_3:
	setp.ge.f32	%p7, %f51, %f2;
	@%p7 bra 	BB68_5;

	mov.u32 	%r22, 0;
	bra.uni 	BB68_16;

BB68_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f2;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p8, %f3, 0f00000000;
	setp.eq.f32	%p9, %f3, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB68_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB68_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB68_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f2, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB68_12;

BB68_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f2, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB68_12;

BB68_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f2, %f37;
	bra.uni 	BB68_12;

BB68_11:
	setp.leu.f32	%p15, %f3, 0f00000000;
	add.f32 	%f39, %f2, %f2;
	selp.f32	%f44, %f39, %f2, %p15;

BB68_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f2;
	@%p17 bra 	BB68_13;
	bra.uni 	BB68_22;

BB68_13:
	mov.f32 	%f52, %f51;

BB68_14:
	mov.f32 	%f10, %f52;
	mov.f32 	%f11, %f45;
	sub.f32 	%f42, %f10, %f11;
	setp.ltu.f32	%p18, %f10, %f11;
	selp.f32	%f52, %f10, %f42, %p18;
	mul.f32 	%f45, %f11, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f2;
	@%p19 bra 	BB68_14;

	setp.ge.f32	%p20, %f10, %f11;
	selp.u32	%r22, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB68_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f15, %f50, %f50;
	setp.gt.f32	%p21, %f15, %f2;
	@%p21 bra 	BB68_18;

	setp.eq.f32	%p22, %f15, %f2;
	setp.ne.s32	%p23, %r22, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB68_19;
	bra.uni 	BB68_18;

BB68_18:
	sub.f32 	%f50, %f50, %f2;

BB68_19:
	mov.b32 	 %r18, %f21;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB68_21;

BB68_20:
	add.f32 	%f53, %f21, %f22;

BB68_21:
	st.param.f32	[funj_retval0+0], %f53;
	ret;

BB68_22:
	mov.u32 	%r22, 0;
	bra.uni 	BB68_16;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIfET_S0_S0_(
	.param .b32 _Z3remIfET_S0_S0__param_0,
	.param .b32 _Z3remIfET_S0_S0__param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<23>;
	.reg .f32 	%f<54>;


	ld.param.f32 	%f21, [_Z3remIfET_S0_S0__param_0];
	ld.param.f32 	%f22, [_Z3remIfET_S0_S0__param_1];
	abs.f32 	%f51, %f21;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	abs.f32 	%f2, %f22;
	setp.gtu.f32	%p2, %f2, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB69_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f2, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB69_3;
	bra.uni 	BB69_2;

BB69_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB69_21;

BB69_3:
	setp.ge.f32	%p7, %f51, %f2;
	@%p7 bra 	BB69_5;

	mov.u32 	%r22, 0;
	bra.uni 	BB69_16;

BB69_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f2;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p8, %f3, 0f00000000;
	setp.eq.f32	%p9, %f3, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB69_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB69_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB69_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f2, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB69_12;

BB69_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f2, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB69_12;

BB69_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f2, %f37;
	bra.uni 	BB69_12;

BB69_11:
	setp.leu.f32	%p15, %f3, 0f00000000;
	add.f32 	%f39, %f2, %f2;
	selp.f32	%f44, %f39, %f2, %p15;

BB69_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f2;
	@%p17 bra 	BB69_13;
	bra.uni 	BB69_22;

BB69_13:
	mov.f32 	%f52, %f51;

BB69_14:
	mov.f32 	%f10, %f52;
	mov.f32 	%f11, %f45;
	sub.f32 	%f42, %f10, %f11;
	setp.ltu.f32	%p18, %f10, %f11;
	selp.f32	%f52, %f10, %f42, %p18;
	mul.f32 	%f45, %f11, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f2;
	@%p19 bra 	BB69_14;

	setp.ge.f32	%p20, %f10, %f11;
	selp.u32	%r22, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB69_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f15, %f50, %f50;
	setp.gt.f32	%p21, %f15, %f2;
	@%p21 bra 	BB69_18;

	setp.eq.f32	%p22, %f15, %f2;
	setp.ne.s32	%p23, %r22, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB69_19;
	bra.uni 	BB69_18;

BB69_18:
	sub.f32 	%f50, %f50, %f2;

BB69_19:
	mov.b32 	 %r18, %f21;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB69_21;

BB69_20:
	add.f32 	%f53, %f21, %f22;

BB69_21:
	st.param.f32	[funj_retval0+0], %f53;
	ret;

BB69_22:
	mov.u32 	%r22, 0;
	bra.uni 	BB69_16;
}

.visible .func  (.param .b32 funj_retval0) ___remii(
	.param .b32 ___remii_param_0,
	.param .b32 ___remii_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<26>;
	.reg .f32 	%f<54>;


	ld.param.u32 	%r5, [___remii_param_0];
	ld.param.u32 	%r6, [___remii_param_1];
	cvt.rn.f32.s32	%f1, %r5;
	abs.f32 	%f51, %f1;
	cvt.rn.f32.s32	%f3, %r6;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB70_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB70_3;
	bra.uni 	BB70_2;

BB70_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB70_21;

BB70_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB70_5;

	mov.u32 	%r25, 0;
	bra.uni 	BB70_16;

BB70_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r7, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r8, %f24;
	sub.s32 	%r1, %r7, %r8;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r7, %r8;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB70_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB70_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB70_9;

	shr.s32 	%r9, %r1, 31;
	shr.u32 	%r10, %r9, 30;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 2;
	cvt.rn.f32.s32	%f26, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r13, %r12, -3, %r1;
	cvt.rn.f32.s32	%f28, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB70_12;

BB70_9:
	shr.u32 	%r14, %r1, 31;
	add.s32 	%r15, %r1, %r14;
	shr.s32 	%r16, %r15, 1;
	cvt.rn.f32.s32	%f33, %r16;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r17, %r1, %r16;
	cvt.rn.f32.s32	%f35, %r17;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB70_12;

BB70_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB70_12;

BB70_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB70_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB70_13;
	bra.uni 	BB70_22;

BB70_13:
	mov.f32 	%f52, %f51;

BB70_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB70_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r25, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB70_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB70_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r25, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB70_19;
	bra.uni 	BB70_18;

BB70_18:
	sub.f32 	%f50, %f50, %f4;

BB70_19:
	mov.b32 	 %r20, %f1;
	and.b32  	%r21, %r20, -2147483648;
	mov.b32 	 %r22, %f50;
	xor.b32  	%r23, %r22, %r21;
	mov.b32 	 %f53, %r23;
	bra.uni 	BB70_21;

BB70_20:
	add.f32 	%f53, %f1, %f3;

BB70_21:
	cvt.rzi.s32.f32	%r24, %f53;
	st.param.b32	[funj_retval0+0], %r24;
	ret;

BB70_22:
	mov.u32 	%r25, 0;
	bra.uni 	BB70_16;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIiET_S0_S0_(
	.param .b32 _Z3remIiET_S0_S0__param_0,
	.param .b32 _Z3remIiET_S0_S0__param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<26>;
	.reg .f32 	%f<54>;


	ld.param.u32 	%r5, [_Z3remIiET_S0_S0__param_0];
	ld.param.u32 	%r6, [_Z3remIiET_S0_S0__param_1];
	cvt.rn.f32.s32	%f1, %r5;
	abs.f32 	%f51, %f1;
	cvt.rn.f32.s32	%f3, %r6;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB71_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB71_3;
	bra.uni 	BB71_2;

BB71_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB71_21;

BB71_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB71_5;

	mov.u32 	%r25, 0;
	bra.uni 	BB71_16;

BB71_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r7, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r8, %f24;
	sub.s32 	%r1, %r7, %r8;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r7, %r8;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB71_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB71_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB71_9;

	shr.s32 	%r9, %r1, 31;
	shr.u32 	%r10, %r9, 30;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 2;
	cvt.rn.f32.s32	%f26, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r13, %r12, -3, %r1;
	cvt.rn.f32.s32	%f28, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB71_12;

BB71_9:
	shr.u32 	%r14, %r1, 31;
	add.s32 	%r15, %r1, %r14;
	shr.s32 	%r16, %r15, 1;
	cvt.rn.f32.s32	%f33, %r16;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r17, %r1, %r16;
	cvt.rn.f32.s32	%f35, %r17;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB71_12;

BB71_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB71_12;

BB71_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB71_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB71_13;
	bra.uni 	BB71_22;

BB71_13:
	mov.f32 	%f52, %f51;

BB71_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB71_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r25, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB71_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB71_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r25, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB71_19;
	bra.uni 	BB71_18;

BB71_18:
	sub.f32 	%f50, %f50, %f4;

BB71_19:
	mov.b32 	 %r20, %f1;
	and.b32  	%r21, %r20, -2147483648;
	mov.b32 	 %r22, %f50;
	xor.b32  	%r23, %r22, %r21;
	mov.b32 	 %f53, %r23;
	bra.uni 	BB71_21;

BB71_20:
	add.f32 	%f53, %f1, %f3;

BB71_21:
	cvt.rzi.s32.f32	%r24, %f53;
	st.param.b32	[funj_retval0+0], %r24;
	ret;

BB71_22:
	mov.u32 	%r25, 0;
	bra.uni 	BB71_16;
}

.visible .func  (.param .b32 funj_retval0) ___remuu(
	.param .b32 ___remuu_param_0,
	.param .b32 ___remuu_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<26>;
	.reg .f32 	%f<54>;


	ld.param.u32 	%r5, [___remuu_param_0];
	ld.param.u32 	%r6, [___remuu_param_1];
	cvt.rn.f32.u32	%f1, %r5;
	abs.f32 	%f51, %f1;
	cvt.rn.f32.u32	%f3, %r6;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB72_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB72_3;
	bra.uni 	BB72_2;

BB72_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB72_21;

BB72_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB72_5;

	mov.u32 	%r25, 0;
	bra.uni 	BB72_16;

BB72_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r7, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r8, %f24;
	sub.s32 	%r1, %r7, %r8;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r7, %r8;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB72_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB72_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB72_9;

	shr.s32 	%r9, %r1, 31;
	shr.u32 	%r10, %r9, 30;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 2;
	cvt.rn.f32.s32	%f26, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r13, %r12, -3, %r1;
	cvt.rn.f32.s32	%f28, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB72_12;

BB72_9:
	shr.u32 	%r14, %r1, 31;
	add.s32 	%r15, %r1, %r14;
	shr.s32 	%r16, %r15, 1;
	cvt.rn.f32.s32	%f33, %r16;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r17, %r1, %r16;
	cvt.rn.f32.s32	%f35, %r17;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB72_12;

BB72_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB72_12;

BB72_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB72_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB72_13;
	bra.uni 	BB72_22;

BB72_13:
	mov.f32 	%f52, %f51;

BB72_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB72_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r25, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB72_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB72_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r25, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB72_19;
	bra.uni 	BB72_18;

BB72_18:
	sub.f32 	%f50, %f50, %f4;

BB72_19:
	mov.b32 	 %r20, %f1;
	and.b32  	%r21, %r20, -2147483648;
	mov.b32 	 %r22, %f50;
	xor.b32  	%r23, %r22, %r21;
	mov.b32 	 %f53, %r23;
	bra.uni 	BB72_21;

BB72_20:
	add.f32 	%f53, %f1, %f3;

BB72_21:
	cvt.rzi.u32.f32	%r24, %f53;
	st.param.b32	[funj_retval0+0], %r24;
	ret;

BB72_22:
	mov.u32 	%r25, 0;
	bra.uni 	BB72_16;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIjET_S0_S0_(
	.param .b32 _Z3remIjET_S0_S0__param_0,
	.param .b32 _Z3remIjET_S0_S0__param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<26>;
	.reg .f32 	%f<54>;


	ld.param.u32 	%r5, [_Z3remIjET_S0_S0__param_0];
	ld.param.u32 	%r6, [_Z3remIjET_S0_S0__param_1];
	cvt.rn.f32.u32	%f1, %r5;
	abs.f32 	%f51, %f1;
	cvt.rn.f32.u32	%f3, %r6;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB73_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB73_3;
	bra.uni 	BB73_2;

BB73_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB73_21;

BB73_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB73_5;

	mov.u32 	%r25, 0;
	bra.uni 	BB73_16;

BB73_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r7, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r8, %f24;
	sub.s32 	%r1, %r7, %r8;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r7, %r8;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB73_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB73_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB73_9;

	shr.s32 	%r9, %r1, 31;
	shr.u32 	%r10, %r9, 30;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 2;
	cvt.rn.f32.s32	%f26, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r13, %r12, -3, %r1;
	cvt.rn.f32.s32	%f28, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB73_12;

BB73_9:
	shr.u32 	%r14, %r1, 31;
	add.s32 	%r15, %r1, %r14;
	shr.s32 	%r16, %r15, 1;
	cvt.rn.f32.s32	%f33, %r16;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r17, %r1, %r16;
	cvt.rn.f32.s32	%f35, %r17;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB73_12;

BB73_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB73_12;

BB73_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB73_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB73_13;
	bra.uni 	BB73_22;

BB73_13:
	mov.f32 	%f52, %f51;

BB73_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB73_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r25, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB73_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB73_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r25, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB73_19;
	bra.uni 	BB73_18;

BB73_18:
	sub.f32 	%f50, %f50, %f4;

BB73_19:
	mov.b32 	 %r20, %f1;
	and.b32  	%r21, %r20, -2147483648;
	mov.b32 	 %r22, %f50;
	xor.b32  	%r23, %r22, %r21;
	mov.b32 	 %f53, %r23;
	bra.uni 	BB73_21;

BB73_20:
	add.f32 	%f53, %f1, %f3;

BB73_21:
	cvt.rzi.u32.f32	%r24, %f53;
	st.param.b32	[funj_retval0+0], %r24;
	ret;

BB73_22:
	mov.u32 	%r25, 0;
	bra.uni 	BB73_16;
}

.visible .func  (.param .b32 funj_retval0) ___remjj(
	.param .b32 ___remjj_param_0,
	.param .b32 ___remjj_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<25>;
	.reg .f32 	%f<54>;


	ld.param.s8 	%rs1, [___remjj_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f51, %f1;
	ld.param.s8 	%rs2, [___remjj_param_1];
	cvt.rn.f32.s16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB74_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB74_3;
	bra.uni 	BB74_2;

BB74_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB74_21;

BB74_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB74_5;

	mov.u32 	%r24, 0;
	bra.uni 	BB74_16;

BB74_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB74_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB74_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB74_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB74_12;

BB74_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB74_12;

BB74_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB74_12;

BB74_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB74_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB74_13;
	bra.uni 	BB74_22;

BB74_13:
	mov.f32 	%f52, %f51;

BB74_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB74_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r24, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB74_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB74_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r24, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB74_19;
	bra.uni 	BB74_18;

BB74_18:
	sub.f32 	%f50, %f50, %f4;

BB74_19:
	mov.b32 	 %r18, %f1;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB74_21;

BB74_20:
	add.f32 	%f53, %f1, %f3;

BB74_21:
	cvt.rzi.s32.f32	%r22, %f53;
	cvt.s32.s8 	%r23, %r22;
	st.param.b32	[funj_retval0+0], %r23;
	ret;

BB74_22:
	mov.u32 	%r24, 0;
	bra.uni 	BB74_16;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIcET_S0_S0_(
	.param .b32 _Z3remIcET_S0_S0__param_0,
	.param .b32 _Z3remIcET_S0_S0__param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<25>;
	.reg .f32 	%f<54>;


	ld.param.s8 	%rs1, [_Z3remIcET_S0_S0__param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	abs.f32 	%f51, %f1;
	ld.param.s8 	%rs2, [_Z3remIcET_S0_S0__param_1];
	cvt.rn.f32.s16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB75_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB75_3;
	bra.uni 	BB75_2;

BB75_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB75_21;

BB75_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB75_5;

	mov.u32 	%r24, 0;
	bra.uni 	BB75_16;

BB75_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB75_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB75_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB75_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB75_12;

BB75_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB75_12;

BB75_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB75_12;

BB75_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB75_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB75_13;
	bra.uni 	BB75_22;

BB75_13:
	mov.f32 	%f52, %f51;

BB75_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB75_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r24, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB75_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB75_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r24, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB75_19;
	bra.uni 	BB75_18;

BB75_18:
	sub.f32 	%f50, %f50, %f4;

BB75_19:
	mov.b32 	 %r18, %f1;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB75_21;

BB75_20:
	add.f32 	%f53, %f1, %f3;

BB75_21:
	cvt.rzi.s32.f32	%r22, %f53;
	cvt.s32.s8 	%r23, %r22;
	st.param.b32	[funj_retval0+0], %r23;
	ret;

BB75_22:
	mov.u32 	%r24, 0;
	bra.uni 	BB75_16;
}

.visible .func  (.param .b32 funj_retval0) ___remvv(
	.param .b32 ___remvv_param_0,
	.param .b32 ___remvv_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<25>;
	.reg .f32 	%f<54>;


	ld.param.u8 	%rs1, [___remvv_param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f51, %f1;
	ld.param.u8 	%rs2, [___remvv_param_1];
	cvt.rn.f32.u16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB76_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB76_3;
	bra.uni 	BB76_2;

BB76_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB76_21;

BB76_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB76_5;

	mov.u32 	%r24, 0;
	bra.uni 	BB76_16;

BB76_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB76_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB76_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB76_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB76_12;

BB76_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB76_12;

BB76_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB76_12;

BB76_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB76_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB76_13;
	bra.uni 	BB76_22;

BB76_13:
	mov.f32 	%f52, %f51;

BB76_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB76_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r24, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB76_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB76_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r24, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB76_19;
	bra.uni 	BB76_18;

BB76_18:
	sub.f32 	%f50, %f50, %f4;

BB76_19:
	mov.b32 	 %r18, %f1;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB76_21;

BB76_20:
	add.f32 	%f53, %f1, %f3;

BB76_21:
	cvt.rzi.u32.f32	%r22, %f53;
	and.b32  	%r23, %r22, 255;
	st.param.b32	[funj_retval0+0], %r23;
	ret;

BB76_22:
	mov.u32 	%r24, 0;
	bra.uni 	BB76_16;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIhET_S0_S0_(
	.param .b32 _Z3remIhET_S0_S0__param_0,
	.param .b32 _Z3remIhET_S0_S0__param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<25>;
	.reg .f32 	%f<54>;


	ld.param.u8 	%rs1, [_Z3remIhET_S0_S0__param_0];
	cvt.rn.f32.u16	%f1, %rs1;
	abs.f32 	%f51, %f1;
	ld.param.u8 	%rs2, [_Z3remIhET_S0_S0__param_1];
	cvt.rn.f32.u16	%f3, %rs2;
	abs.f32 	%f4, %f3;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	setp.gtu.f32	%p2, %f4, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB77_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f4, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB77_3;
	bra.uni 	BB77_2;

BB77_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB77_21;

BB77_3:
	setp.ge.f32	%p7, %f51, %f4;
	@%p7 bra 	BB77_5;

	mov.u32 	%r24, 0;
	bra.uni 	BB77_16;

BB77_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f4;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f5, %f4;
	setp.eq.f32	%p8, %f5, 0f00000000;
	setp.eq.f32	%p9, %f5, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB77_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB77_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB77_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f4, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB77_12;

BB77_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f4, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB77_12;

BB77_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f4, %f37;
	bra.uni 	BB77_12;

BB77_11:
	setp.leu.f32	%p15, %f5, 0f00000000;
	add.f32 	%f39, %f4, %f4;
	selp.f32	%f44, %f39, %f4, %p15;

BB77_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f4;
	@%p17 bra 	BB77_13;
	bra.uni 	BB77_22;

BB77_13:
	mov.f32 	%f52, %f51;

BB77_14:
	mov.f32 	%f12, %f52;
	mov.f32 	%f13, %f45;
	sub.f32 	%f42, %f12, %f13;
	setp.ltu.f32	%p18, %f12, %f13;
	selp.f32	%f52, %f12, %f42, %p18;
	mul.f32 	%f45, %f13, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f4;
	@%p19 bra 	BB77_14;

	setp.ge.f32	%p20, %f12, %f13;
	selp.u32	%r24, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB77_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f17, %f50, %f50;
	setp.gt.f32	%p21, %f17, %f4;
	@%p21 bra 	BB77_18;

	setp.eq.f32	%p22, %f17, %f4;
	setp.ne.s32	%p23, %r24, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB77_19;
	bra.uni 	BB77_18;

BB77_18:
	sub.f32 	%f50, %f50, %f4;

BB77_19:
	mov.b32 	 %r18, %f1;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB77_21;

BB77_20:
	add.f32 	%f53, %f1, %f3;

BB77_21:
	cvt.rzi.u32.f32	%r22, %f53;
	and.b32  	%r23, %r22, 255;
	st.param.b32	[funj_retval0+0], %r23;
	ret;

BB77_22:
	mov.u32 	%r24, 0;
	bra.uni 	BB77_16;
}

.visible .func  (.param .b64 funj_retval0) ___remdd(
	.param .b64 ___remdd_param_0,
	.param .b64 ___remdd_param_1
)
{
	.reg .pred 	%p<24>;
	.reg .s32 	%r<35>;
	.reg .f64 	%fd<39>;


	ld.param.f64 	%fd22, [___remdd_param_0];
	ld.param.f64 	%fd23, [___remdd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd22;
	}
	and.b32  	%r12, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd23;
	}
	and.b32  	%r31, %r13, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd22;
	}
	mov.b64 	%fd37, {%r14, %r12};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd23;
	}
	mov.b64 	%fd2, {%r32, %r31};
	setp.gt.u32	%p1, %r12, 2146435071;
	setp.gt.u32	%p2, %r31, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB78_22;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB78_3;

	mov.f64 	%fd38, 0dFFF8000000000000;
	bra.uni 	BB78_25;

BB78_3:
	setp.ge.f64	%p5, %fd37, %fd2;
	@%p5 bra 	BB78_5;

	mov.u32 	%r34, 0;
	bra.uni 	BB78_18;

BB78_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.s32	%p6, %r15, 1048576;
	@%p6 bra 	BB78_7;

	mov.f64 	%fd33, 0d0000000000000000;
	bra.uni 	BB78_11;

BB78_7:
	setp.geu.f64	%p7, %fd2, %fd37;
	mov.f64 	%fd34, %fd2;
	@%p7 bra 	BB78_10;

	mov.f64 	%fd35, %fd2;

BB78_9:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p8, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd35;
	}
	setp.lt.s32	%p9, %r16, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p10 bra 	BB78_9;

BB78_10:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd33;
	}

BB78_11:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd37;
	}
	setp.lt.s32	%p11, %r17, 1048576;
	@%p11 bra 	BB78_13;

	and.b32  	%r18, %r31, 1048575;
	and.b32  	%r19, %r1, 2146435072;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd32, {%r32, %r20};

BB78_13:
	mul.f64 	%fd25, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd32, %fd37;
	selp.f64	%fd36, %fd25, %fd32, %p12;
	setp.ge.f64	%p13, %fd36, %fd2;
	@%p13 bra 	BB78_15;

	mov.u32 	%r34, 0;
	bra.uni 	BB78_18;

BB78_15:
	mov.u32 	%r33, -1;

BB78_16:
	setp.ltu.f64	%p14, %fd37, %fd36;
	selp.u32	%r22, 1, 0, %p14;
	shl.b32 	%r23, %r33, 1;
	add.s32 	%r33, %r22, %r23;
	sub.f64 	%fd26, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd26, %p14;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd36, %fd2;
	@%p15 bra 	BB78_16;

	not.b32 	%r24, %r33;
	and.b32  	%r34, %r24, 1;

BB78_18:
	add.f64 	%fd15, %fd37, %fd37;
	setp.gt.f64	%p16, %fd15, %fd2;
	@%p16 bra 	BB78_20;

	setp.eq.f64	%p17, %fd15, %fd2;
	setp.ne.s32	%p18, %r34, 0;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB78_21;
	bra.uni 	BB78_20;

BB78_20:
	sub.f64 	%fd37, %fd37, %fd2;

BB78_21:
	and.b32  	%r27, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd37;
	}
	xor.b32  	%r29, %r28, %r27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd37;
	}
	mov.b64 	%fd38, {%r30, %r29};
	bra.uni 	BB78_25;

BB78_22:
	setp.gtu.f64	%p20, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p21, %fd2, 0d7FF0000000000000;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	BB78_24;

	setp.eq.f64	%p23, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd22, %p23;
	bra.uni 	BB78_25;

BB78_24:
	add.f64 	%fd38, %fd22, %fd23;

BB78_25:
	st.param.f64	[funj_retval0+0], %fd38;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___hypotss(
	.param .b32 ___hypotss_param_0,
	.param .b32 ___hypotss_param_1
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<21>;


	ld.param.f32 	%f1, [___hypotss_param_0];
	ld.param.f32 	%f2, [___hypotss_param_1];
	abs.f32 	%f3, %f1;
	abs.f32 	%f4, %f2;
	max.f32 	%f5, %f3, %f4;
	min.f32 	%f6, %f3, %f4;
	setp.gt.f32	%p1, %f5, 0f7E800000;
	mul.f32 	%f7, %f5, 0f3E800000;
	mul.f32 	%f8, %f6, 0f3E800000;
	selp.f32	%f9, %f7, %f5, %p1;
	selp.f32	%f10, %f8, %f6, %p1;
	div.approx.f32 	%f11, %f10, %f9;
	mov.f32 	%f12, 0f3F800000;
	fma.rn.f32 	%f13, %f11, %f11, %f12;
	sqrt.rn.f32 	%f14, %f13;
	mul.f32 	%f15, %f5, %f14;
	setp.eq.f32	%p2, %f5, 0f00000000;
	add.f32 	%f16, %f5, %f6;
	selp.f32	%f17, %f16, %f15, %p2;
	setp.gtu.f32	%p3, %f3, 0f7F800000;
	setp.gtu.f32	%p4, %f4, 0f7F800000;
	or.pred  	%p5, %p3, %p4;
	add.f32 	%f18, %f1, %f2;
	selp.f32	%f19, %f18, %f17, %p5;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	selp.f32	%f20, %f16, %f19, %p6;
	st.param.f32	[funj_retval0+0], %f20;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___hypotdd(
	.param .b64 ___hypotdd_param_0,
	.param .b64 ___hypotdd_param_1
)
{
	.reg .pred 	%p<6>;
	.reg .s32 	%r<34>;
	.reg .f64 	%fd<40>;


	ld.param.f64 	%fd14, [___hypotdd_param_0];
	ld.param.f64 	%fd15, [___hypotdd_param_1];
	abs.f64 	%fd1, %fd14;
	abs.f64 	%fd2, %fd15;
	max.f64 	%fd3, %fd1, %fd2;
	min.f64 	%fd37, %fd1, %fd2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd3;
	}
	shr.u32 	%r2, %r1, 20;
	add.s32 	%r3, %r2, -1023;
	mov.u32 	%r12, 1023;
	sub.s32 	%r4, %r12, %r2;
	abs.s32 	%r13, %r4;
	setp.lt.s32	%p1, %r13, 1023;
	shl.b32 	%r5, %r4, 20;
	@%p1 bra 	BB80_2;

	add.s32 	%r14, %r4, 2046;
	shl.b32 	%r15, %r14, 19;
	and.b32  	%r16, %r15, -1048576;
	shl.b32 	%r17, %r14, 20;
	sub.s32 	%r32, %r17, %r16;
	mov.u32 	%r18, 0;
	mov.b64 	%fd16, {%r18, %r16};
	mul.f64 	%fd37, %fd37, %fd16;
	bra.uni 	BB80_3;

BB80_2:
	add.s32 	%r32, %r5, 1072693248;

BB80_3:
	mov.u32 	%r19, 0;
	mov.b64 	%fd17, {%r19, %r32};
	mul.f64 	%fd7, %fd37, %fd17;
	setp.lt.s32	%p2, %r3, 0;
	@%p2 bra 	BB80_5;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd3;
	}
	add.s32 	%r21, %r5, %r1;
	mov.b64 	%fd38, {%r20, %r21};
	bra.uni 	BB80_6;

BB80_5:
	add.s32 	%r22, %r5, 1072693248;
	mov.b64 	%fd18, {%r19, %r22};
	mul.f64 	%fd38, %fd3, %fd18;

BB80_6:
	mul.f64 	%fd21, %fd7, %fd7;
	fma.rn.f64 	%fd20, %fd38, %fd38, %fd21;
	// inline asm
	rsqrt.approx.ftz.f64 %fd19, %fd20;
	// inline asm
	mul.rn.f64 	%fd22, %fd19, %fd19;
	neg.f64 	%fd23, %fd22;
	mov.f64 	%fd24, 0d3FF0000000000000;
	fma.rn.f64 	%fd25, %fd20, %fd23, %fd24;
	mov.f64 	%fd26, 0d3FE0000000000000;
	mov.f64 	%fd27, 0d3FD8000000000000;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mul.rn.f64 	%fd29, %fd25, %fd19;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd19;
	mul.f64 	%fd39, %fd20, %fd30;
	abs.s32 	%r24, %r3;
	setp.lt.s32	%p3, %r24, 1023;
	@%p3 bra 	BB80_8;

	add.s32 	%r25, %r2, 1023;
	shl.b32 	%r26, %r25, 19;
	and.b32  	%r27, %r26, -1048576;
	shl.b32 	%r28, %r25, 20;
	sub.s32 	%r33, %r28, %r27;
	mov.b64 	%fd31, {%r19, %r27};
	mul.f64 	%fd39, %fd39, %fd31;
	bra.uni 	BB80_9;

BB80_8:
	shl.b32 	%r30, %r3, 20;
	add.s32 	%r33, %r30, 1072693248;

BB80_9:
	mov.b64 	%fd32, {%r19, %r33};
	mul.f64 	%fd33, %fd39, %fd32;
	add.f64 	%fd34, %fd1, %fd2;
	setp.leu.f64	%p4, %fd34, %fd3;
	selp.f64	%fd35, %fd34, %fd33, %p4;
	setp.eq.f64	%p5, %fd3, 0d7FF0000000000000;
	selp.f64	%fd36, %fd3, %fd35, %p5;
	st.param.f64	[funj_retval0+0], %fd36;
	ret;
}

.visible .func  (.param .align 8 .b8 funj_retval0[8]) ___mincc(
	.param .align 8 .b8 ___mincc_param_0[8],
	.param .align 8 .b8 ___mincc_param_1[8]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;


	ld.param.f32 	%f1, [___mincc_param_0];
	ld.param.f32 	%f2, [___mincc_param_0+4];
	ld.param.f32 	%f3, [___mincc_param_1];
	ld.param.f32 	%f4, [___mincc_param_1+4];
	mul.f32 	%f5, %f2, %f2;
	fma.rn.f32 	%f6, %f1, %f1, %f5;
	mul.f32 	%f7, %f4, %f4;
	fma.rn.f32 	%f8, %f3, %f3, %f7;
	setp.lt.f32	%p1, %f6, %f8;
	selp.f32	%f9, %f2, %f4, %p1;
	selp.f32	%f10, %f1, %f3, %p1;
	st.param.f32	[funj_retval0+0], %f10;
	st.param.f32	[funj_retval0+4], %f9;
	ret;
}

.visible .func  (.param .align 16 .b8 funj_retval0[16]) ___minzz(
	.param .align 16 .b8 ___minzz_param_0[16],
	.param .align 16 .b8 ___minzz_param_1[16]
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<11>;


	ld.param.f64 	%fd1, [___minzz_param_0];
	ld.param.f64 	%fd2, [___minzz_param_0+8];
	ld.param.f64 	%fd3, [___minzz_param_1];
	ld.param.f64 	%fd4, [___minzz_param_1+8];
	mul.f64 	%fd5, %fd2, %fd2;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd5;
	mul.f64 	%fd7, %fd4, %fd4;
	fma.rn.f64 	%fd8, %fd3, %fd3, %fd7;
	setp.lt.f64	%p1, %fd6, %fd8;
	selp.f64	%fd9, %fd2, %fd4, %p1;
	selp.f64	%fd10, %fd1, %fd3, %p1;
	st.param.f64	[funj_retval0+0], %fd10;
	st.param.f64	[funj_retval0+8], %fd9;
	ret;
}

.visible .func  (.param .align 8 .b8 funj_retval0[8]) ___maxcc(
	.param .align 8 .b8 ___maxcc_param_0[8],
	.param .align 8 .b8 ___maxcc_param_1[8]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;


	ld.param.f32 	%f1, [___maxcc_param_0];
	ld.param.f32 	%f2, [___maxcc_param_0+4];
	ld.param.f32 	%f3, [___maxcc_param_1];
	ld.param.f32 	%f4, [___maxcc_param_1+4];
	mul.f32 	%f5, %f2, %f2;
	fma.rn.f32 	%f6, %f1, %f1, %f5;
	mul.f32 	%f7, %f4, %f4;
	fma.rn.f32 	%f8, %f3, %f3, %f7;
	setp.gt.f32	%p1, %f6, %f8;
	selp.f32	%f9, %f2, %f4, %p1;
	selp.f32	%f10, %f1, %f3, %p1;
	st.param.f32	[funj_retval0+0], %f10;
	st.param.f32	[funj_retval0+4], %f9;
	ret;
}

.visible .func  (.param .align 16 .b8 funj_retval0[16]) ___maxzz(
	.param .align 16 .b8 ___maxzz_param_0[16],
	.param .align 16 .b8 ___maxzz_param_1[16]
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<11>;


	ld.param.f64 	%fd1, [___maxzz_param_0];
	ld.param.f64 	%fd2, [___maxzz_param_0+8];
	ld.param.f64 	%fd3, [___maxzz_param_1];
	ld.param.f64 	%fd4, [___maxzz_param_1+8];
	mul.f64 	%fd5, %fd2, %fd2;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd5;
	mul.f64 	%fd7, %fd4, %fd4;
	fma.rn.f64 	%fd8, %fd3, %fd3, %fd7;
	setp.gt.f64	%p1, %fd6, %fd8;
	selp.f64	%fd9, %fd2, %fd4, %p1;
	selp.f64	%fd10, %fd1, %fd3, %p1;
	st.param.f64	[funj_retval0+0], %fd10;
	st.param.f64	[funj_retval0+8], %fd9;
	ret;
}

.func  (.param .b64 funj_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .s32 	%r<48>;
	.reg .f32 	%f<3>;
	.reg .f64 	%fd<141>;


	ld.param.f64 	%fd14, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd15, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd14;
	}
	shr.u32 	%r45, %r44, 20;
	setp.ne.s32	%p1, %r45, 0;
	@%p1 bra 	BB85_2;

	mul.f64 	%fd16, %fd14, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd16;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd16;
	}
	shr.u32 	%r18, %r44, 20;
	add.s32 	%r45, %r18, -54;

BB85_2:
	add.s32 	%r46, %r45, -1023;
	and.b32  	%r19, %r44, -2146435073;
	or.b32  	%r20, %r19, 1072693248;
	mov.b64 	%fd138, {%r43, %r20};
	setp.lt.u32	%p2, %r20, 1073127583;
	@%p2 bra 	BB85_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd138;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd138;
	}
	add.s32 	%r23, %r22, -1048576;
	mov.b64 	%fd138, {%r21, %r23};
	add.s32 	%r46, %r45, -1022;

BB85_4:
	add.f64 	%fd18, %fd138, 0d3FF0000000000000;
	mov.f64 	%fd19, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd17,%fd18;
	// inline asm
	neg.f64 	%fd20, %fd18;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd19;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd17, %fd17;
	add.f64 	%fd24, %fd138, 0dBFF0000000000000;
	mul.f64 	%fd25, %fd24, %fd23;
	fma.rn.f64 	%fd26, %fd24, %fd23, %fd25;
	mul.f64 	%fd27, %fd26, %fd26;
	mov.f64 	%fd28, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd29, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3F6249249242B910;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3F89999999999DFB;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	sub.f64 	%fd41, %fd24, %fd26;
	add.f64 	%fd42, %fd41, %fd41;
	neg.f64 	%fd43, %fd26;
	fma.rn.f64 	%fd44, %fd43, %fd24, %fd42;
	mul.f64 	%fd45, %fd23, %fd44;
	fma.rn.f64 	%fd46, %fd40, %fd27, 0d3FB5555555555555;
	mov.f64 	%fd47, 0d3FB5555555555555;
	sub.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd40, %fd27, %fd48;
	add.f64 	%fd50, %fd49, 0d0000000000000000;
	add.f64 	%fd51, %fd50, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd52, %fd46, %fd51;
	sub.f64 	%fd53, %fd46, %fd52;
	add.f64 	%fd54, %fd53, %fd51;
	mul.rn.f64 	%fd55, %fd26, %fd26;
	neg.f64 	%fd56, %fd55;
	fma.rn.f64 	%fd57, %fd26, %fd26, %fd56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd45;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd45;
	}
	add.s32 	%r26, %r25, 1048576;
	mov.b64 	%fd58, {%r24, %r26};
	fma.rn.f64 	%fd59, %fd26, %fd58, %fd57;
	mul.rn.f64 	%fd60, %fd55, %fd26;
	neg.f64 	%fd61, %fd60;
	fma.rn.f64 	%fd62, %fd55, %fd26, %fd61;
	fma.rn.f64 	%fd63, %fd55, %fd45, %fd62;
	fma.rn.f64 	%fd64, %fd59, %fd26, %fd63;
	mul.rn.f64 	%fd65, %fd52, %fd60;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd60, %fd66;
	fma.rn.f64 	%fd68, %fd52, %fd64, %fd67;
	fma.rn.f64 	%fd69, %fd54, %fd60, %fd68;
	add.f64 	%fd70, %fd65, %fd69;
	sub.f64 	%fd71, %fd65, %fd70;
	add.f64 	%fd72, %fd71, %fd69;
	add.f64 	%fd73, %fd26, %fd70;
	sub.f64 	%fd74, %fd26, %fd73;
	add.f64 	%fd75, %fd74, %fd70;
	add.f64 	%fd76, %fd75, %fd72;
	add.f64 	%fd77, %fd76, %fd45;
	add.f64 	%fd78, %fd73, %fd77;
	sub.f64 	%fd79, %fd73, %fd78;
	add.f64 	%fd80, %fd79, %fd77;
	xor.b32  	%r27, %r46, -2147483648;
	mov.u32 	%r28, -2147483648;
	mov.u32 	%r29, 1127219200;
	mov.b64 	%fd81, {%r27, %r29};
	mov.b64 	%fd82, {%r28, %r29};
	sub.f64 	%fd83, %fd81, %fd82;
	mov.f64 	%fd84, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd85, %fd83, %fd84, %fd78;
	neg.f64 	%fd86, %fd83;
	fma.rn.f64 	%fd87, %fd86, %fd84, %fd85;
	sub.f64 	%fd88, %fd87, %fd78;
	sub.f64 	%fd89, %fd80, %fd88;
	mov.f64 	%fd90, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd91, %fd83, %fd90, %fd89;
	add.f64 	%fd92, %fd85, %fd91;
	sub.f64 	%fd93, %fd85, %fd92;
	add.f64 	%fd94, %fd93, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd15;
	}
	add.s32 	%r31, %r30, %r30;
	and.b32  	%r32, %r30, -15728641;
	setp.gt.u32	%p3, %r31, -33554433;
	selp.b32	%r33, %r32, %r30, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd15;
	}
	mov.b64 	%fd95, {%r34, %r33};
	mul.rn.f64 	%fd96, %fd92, %fd95;
	neg.f64 	%fd97, %fd96;
	fma.rn.f64 	%fd98, %fd92, %fd95, %fd97;
	fma.rn.f64 	%fd99, %fd94, %fd95, %fd98;
	add.f64 	%fd4, %fd96, %fd99;
	sub.f64 	%fd100, %fd96, %fd4;
	add.f64 	%fd5, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd4;
	}
	mov.b32 	 %f1, %r13;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p4, %f2, 0f40874911;
	@%p4 bra 	BB85_6;

	setp.lt.s32	%p5, %r13, 0;
	selp.f64	%fd101, 0d0000000000000000, 0d7FF0000000000000, %p5;
	abs.f64 	%fd102, %fd4;
	setp.gtu.f64	%p6, %fd102, 0d7FF0000000000000;
	add.f64 	%fd103, %fd4, %fd4;
	selp.f64	%fd140, %fd103, %fd101, %p6;
	bra.uni 	BB85_10;

BB85_6:
	mov.f64 	%fd104, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd105, %fd4, %fd104;
	mov.f64 	%fd106, 0d4338000000000000;
	add.rn.f64 	%fd107, %fd105, %fd106;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd107;
	}
	mov.f64 	%fd108, 0dC338000000000000;
	add.rn.f64 	%fd109, %fd107, %fd108;
	mov.f64 	%fd110, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd111, %fd109, %fd110, %fd4;
	mov.f64 	%fd112, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd113, %fd109, %fd112, %fd111;
	mov.f64 	%fd114, 0d3E928AF3FCA213EA;
	mov.f64 	%fd115, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd116, %fd115, %fd113, %fd114;
	mov.f64 	%fd117, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd118, %fd116, %fd113, %fd117;
	mov.f64 	%fd119, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd120, %fd118, %fd113, %fd119;
	mov.f64 	%fd121, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd122, %fd120, %fd113, %fd121;
	mov.f64 	%fd123, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd124, %fd122, %fd113, %fd123;
	mov.f64 	%fd125, 0d3F81111111122322;
	fma.rn.f64 	%fd126, %fd124, %fd113, %fd125;
	mov.f64 	%fd127, 0d3FA55555555502A1;
	fma.rn.f64 	%fd128, %fd126, %fd113, %fd127;
	mov.f64 	%fd129, 0d3FC5555555555511;
	fma.rn.f64 	%fd130, %fd128, %fd113, %fd129;
	mov.f64 	%fd131, 0d3FE000000000000B;
	fma.rn.f64 	%fd132, %fd130, %fd113, %fd131;
	fma.rn.f64 	%fd134, %fd132, %fd113, %fd19;
	fma.rn.f64 	%fd139, %fd134, %fd113, %fd19;
	abs.s32 	%r35, %r14;
	setp.lt.s32	%p7, %r35, 1023;
	@%p7 bra 	BB85_8;

	add.s32 	%r36, %r14, 2046;
	shl.b32 	%r37, %r36, 19;
	and.b32  	%r38, %r37, -1048576;
	shl.b32 	%r39, %r36, 20;
	sub.s32 	%r47, %r39, %r38;
	mov.u32 	%r40, 0;
	mov.b64 	%fd135, {%r40, %r38};
	mul.f64 	%fd139, %fd139, %fd135;
	bra.uni 	BB85_9;

BB85_8:
	shl.b32 	%r41, %r14, 20;
	add.s32 	%r47, %r41, 1072693248;

BB85_9:
	mov.u32 	%r42, 0;
	mov.b64 	%fd136, {%r42, %r47};
	mul.f64 	%fd140, %fd139, %fd136;

BB85_10:
	abs.f64 	%fd137, %fd140;
	setp.eq.f64	%p8, %fd137, 0d7FF0000000000000;
	@%p8 bra 	BB85_12;

	fma.rn.f64 	%fd140, %fd140, %fd5, %fd140;

BB85_12:
	st.param.f64	[funj_retval0+0], %fd140;
	ret;
}

.func  (.param .b64 funj_retval0) __internal_lgamma_pos(
	.param .b64 __internal_lgamma_pos_param_0
)
{
	.reg .pred 	%p<22>;
	.reg .s32 	%r<54>;
	.reg .f64 	%fd<298>;


	ld.param.f64 	%fd24, [__internal_lgamma_pos_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd24;
	}
	setp.gt.s32	%p1, %r50, 1074266111;
	@%p1 bra 	BB86_17;

	setp.gt.s32	%p2, %r50, 1073217535;
	@%p2 bra 	BB86_16;

	setp.gt.s32	%p3, %r50, 1072064101;
	@%p3 bra 	BB86_15;

	mov.f64 	%fd25, 0d3EA7B77CEB0625E8;
	mov.f64 	%fd26, 0dBE7844988BFE6590;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0dBE998C69C8710CC4;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0dBEF6527A5A11CF6E;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F20EC2950B1B5DE;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0dBF2C4D80C24BA278;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0dBF5315B4E8CC0D09;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3F7D917F15D50020;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0dBF83B4ABB41CB6FA;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0dBFA59AF1275B7120;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3FC5512321A168A0;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	mov.f64 	%fd46, 0dBFA5815E8FDCE74C;
	fma.rn.f64 	%fd47, %fd45, %fd24, %fd46;
	mov.f64 	%fd48, 0dBFE4FCF4026ADD1A;
	fma.rn.f64 	%fd49, %fd47, %fd24, %fd48;
	mov.f64 	%fd50, 0d3FE2788CFC6FB5C8;
	fma.rn.f64 	%fd51, %fd49, %fd24, %fd50;
	mul.f64 	%fd52, %fd51, %fd24;
	fma.rn.f64 	%fd1, %fd52, %fd24, %fd24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd1;
	}
	setp.gt.f64	%p4, %fd1, 0d0000000000000000;
	setp.lt.s32	%p5, %r46, 2146435072;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB86_9;

	abs.f64 	%fd53, %fd1;
	setp.gtu.f64	%p7, %fd53, 0d7FF0000000000000;
	@%p7 bra 	BB86_8;

	setp.neu.f64	%p8, %fd1, 0d0000000000000000;
	@%p8 bra 	BB86_7;

	mov.f64 	%fd54, 0dFFF0000000000000;
	neg.f64 	%fd297, %fd54;
	bra.uni 	BB86_32;

BB86_7:
	setp.eq.f64	%p9, %fd1, 0d7FF0000000000000;
	selp.f64	%fd2, %fd1, 0dFFF8000000000000, %p9;
	neg.f64 	%fd297, %fd2;
	bra.uni 	BB86_32;

BB86_8:
	add.f64 	%fd3, %fd1, %fd1;
	neg.f64 	%fd297, %fd3;
	bra.uni 	BB86_32;

BB86_9:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd1;
	}
	setp.lt.s32	%p10, %r46, 1048576;
	@%p10 bra 	BB86_11;

	mov.u32 	%r48, -1023;
	bra.uni 	BB86_12;

BB86_11:
	mul.f64 	%fd55, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd55;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd55;
	}
	mov.u32 	%r48, -1077;

BB86_12:
	shr.u32 	%r23, %r46, 20;
	add.s32 	%r49, %r48, %r23;
	and.b32  	%r24, %r46, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd294, {%r47, %r25};
	setp.lt.s32	%p11, %r25, 1073127583;
	@%p11 bra 	BB86_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd294;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd294;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd294, {%r26, %r28};
	add.s32 	%r49, %r49, 1;

BB86_14:
	add.f64 	%fd57, %fd294, 0d3FF0000000000000;
	mov.f64 	%fd58, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd56,%fd57;
	// inline asm
	neg.f64 	%fd59, %fd57;
	fma.rn.f64 	%fd60, %fd59, %fd56, %fd58;
	fma.rn.f64 	%fd61, %fd60, %fd60, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd56, %fd56;
	add.f64 	%fd63, %fd294, 0dBFF0000000000000;
	mul.f64 	%fd64, %fd63, %fd62;
	fma.rn.f64 	%fd65, %fd63, %fd62, %fd64;
	mul.f64 	%fd66, %fd65, %fd65;
	mov.f64 	%fd67, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd68, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	mov.f64 	%fd70, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd71, %fd69, %fd66, %fd70;
	mov.f64 	%fd72, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd73, %fd71, %fd66, %fd72;
	mov.f64 	%fd74, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd75, %fd73, %fd66, %fd74;
	mov.f64 	%fd76, 0d3F624924923BE72D;
	fma.rn.f64 	%fd77, %fd75, %fd66, %fd76;
	mov.f64 	%fd78, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd79, %fd77, %fd66, %fd78;
	mov.f64 	%fd80, 0d3FB5555555555554;
	fma.rn.f64 	%fd81, %fd79, %fd66, %fd80;
	sub.f64 	%fd82, %fd63, %fd65;
	add.f64 	%fd83, %fd82, %fd82;
	neg.f64 	%fd84, %fd65;
	fma.rn.f64 	%fd85, %fd84, %fd63, %fd83;
	mul.f64 	%fd86, %fd62, %fd85;
	mul.f64 	%fd87, %fd81, %fd66;
	fma.rn.f64 	%fd88, %fd87, %fd65, %fd86;
	xor.b32  	%r29, %r49, -2147483648;
	mov.u32 	%r30, -2147483648;
	mov.u32 	%r31, 1127219200;
	mov.b64 	%fd89, {%r29, %r31};
	mov.b64 	%fd90, {%r30, %r31};
	sub.f64 	%fd91, %fd89, %fd90;
	mov.f64 	%fd92, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd93, %fd91, %fd92, %fd65;
	neg.f64 	%fd94, %fd91;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	sub.f64 	%fd96, %fd95, %fd65;
	sub.f64 	%fd97, %fd88, %fd96;
	mov.f64 	%fd98, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd99, %fd91, %fd98, %fd97;
	add.f64 	%fd7, %fd93, %fd99;
	neg.f64 	%fd297, %fd7;
	bra.uni 	BB86_32;

BB86_15:
	mov.f64 	%fd100, 0d3FF0000000000000;
	sub.f64 	%fd101, %fd100, %fd24;
	mov.f64 	%fd102, 0d3FA3EB504359EB88;
	mov.f64 	%fd103, 0d3F881F6D2A4C4310;
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0d3FAE35D8DEB06317;
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3FAED469A8B6ECCE;
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0d3FACC1B1C357BEFE;
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3FAD7154DB67F79F;
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0d3FAFCC622CF2F7BB;
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FB11747A4D1CC43;
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0d3FB24CE16A21B8AC;
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FB3B1C21A7BCB00;
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0d3FB556723452ED57;
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	mov.f64 	%fd123, 0d3FB748C00891544F;
	fma.rn.f64 	%fd124, %fd122, %fd101, %fd123;
	mov.f64 	%fd125, 0d3FB9A0207808CF40;
	fma.rn.f64 	%fd126, %fd124, %fd101, %fd125;
	mov.f64 	%fd127, 0d3FBC80673B8AE26B;
	fma.rn.f64 	%fd128, %fd126, %fd101, %fd127;
	mov.f64 	%fd129, 0d3FC010B364B7E555;
	fma.rn.f64 	%fd130, %fd128, %fd101, %fd129;
	mov.f64 	%fd131, 0d3FC2703A1D239658;
	fma.rn.f64 	%fd132, %fd130, %fd101, %fd131;
	mov.f64 	%fd133, 0d3FC5B40CB1137E6E;
	fma.rn.f64 	%fd134, %fd132, %fd101, %fd133;
	mov.f64 	%fd135, 0d3FCA8B9C17AC4F03;
	fma.rn.f64 	%fd136, %fd134, %fd101, %fd135;
	mov.f64 	%fd137, 0d3FD151322AC7CB52;
	fma.rn.f64 	%fd138, %fd136, %fd101, %fd137;
	mov.f64 	%fd139, 0d3FD9A4D55BEAB1D4;
	fma.rn.f64 	%fd140, %fd138, %fd101, %fd139;
	mov.f64 	%fd141, 0d3FEA51A6625307D6;
	fma.rn.f64 	%fd142, %fd140, %fd101, %fd141;
	mov.f64 	%fd143, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd144, %fd142, %fd101, %fd143;
	mul.f64 	%fd297, %fd144, %fd101;
	bra.uni 	BB86_32;

BB86_16:
	add.f64 	%fd145, %fd24, 0dC000000000000000;
	mov.f64 	%fd146, 0dBE71FA71D78C0EE2;
	mov.f64 	%fd147, 0d3E452636124338B3;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	mov.f64 	%fd149, 0d3E8D111F31E61306;
	fma.rn.f64 	%fd150, %fd148, %fd145, %fd149;
	mov.f64 	%fd151, 0dBEA0502BBE1B2706;
	fma.rn.f64 	%fd152, %fd150, %fd145, %fd151;
	mov.f64 	%fd153, 0d3EB06850B2970292;
	fma.rn.f64 	%fd154, %fd152, %fd145, %fd153;
	mov.f64 	%fd155, 0dBEC108474875033D;
	fma.rn.f64 	%fd156, %fd154, %fd145, %fd155;
	mov.f64 	%fd157, 0d3ED24ACCC62909DC;
	fma.rn.f64 	%fd158, %fd156, %fd145, %fd157;
	mov.f64 	%fd159, 0dBEE3CB25209E63BE;
	fma.rn.f64 	%fd160, %fd158, %fd145, %fd159;
	mov.f64 	%fd161, 0d3EF581CBBC8CDC7B;
	fma.rn.f64 	%fd162, %fd160, %fd145, %fd161;
	mov.f64 	%fd163, 0dBF078E04B85C7597;
	fma.rn.f64 	%fd164, %fd162, %fd145, %fd163;
	mov.f64 	%fd165, 0d3F1A12730CF45051;
	fma.rn.f64 	%fd166, %fd164, %fd145, %fd165;
	mov.f64 	%fd167, 0dBF2D3FD354062012;
	fma.rn.f64 	%fd168, %fd166, %fd145, %fd167;
	mov.f64 	%fd169, 0d3F40B36B0B4DE323;
	fma.rn.f64 	%fd170, %fd168, %fd145, %fd169;
	mov.f64 	%fd171, 0dBF538AC5C6D0317A;
	fma.rn.f64 	%fd172, %fd170, %fd145, %fd171;
	mov.f64 	%fd173, 0d3F67ADD6EAAB19FC;
	fma.rn.f64 	%fd174, %fd172, %fd145, %fd173;
	mov.f64 	%fd175, 0dBF7E404FC20E4D5B;
	fma.rn.f64 	%fd176, %fd174, %fd145, %fd175;
	mov.f64 	%fd177, 0d3F951322AC7DA390;
	fma.rn.f64 	%fd178, %fd176, %fd145, %fd177;
	mov.f64 	%fd179, 0dBFB13E001A5578A3;
	fma.rn.f64 	%fd180, %fd178, %fd145, %fd179;
	mov.f64 	%fd181, 0d3FD4A34CC4A60FA3;
	fma.rn.f64 	%fd182, %fd180, %fd145, %fd181;
	mov.f64 	%fd183, 0d3FDB0EE6072093CF;
	fma.rn.f64 	%fd184, %fd182, %fd145, %fd183;
	mul.f64 	%fd297, %fd184, %fd145;
	bra.uni 	BB86_32;

BB86_17:
	setp.gt.s32	%p12, %r50, 1075838975;
	@%p12 bra 	BB86_19;

	add.f64 	%fd185, %fd24, 0dC008000000000000;
	mov.f64 	%fd186, 0dC1122B7730207EF3;
	mov.f64 	%fd187, 0dC0AF7040BB18FB05;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0dC1585A0DB81DE7D0;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0dC18A992B8BA94677;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0dC1AAC5CB6957CC20;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0dC1BC0E2B308774BE;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0dC1C6BA13DCAE7F67;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0dC1CCF33B9C3D120C;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	add.f64 	%fd201, %fd185, 0dC08FF62E0BE189FE;
	mov.f64 	%fd202, 0dC10074FACE10C93F;
	fma.rn.f64 	%fd203, %fd201, %fd185, %fd202;
	mov.f64 	%fd204, 0dC151B662F8D75791;
	fma.rn.f64 	%fd205, %fd203, %fd185, %fd204;
	mov.f64 	%fd206, 0dC18EE64AB4D207F7;
	fma.rn.f64 	%fd207, %fd205, %fd185, %fd206;
	mov.f64 	%fd208, 0dC1B9051687C9951A;
	fma.rn.f64 	%fd209, %fd207, %fd185, %fd208;
	mov.f64 	%fd210, 0dC1D2B866BF0B853D;
	fma.rn.f64 	%fd211, %fd209, %fd185, %fd210;
	mov.f64 	%fd212, 0dC1D4E2130E9DC133;
	fma.rn.f64 	%fd213, %fd211, %fd185, %fd212;
	div.rn.f64 	%fd214, %fd200, %fd213;
	add.f64 	%fd297, %fd214, %fd185;
	bra.uni 	BB86_32;

BB86_19:
	// inline asm
	rcp.approx.ftz.f64 %fd215,%fd24;
	// inline asm
	neg.f64 	%fd13, %fd24;
	mov.f64 	%fd217, 0d3FF0000000000000;
	fma.rn.f64 	%fd218, %fd13, %fd215, %fd217;
	fma.rn.f64 	%fd219, %fd218, %fd218, %fd218;
	fma.rn.f64 	%fd220, %fd219, %fd215, %fd215;
	mul.f64 	%fd221, %fd220, %fd220;
	mov.f64 	%fd222, 0d3F4B68B992738FBF;
	mov.f64 	%fd223, 0dBF5AC321034783F9;
	fma.rn.f64 	%fd224, %fd223, %fd221, %fd222;
	mov.f64 	%fd225, 0dBF4380D01E4F7B8C;
	fma.rn.f64 	%fd226, %fd224, %fd221, %fd225;
	mov.f64 	%fd227, 0d3F4A019FA29F7264;
	fma.rn.f64 	%fd228, %fd226, %fd221, %fd227;
	mov.f64 	%fd229, 0dBF66C16C16B2ACEC;
	fma.rn.f64 	%fd230, %fd228, %fd221, %fd229;
	mov.f64 	%fd231, 0d3FB5555555555545;
	fma.rn.f64 	%fd232, %fd230, %fd221, %fd231;
	mov.f64 	%fd233, 0d3FED67F1C864BEAE;
	fma.rn.f64 	%fd14, %fd232, %fd220, %fd233;
	setp.lt.s32	%p13, %r50, 2146435072;
	setp.gt.f64	%p14, %fd24, 0d0000000000000000;
	and.pred  	%p15, %p14, %p13;
	@%p15 bra 	BB86_25;

	abs.f64 	%fd234, %fd24;
	setp.gtu.f64	%p16, %fd234, 0d7FF0000000000000;
	@%p16 bra 	BB86_24;

	setp.neu.f64	%p17, %fd24, 0d0000000000000000;
	@%p17 bra 	BB86_23;

	mov.f64 	%fd296, 0dFFF0000000000000;
	bra.uni 	BB86_31;

BB86_23:
	setp.eq.f64	%p18, %fd24, 0d7FF0000000000000;
	selp.f64	%fd296, %fd24, 0dFFF8000000000000, %p18;
	bra.uni 	BB86_31;

BB86_24:
	add.f64 	%fd296, %fd24, %fd24;
	bra.uni 	BB86_31;

BB86_25:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd24;
	}
	setp.lt.s32	%p19, %r50, 1048576;
	@%p19 bra 	BB86_27;

	mov.u32 	%r52, -1023;
	bra.uni 	BB86_28;

BB86_27:
	mul.f64 	%fd236, %fd24, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd236;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd236;
	}
	mov.u32 	%r52, -1077;

BB86_28:
	shr.u32 	%r34, %r50, 20;
	add.s32 	%r53, %r52, %r34;
	and.b32  	%r35, %r50, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd295, {%r51, %r36};
	setp.lt.s32	%p20, %r36, 1073127583;
	@%p20 bra 	BB86_30;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd295;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd295;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd295, {%r37, %r39};
	add.s32 	%r53, %r53, 1;

BB86_30:
	add.f64 	%fd238, %fd295, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd237,%fd238;
	// inline asm
	neg.f64 	%fd240, %fd238;
	fma.rn.f64 	%fd241, %fd240, %fd237, %fd217;
	fma.rn.f64 	%fd242, %fd241, %fd241, %fd241;
	fma.rn.f64 	%fd243, %fd242, %fd237, %fd237;
	add.f64 	%fd244, %fd295, 0dBFF0000000000000;
	mul.f64 	%fd245, %fd244, %fd243;
	fma.rn.f64 	%fd246, %fd244, %fd243, %fd245;
	mul.f64 	%fd247, %fd246, %fd246;
	mov.f64 	%fd248, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd249, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd250, %fd249, %fd247, %fd248;
	mov.f64 	%fd251, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd252, %fd250, %fd247, %fd251;
	mov.f64 	%fd253, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd254, %fd252, %fd247, %fd253;
	mov.f64 	%fd255, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd256, %fd254, %fd247, %fd255;
	mov.f64 	%fd257, 0d3F624924923BE72D;
	fma.rn.f64 	%fd258, %fd256, %fd247, %fd257;
	mov.f64 	%fd259, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd260, %fd258, %fd247, %fd259;
	mov.f64 	%fd261, 0d3FB5555555555554;
	fma.rn.f64 	%fd262, %fd260, %fd247, %fd261;
	sub.f64 	%fd263, %fd244, %fd246;
	add.f64 	%fd264, %fd263, %fd263;
	neg.f64 	%fd265, %fd246;
	fma.rn.f64 	%fd266, %fd265, %fd244, %fd264;
	mul.f64 	%fd267, %fd243, %fd266;
	mul.f64 	%fd268, %fd262, %fd247;
	fma.rn.f64 	%fd269, %fd268, %fd246, %fd267;
	xor.b32  	%r40, %r53, -2147483648;
	mov.u32 	%r41, -2147483648;
	mov.u32 	%r42, 1127219200;
	mov.b64 	%fd270, {%r40, %r42};
	mov.b64 	%fd271, {%r41, %r42};
	sub.f64 	%fd272, %fd270, %fd271;
	mov.f64 	%fd273, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd246;
	neg.f64 	%fd275, %fd272;
	fma.rn.f64 	%fd276, %fd275, %fd273, %fd274;
	sub.f64 	%fd277, %fd276, %fd246;
	sub.f64 	%fd278, %fd269, %fd277;
	mov.f64 	%fd279, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd280, %fd272, %fd279, %fd278;
	add.f64 	%fd296, %fd274, %fd280;

BB86_31:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd296;
	}
	add.s32 	%r44, %r43, -1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd296;
	}
	mov.b64 	%fd281, {%r45, %r44};
	add.f64 	%fd282, %fd24, 0dBFE0000000000000;
	fma.rn.f64 	%fd283, %fd281, %fd282, %fd14;
	fma.rn.f64 	%fd284, %fd281, %fd282, %fd13;
	add.f64 	%fd285, %fd284, %fd283;
	setp.eq.f64	%p21, %fd24, 0d7FF0000000000000;
	selp.f64	%fd297, %fd24, %fd285, %p21;

BB86_32:
	st.param.f64	[funj_retval0+0], %fd297;
	ret;
}


