//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Jul 31 22:29:38 2014 (1406860178)
// Cuda compilation tools, release 6.5, V6.5.14
//

.version 4.1
.target sm_20
.address_size 64

.func  (.param .b64 funj_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.func  (.param .b64 funj_retval0) __internal_lgamma_pos
(
	.param .b64 __internal_lgamma_pos_param_0
)
;
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .func  (.param .b32 funj_retval0) ___floors(
	.param .b32 ___floors_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___floors_param_0];
	cvt.rmi.f32.f32	%f2, %f1;
	st.param.f32	[funj_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___floord(
	.param .b64 ___floord_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___floord_param_0];
	cvt.rmi.f64.f64	%fd2, %fd1;
	st.param.f64	[funj_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floori(
	.param .b32 ___floori_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___floori_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___flooru(
	.param .b32 ___flooru_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___flooru_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floorj(
	.param .b32 ___floorj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.s8 	%rs1, [___floorj_param_0];
	cvt.s32.s16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___floorv(
	.param .b32 ___floorv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___floorv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceils(
	.param .b32 ___ceils_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___ceils_param_0];
	cvt.rpi.f32.f32	%f2, %f1;
	st.param.f32	[funj_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___ceild(
	.param .b64 ___ceild_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___ceild_param_0];
	cvt.rpi.f64.f64	%fd2, %fd1;
	st.param.f64	[funj_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceili(
	.param .b32 ___ceili_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___ceili_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceilu(
	.param .b32 ___ceilu_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___ceilu_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceilj(
	.param .b32 ___ceilj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.s8 	%rs1, [___ceilj_param_0];
	cvt.s32.s16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___ceilv(
	.param .b32 ___ceilv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___ceilv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___rounds(
	.param .b32 ___rounds_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<4>;
	.reg .f32 	%f<10>;


	ld.param.f32 	%f4, [___rounds_param_0];
	abs.f32 	%f5, %f4;
	mov.b32 	 %r1, %f4;
	and.b32  	%r2, %r1, -2147483648;
	or.b32  	%r3, %r2, 1056964608;
	mov.b32 	 %f6, %r3;
	add.f32 	%f7, %f6, %f4;
	cvt.rzi.f32.f32	%f8, %f7;
	setp.gt.f32	%p1, %f5, 0f4B000000;
	selp.f32	%f9, %f4, %f8, %p1;
	setp.geu.f32	%p2, %f5, 0f3F000000;
	@%p2 bra 	BB12_2;

	cvt.rzi.f32.f32	%f9, %f4;

BB12_2:
	st.param.f32	[funj_retval0+0], %f9;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___roundd(
	.param .b64 ___roundd_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f64 	%fd<9>;


	ld.param.f64 	%fd8, [___roundd_param_0];
	abs.f64 	%fd1, %fd8;
	setp.ge.f64	%p1, %fd1, 0d4330000000000000;
	@%p1 bra 	BB13_2;

	add.f64 	%fd5, %fd1, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd6, %fd5;
	setp.lt.f64	%p2, %fd1, 0d3FE0000000000000;
	selp.f64	%fd7, 0d0000000000000000, %fd6, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1, %temp}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd8;
	}
	and.b32  	%r4, %r3, -2147483648;
	or.b32  	%r5, %r2, %r4;
	mov.b64 	%fd8, {%r1, %r5};

BB13_2:
	st.param.f64	[funj_retval0+0], %fd8;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundi(
	.param .b32 ___roundi_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___roundi_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundu(
	.param .b32 ___roundu_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___roundu_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundj(
	.param .b32 ___roundj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.s8 	%rs1, [___roundj_param_0];
	cvt.s32.s16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___roundv(
	.param .b32 ___roundv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___roundv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___abss(
	.param .b32 ___abss_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [___abss_param_0];
	abs.f32 	%f2, %f1;
	st.param.f32	[funj_retval0+0], %f2;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___absd(
	.param .b64 ___absd_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [___absd_param_0];
	abs.f64 	%fd2, %fd1;
	st.param.f64	[funj_retval0+0], %fd2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absi(
	.param .b32 ___absi_param_0
)
{
	.reg .s32 	%r<3>;


	ld.param.u32 	%r1, [___absi_param_0];
	abs.s32 	%r2, %r1;
	st.param.b32	[funj_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absj(
	.param .b32 ___absj_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<4>;


	ld.param.s8 	%rs1, [___absj_param_0];
	cvt.s32.s16	%r1, %rs1;
	abs.s32 	%r2, %r1;
	cvt.s32.s8 	%r3, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absu(
	.param .b32 ___absu_param_0
)
{
	.reg .s32 	%r<2>;


	ld.param.u32 	%r1, [___absu_param_0];
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absv(
	.param .b32 ___absv_param_0
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___absv_param_0];
	cvt.u32.u16	%r1, %rs1;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammas(
	.param .b32 ___tgammas_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<99>;


	ld.param.f32 	%f24, [___tgammas_param_0];
	setp.ltu.f32	%p1, %f24, 0f00000000;
	@%p1 bra 	BB24_5;

	setp.gt.f32	%p2, %f24, 0f42100000;
	selp.f32	%f1, 0f42100000, %f24, %p2;
	setp.gt.f32	%p3, %f1, 0f42081EB8;
	add.f32 	%f2, %f1, 0fBF800000;
	selp.f32	%f86, %f2, %f1, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB24_2;
	bra.uni 	BB24_4;

BB24_2:
	mov.f32 	%f88, %f87;

BB24_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB24_3;

BB24_4:
	setp.ltu.f32	%p6, %f1, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f1;
	setp.lt.f32	%p7, %f1, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f2;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB24_12;

BB24_5:
	cvt.rmi.f32.f32	%f49, %f24;
	setp.eq.f32	%p9, %f49, %f24;
	selp.f32	%f50, 0f7FFFFFFF, %f24, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f13, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f13, 0fC2081EB8;
	add.f32 	%f51, %f13, 0f40C00000;
	selp.f32	%f95, %f51, %f13, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB24_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB24_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB24_7;

BB24_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f13, 0fC2081EB8;
	@%p14 bra 	BB24_12;

	add.f32 	%f70, %f13, 0f3F800000;
	mul.f32 	%f71, %f13, %f70;
	add.f32 	%f72, %f13, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f13, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f13, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f13, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f24, 0fC2280000;
	@%p15 bra 	BB24_12;

	cvt.rzi.s32.f32	%r1, %f24;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32	%p16, %r2, 1;
	@%p16 bra 	BB24_12;

	mov.f32 	%f98, 0f80000000;

BB24_12:
	st.param.f32	[funj_retval0+0], %f98;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___tgammad(
	.param .b64 ___tgammad_param_0
)
{
	.reg .pred 	%p<27>;
	.reg .s32 	%r<69>;
	.reg .f32 	%f<3>;
	.reg .s64 	%rd<5>;
	.reg .f64 	%fd<409>;


	ld.param.f64 	%fd401, [___tgammad_param_0];
	setp.ltu.f64	%p1, %fd401, 0d0000000000000000;
	@%p1 bra 	BB25_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd401;
	}
	setp.lt.s32	%p2, %r64, 1076756480;
	@%p2 bra 	BB25_17;

	setp.lt.f64	%p3, %fd401, 0d406573FAE561F648;
	@%p3 bra 	BB25_4;

	mov.f64 	%fd408, 0d7FF0000000000000;
	bra.uni 	BB25_43;

BB25_4:
	// inline asm
	rcp.approx.ftz.f64 %fd54,%fd401;
	// inline asm
	neg.f64 	%fd56, %fd401;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd54, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd54, %fd54;
	mov.f64 	%fd61, 0d3F4B8239C670E690;
	mov.f64 	%fd62, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0dBF0B1D75D3346711;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0dBF436773BDB97B48;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3F1247604839C038;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0d3F49B0FF6874F2C4;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0dBF2E13CE465FA859;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0dBF65F7268EDAB4C8;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3F6C71C71C71C71C;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	mov.f64 	%fd78, 0d3FB5555555555555;
	fma.rn.f64 	%fd79, %fd77, %fd60, %fd78;
	fma.rn.f64 	%fd1, %fd79, %fd60, %fd57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd401;
	}
	shr.u32 	%r65, %r64, 20;
	setp.ne.s32	%p4, %r65, 0;
	@%p4 bra 	BB25_6;

	mul.f64 	%fd80, %fd401, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd80;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd80;
	}
	shr.u32 	%r25, %r64, 20;
	add.s32 	%r65, %r25, -54;

BB25_6:
	add.s32 	%r66, %r65, -1023;
	and.b32  	%r26, %r64, -2146435073;
	or.b32  	%r27, %r26, 1072693248;
	mov.b64 	%fd382, {%r63, %r27};
	setp.lt.u32	%p5, %r27, 1073127583;
	@%p5 bra 	BB25_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd382;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd382;
	}
	add.s32 	%r30, %r29, -1048576;
	mov.b64 	%fd382, {%r28, %r30};
	add.s32 	%r66, %r65, -1022;

BB25_8:
	add.f64 	%fd83, %fd401, 0dBFE0000000000000;
	add.f64 	%fd82, %fd382, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd81,%fd82;
	// inline asm
	neg.f64 	%fd85, %fd82;
	fma.rn.f64 	%fd86, %fd85, %fd81, %fd57;
	fma.rn.f64 	%fd87, %fd86, %fd86, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd81, %fd81;
	add.f64 	%fd89, %fd382, 0dBFF0000000000000;
	mul.f64 	%fd90, %fd89, %fd88;
	fma.rn.f64 	%fd91, %fd89, %fd88, %fd90;
	mul.f64 	%fd92, %fd91, %fd91;
	mov.f64 	%fd93, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd94, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F6249249242B910;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F89999999999DFB;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	sub.f64 	%fd106, %fd89, %fd91;
	add.f64 	%fd107, %fd106, %fd106;
	neg.f64 	%fd108, %fd91;
	fma.rn.f64 	%fd109, %fd108, %fd89, %fd107;
	mul.f64 	%fd110, %fd88, %fd109;
	fma.rn.f64 	%fd111, %fd105, %fd92, 0d3FB5555555555555;
	sub.f64 	%fd113, %fd78, %fd111;
	fma.rn.f64 	%fd114, %fd105, %fd92, %fd113;
	add.f64 	%fd115, %fd114, 0d0000000000000000;
	add.f64 	%fd116, %fd115, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd117, %fd111, %fd116;
	sub.f64 	%fd118, %fd111, %fd117;
	add.f64 	%fd119, %fd118, %fd116;
	mul.rn.f64 	%fd120, %fd91, %fd91;
	neg.f64 	%fd121, %fd120;
	fma.rn.f64 	%fd122, %fd91, %fd91, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd110;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd110;
	}
	add.s32 	%r33, %r32, 1048576;
	mov.b64 	%fd123, {%r31, %r33};
	fma.rn.f64 	%fd124, %fd91, %fd123, %fd122;
	mul.rn.f64 	%fd125, %fd120, %fd91;
	neg.f64 	%fd126, %fd125;
	fma.rn.f64 	%fd127, %fd120, %fd91, %fd126;
	fma.rn.f64 	%fd128, %fd120, %fd110, %fd127;
	fma.rn.f64 	%fd129, %fd124, %fd91, %fd128;
	mul.rn.f64 	%fd130, %fd117, %fd125;
	neg.f64 	%fd131, %fd130;
	fma.rn.f64 	%fd132, %fd117, %fd125, %fd131;
	fma.rn.f64 	%fd133, %fd117, %fd129, %fd132;
	fma.rn.f64 	%fd134, %fd119, %fd125, %fd133;
	add.f64 	%fd135, %fd130, %fd134;
	sub.f64 	%fd136, %fd130, %fd135;
	add.f64 	%fd137, %fd136, %fd134;
	add.f64 	%fd138, %fd91, %fd135;
	sub.f64 	%fd139, %fd91, %fd138;
	add.f64 	%fd140, %fd139, %fd135;
	add.f64 	%fd141, %fd140, %fd137;
	add.f64 	%fd142, %fd141, %fd110;
	add.f64 	%fd143, %fd138, %fd142;
	sub.f64 	%fd144, %fd138, %fd143;
	add.f64 	%fd145, %fd144, %fd142;
	xor.b32  	%r34, %r66, -2147483648;
	mov.u32 	%r35, -2147483648;
	mov.u32 	%r36, 1127219200;
	mov.b64 	%fd146, {%r34, %r36};
	mov.b64 	%fd147, {%r35, %r36};
	sub.f64 	%fd148, %fd146, %fd147;
	mov.f64 	%fd149, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd150, %fd148, %fd149, %fd143;
	neg.f64 	%fd151, %fd148;
	fma.rn.f64 	%fd152, %fd151, %fd149, %fd150;
	sub.f64 	%fd153, %fd152, %fd143;
	sub.f64 	%fd154, %fd145, %fd153;
	mov.f64 	%fd155, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd156, %fd148, %fd155, %fd154;
	add.f64 	%fd157, %fd150, %fd156;
	sub.f64 	%fd158, %fd150, %fd157;
	add.f64 	%fd159, %fd158, %fd156;
	mul.rn.f64 	%fd160, %fd157, %fd83;
	neg.f64 	%fd161, %fd160;
	fma.rn.f64 	%fd162, %fd157, %fd83, %fd161;
	fma.rn.f64 	%fd163, %fd159, %fd83, %fd162;
	add.f64 	%fd164, %fd160, %fd163;
	sub.f64 	%fd165, %fd160, %fd164;
	add.f64 	%fd166, %fd165, %fd163;
	sub.f64 	%fd167, %fd164, %fd401;
	sub.f64 	%fd168, %fd164, %fd167;
	sub.f64 	%fd169, %fd168, %fd401;
	add.f64 	%fd170, %fd169, 0d0000000000000000;
	add.f64 	%fd171, %fd170, %fd166;
	add.f64 	%fd5, %fd167, %fd171;
	sub.f64 	%fd172, %fd167, %fd5;
	add.f64 	%fd6, %fd172, %fd171;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd5;
	}
	mov.b32 	 %f1, %r13;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p6, %f2, 0f40874911;
	@%p6 bra 	BB25_10;

	setp.lt.s32	%p7, %r13, 0;
	selp.f64	%fd173, 0d0000000000000000, 0d7FF0000000000000, %p7;
	abs.f64 	%fd174, %fd5;
	setp.gtu.f64	%p8, %fd174, 0d7FF0000000000000;
	add.f64 	%fd175, %fd5, %fd5;
	selp.f64	%fd384, %fd175, %fd173, %p8;
	bra.uni 	BB25_14;

BB25_10:
	mov.f64 	%fd381, 0d3FF0000000000000;
	mov.f64 	%fd176, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd177, %fd5, %fd176;
	mov.f64 	%fd178, 0d4338000000000000;
	add.rn.f64 	%fd179, %fd177, %fd178;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd179;
	}
	mov.f64 	%fd180, 0dC338000000000000;
	add.rn.f64 	%fd181, %fd179, %fd180;
	mov.f64 	%fd182, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd183, %fd181, %fd182, %fd5;
	mov.f64 	%fd184, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd185, %fd181, %fd184, %fd183;
	mov.f64 	%fd186, 0d3E928AF3FCA213EA;
	mov.f64 	%fd187, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0d3F81111111122322;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0d3FA55555555502A1;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	mov.f64 	%fd201, 0d3FC5555555555511;
	fma.rn.f64 	%fd202, %fd200, %fd185, %fd201;
	mov.f64 	%fd203, 0d3FE000000000000B;
	fma.rn.f64 	%fd204, %fd202, %fd185, %fd203;
	fma.rn.f64 	%fd206, %fd204, %fd185, %fd381;
	fma.rn.f64 	%fd383, %fd206, %fd185, %fd381;
	abs.s32 	%r37, %r14;
	setp.lt.s32	%p9, %r37, 1023;
	@%p9 bra 	BB25_12;

	add.s32 	%r38, %r14, 2046;
	shl.b32 	%r39, %r38, 19;
	and.b32  	%r40, %r39, -1048576;
	shl.b32 	%r41, %r38, 20;
	sub.s32 	%r67, %r41, %r40;
	mov.u32 	%r42, 0;
	mov.b64 	%fd207, {%r42, %r40};
	mul.f64 	%fd383, %fd383, %fd207;
	bra.uni 	BB25_13;

BB25_12:
	shl.b32 	%r43, %r14, 20;
	add.s32 	%r67, %r43, 1072693248;

BB25_13:
	mov.u32 	%r44, 0;
	mov.b64 	%fd208, {%r44, %r67};
	mul.f64 	%fd384, %fd383, %fd208;

BB25_14:
	abs.f64 	%fd209, %fd384;
	setp.eq.f64	%p10, %fd209, 0d7FF0000000000000;
	@%p10 bra 	BB25_16;

	fma.rn.f64 	%fd384, %fd384, %fd6, %fd384;

BB25_16:
	mul.f64 	%fd210, %fd384, 0dBCAA6A0D6F814637;
	mov.f64 	%fd211, 0d40040D931FF62706;
	fma.rn.f64 	%fd212, %fd384, %fd211, %fd210;
	mul.f64 	%fd408, %fd212, %fd1;
	bra.uni 	BB25_43;

BB25_17:
	setp.gt.s32	%p11, %r64, 1073217535;
	add.f64 	%fd405, %fd401, 0dBFF0000000000000;
	mov.f64 	%fd385, 0d3FF0000000000000;
	@%p11 bra 	BB25_19;

	mov.f64 	%fd404, %fd401;
	bra.uni 	BB25_21;

BB25_19:
	mov.f64 	%fd407, %fd405;
	mov.f64 	%fd406, %fd401;

BB25_20:
	mov.f64 	%fd19, %fd406;
	mov.f64 	%fd406, %fd407;
	neg.f64 	%fd215, %fd385;
	fma.rn.f64 	%fd385, %fd385, %fd19, %fd215;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd406;
	}
	setp.gt.s32	%p12, %r45, 1073217535;
	add.f64 	%fd407, %fd406, 0dBFF0000000000000;
	mov.f64 	%fd405, %fd407;
	mov.f64 	%fd404, %fd406;
	@%p12 bra 	BB25_20;

BB25_21:
	setp.gt.s32	%p13, %r64, 1071644671;
	selp.f64	%fd216, %fd405, %fd404, %p13;
	mov.f64 	%fd217, 0dBE8B338C457183B6;
	mov.f64 	%fd218, 0dBDFE6BDF8CC487CD;
	fma.rn.f64 	%fd219, %fd218, %fd216, %fd217;
	mov.f64 	%fd220, 0d3EB31831766A0388;
	fma.rn.f64 	%fd221, %fd219, %fd216, %fd220;
	mov.f64 	%fd222, 0dBEB4FC07FC9F1563;
	fma.rn.f64 	%fd223, %fd221, %fd216, %fd222;
	mov.f64 	%fd224, 0dBEF51D59DCE6A679;
	fma.rn.f64 	%fd225, %fd223, %fd216, %fd224;
	mov.f64 	%fd226, 0d3F20C8A6351CB1F9;
	fma.rn.f64 	%fd227, %fd225, %fd216, %fd226;
	mov.f64 	%fd228, 0dBF2C364D9E00D4CA;
	fma.rn.f64 	%fd229, %fd227, %fd216, %fd228;
	mov.f64 	%fd230, 0dBF5317112046830B;
	fma.rn.f64 	%fd231, %fd229, %fd216, %fd230;
	mov.f64 	%fd232, 0d3F7D919C50FF9416;
	fma.rn.f64 	%fd233, %fd231, %fd216, %fd232;
	mov.f64 	%fd234, 0dBF83B4AF28728BB0;
	fma.rn.f64 	%fd235, %fd233, %fd216, %fd234;
	mov.f64 	%fd236, 0dBFA59AF103C171DC;
	fma.rn.f64 	%fd237, %fd235, %fd216, %fd236;
	mov.f64 	%fd238, 0d3FC5512320B45D97;
	fma.rn.f64 	%fd239, %fd237, %fd216, %fd238;
	mov.f64 	%fd240, 0dBFA5815E8FA27607;
	fma.rn.f64 	%fd241, %fd239, %fd216, %fd240;
	mov.f64 	%fd242, 0dBFE4FCF4026AFA4B;
	fma.rn.f64 	%fd243, %fd241, %fd216, %fd242;
	mov.f64 	%fd244, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd245, %fd243, %fd216, %fd244;
	mov.f64 	%fd246, 0d3FF0000000000000;
	fma.rn.f64 	%fd247, %fd245, %fd216, %fd246;
	mul.f64 	%fd248, %fd247, %fd401;
	setp.lt.s32	%p14, %r64, 1071644672;
	selp.f64	%fd249, %fd248, %fd247, %p14;
	div.rn.f64 	%fd408, %fd385, %fd249;
	bra.uni 	BB25_43;

BB25_22:
	setp.lt.f64	%p15, %fd401, 0d0000000000000000;
	@%p15 bra 	BB25_24;

	add.f64 	%fd408, %fd401, %fd401;
	bra.uni 	BB25_43;

BB25_24:
	cvt.rzi.f64.f64	%fd250, %fd401;
	setp.neu.f64	%p16, %fd250, %fd401;
	@%p16 bra 	BB25_26;

	mov.f64 	%fd408, 0dFFF8000000000000;
	bra.uni 	BB25_43;

BB25_26:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd401;
	}
	setp.lt.u32	%p17, %r18, -1070727168;
	@%p17 bra 	BB25_39;

	setp.lt.u32	%p18, %r18, -1066983424;
	@%p18 bra 	BB25_29;

	cvt.rmi.f64.f64	%fd251, %fd401;
	mul.f64 	%fd252, %fd251, 0d3FE0000000000000;
	cvt.rmi.f64.f64	%fd253, %fd252;
	fma.rn.f64 	%fd254, %fd253, 0dC000000000000000, %fd251;
	setp.eq.f64	%p19, %fd254, 0d3FF0000000000000;
	selp.f64	%fd408, 0d8000000000000000, 0d0000000000000000, %p19;
	bra.uni 	BB25_43;

BB25_29:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd401;
	}
	add.s32 	%r47, %r18, 1048576;
	mov.b64 	%fd255, {%r46, %r47};
	cvt.rni.f64.f64	%fd256, %fd255;
	cvt.rzi.s64.f64	%rd1, %fd256;
	cvt.u32.u64	%r19, %rd1;
	neg.f64 	%fd257, %fd256;
	mov.f64 	%fd258, 0d3FE0000000000000;
	fma.rn.f64 	%fd259, %fd257, %fd258, %fd401;
	mul.f64 	%fd260, %fd259, 0d3CA1A62633145C07;
	mov.f64 	%fd261, 0d400921FB54442D18;
	fma.rn.f64 	%fd262, %fd259, %fd261, %fd260;
	and.b32  	%r48, %r19, 1;
	shl.b32 	%r49, %r48, 3;
	mul.rn.f64 	%fd28, %fd262, %fd262;
	setp.eq.s32	%p20, %r48, 0;
	selp.f64	%fd263, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p20;
	mul.wide.u32 	%rd2, %r49, 8;
	mov.u64 	%rd3, __cudart_sin_cos_coeffs;
	add.s64 	%rd4, %rd2, %rd3;
	ld.const.f64 	%fd264, [%rd4+8];
	fma.rn.f64 	%fd265, %fd263, %fd28, %fd264;
	ld.const.f64 	%fd266, [%rd4+16];
	fma.rn.f64 	%fd267, %fd265, %fd28, %fd266;
	ld.const.f64 	%fd268, [%rd4+24];
	fma.rn.f64 	%fd269, %fd267, %fd28, %fd268;
	ld.const.f64 	%fd270, [%rd4+32];
	fma.rn.f64 	%fd271, %fd269, %fd28, %fd270;
	ld.const.f64 	%fd272, [%rd4+40];
	fma.rn.f64 	%fd273, %fd271, %fd28, %fd272;
	ld.const.f64 	%fd274, [%rd4+48];
	fma.rn.f64 	%fd29, %fd273, %fd28, %fd274;
	fma.rn.f64 	%fd386, %fd29, %fd262, %fd262;
	@%p20 bra 	BB25_31;

	mov.f64 	%fd275, 0d3FF0000000000000;
	fma.rn.f64 	%fd386, %fd29, %fd28, %fd275;

BB25_31:
	and.b32  	%r50, %r19, 2;
	setp.eq.s32	%p21, %r50, 0;
	@%p21 bra 	BB25_33;

	mov.f64 	%fd276, 0d0000000000000000;
	mov.f64 	%fd277, 0dBFF0000000000000;
	fma.rn.f64 	%fd386, %fd386, %fd277, %fd276;

BB25_33:
	mov.f64 	%fd278, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd279, %fd401, %fd278;
	mov.f64 	%fd280, 0d4338000000000000;
	add.rn.f64 	%fd281, %fd279, %fd280;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd281;
	}
	mov.f64 	%fd282, 0dC338000000000000;
	add.rn.f64 	%fd283, %fd281, %fd282;
	mov.f64 	%fd284, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd285, %fd283, %fd284, %fd401;
	mov.f64 	%fd286, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd287, %fd283, %fd286, %fd285;
	mov.f64 	%fd288, 0d3E928AF3FCA213EA;
	mov.f64 	%fd289, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd290, %fd289, %fd287, %fd288;
	mov.f64 	%fd291, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd292, %fd290, %fd287, %fd291;
	mov.f64 	%fd293, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd294, %fd292, %fd287, %fd293;
	mov.f64 	%fd295, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd296, %fd294, %fd287, %fd295;
	mov.f64 	%fd297, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd298, %fd296, %fd287, %fd297;
	mov.f64 	%fd299, 0d3F81111111122322;
	fma.rn.f64 	%fd300, %fd298, %fd287, %fd299;
	mov.f64 	%fd301, 0d3FA55555555502A1;
	fma.rn.f64 	%fd302, %fd300, %fd287, %fd301;
	mov.f64 	%fd303, 0d3FC5555555555511;
	fma.rn.f64 	%fd304, %fd302, %fd287, %fd303;
	mov.f64 	%fd305, 0d3FE000000000000B;
	fma.rn.f64 	%fd306, %fd304, %fd287, %fd305;
	mov.f64 	%fd307, 0d3FF0000000000000;
	fma.rn.f64 	%fd308, %fd306, %fd287, %fd307;
	fma.rn.f64 	%fd387, %fd308, %fd287, %fd307;
	abs.s32 	%r51, %r20;
	setp.lt.s32	%p22, %r51, 1023;
	@%p22 bra 	BB25_35;

	add.s32 	%r52, %r20, 2046;
	shl.b32 	%r53, %r52, 19;
	and.b32  	%r54, %r53, -1048576;
	shl.b32 	%r55, %r52, 20;
	sub.s32 	%r68, %r55, %r54;
	mov.u32 	%r56, 0;
	mov.b64 	%fd309, {%r56, %r54};
	mul.f64 	%fd387, %fd387, %fd309;
	bra.uni 	BB25_36;

BB25_35:
	shl.b32 	%r57, %r20, 20;
	add.s32 	%r68, %r57, 1072693248;

BB25_36:
	mov.u32 	%r58, 0;
	mov.b64 	%fd310, {%r58, %r68};
	mul.f64 	%fd38, %fd387, %fd310;
	abs.f64 	%fd39, %fd401;
	add.f64 	%fd388, %fd39, 0dBFE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd39;
	}
	setp.lt.s32	%p23, %r24, 1080131584;
	@%p23 bra 	BB25_38;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd388;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd388;
	}
	add.s32 	%r61, %r60, -1048576;
	mov.b64 	%fd388, {%r59, %r61};

BB25_38:
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd388;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd313, [retval0+0];
	}
	// Callseq End 0
	mul.f64 	%fd314, %fd38, %fd313;
	setp.gt.s32	%p24, %r24, 1080131583;
	selp.f64	%fd315, %fd314, %fd38, %p24;
	// inline asm
	rcp.approx.ftz.f64 %fd311,%fd39;
	// inline asm
	neg.f64 	%fd316, %fd39;
	fma.rn.f64 	%fd318, %fd316, %fd311, %fd307;
	fma.rn.f64 	%fd319, %fd318, %fd318, %fd318;
	fma.rn.f64 	%fd320, %fd319, %fd311, %fd311;
	mov.f64 	%fd321, 0d3F4B8239C670E690;
	mov.f64 	%fd322, 0d0000000000000000;
	fma.rn.f64 	%fd323, %fd322, %fd320, %fd321;
	mov.f64 	%fd324, 0dBF0B1D75D3346711;
	fma.rn.f64 	%fd325, %fd323, %fd320, %fd324;
	mov.f64 	%fd326, 0dBF436773BDB97B48;
	fma.rn.f64 	%fd327, %fd325, %fd320, %fd326;
	mov.f64 	%fd328, 0d3F1247604839C038;
	fma.rn.f64 	%fd329, %fd327, %fd320, %fd328;
	mov.f64 	%fd330, 0d3F49B0FF6874F2C4;
	fma.rn.f64 	%fd331, %fd329, %fd320, %fd330;
	mov.f64 	%fd332, 0dBF2E13CE465FA859;
	fma.rn.f64 	%fd333, %fd331, %fd320, %fd332;
	mov.f64 	%fd334, 0dBF65F7268EDAB4C8;
	fma.rn.f64 	%fd335, %fd333, %fd320, %fd334;
	mov.f64 	%fd336, 0d3F6C71C71C71C71C;
	fma.rn.f64 	%fd337, %fd335, %fd320, %fd336;
	mov.f64 	%fd338, 0d3FB5555555555555;
	fma.rn.f64 	%fd339, %fd337, %fd320, %fd338;
	fma.rn.f64 	%fd340, %fd339, %fd320, %fd307;
	mul.f64 	%fd341, %fd315, %fd340;
	mul.f64 	%fd342, %fd341, %fd39;
	mul.f64 	%fd343, %fd342, %fd386;
	rcp.rn.f64 	%fd344, %fd343;
	mul.f64 	%fd345, %fd344, 0dBC9A6A0D6F814637;
	mov.f64 	%fd346, 0d3FF40D931FF62706;
	fma.rn.f64 	%fd347, %fd344, %fd346, %fd345;
	div.rn.f64 	%fd408, %fd347, %fd313;
	bra.uni 	BB25_43;

BB25_39:
	setp.lt.u32	%p25, %r18, -1075838976;
	mov.f64 	%fd400, %fd401;
	@%p25 bra 	BB25_42;

	mov.f64 	%fd402, %fd401;
	mov.f64 	%fd403, %fd401;

BB25_41:
	fma.rn.f64 	%fd403, %fd403, %fd402, %fd403;
	add.f64 	%fd402, %fd402, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd402;
	}
	setp.gt.u32	%p26, %r62, -1075838977;
	mov.f64 	%fd401, %fd403;
	mov.f64 	%fd400, %fd402;
	@%p26 bra 	BB25_41;

BB25_42:
	mov.f64 	%fd348, 0dBE8B338C457183B6;
	mov.f64 	%fd349, 0dBDFE6BDF8CC487CD;
	fma.rn.f64 	%fd350, %fd349, %fd400, %fd348;
	mov.f64 	%fd351, 0d3EB31831766A0388;
	fma.rn.f64 	%fd352, %fd350, %fd400, %fd351;
	mov.f64 	%fd353, 0dBEB4FC07FC9F1563;
	fma.rn.f64 	%fd354, %fd352, %fd400, %fd353;
	mov.f64 	%fd355, 0dBEF51D59DCE6A679;
	fma.rn.f64 	%fd356, %fd354, %fd400, %fd355;
	mov.f64 	%fd357, 0d3F20C8A6351CB1F9;
	fma.rn.f64 	%fd358, %fd356, %fd400, %fd357;
	mov.f64 	%fd359, 0dBF2C364D9E00D4CA;
	fma.rn.f64 	%fd360, %fd358, %fd400, %fd359;
	mov.f64 	%fd361, 0dBF5317112046830B;
	fma.rn.f64 	%fd362, %fd360, %fd400, %fd361;
	mov.f64 	%fd363, 0d3F7D919C50FF9416;
	fma.rn.f64 	%fd364, %fd362, %fd400, %fd363;
	mov.f64 	%fd365, 0dBF83B4AF28728BB0;
	fma.rn.f64 	%fd366, %fd364, %fd400, %fd365;
	mov.f64 	%fd367, 0dBFA59AF103C171DC;
	fma.rn.f64 	%fd368, %fd366, %fd400, %fd367;
	mov.f64 	%fd369, 0d3FC5512320B45D97;
	fma.rn.f64 	%fd370, %fd368, %fd400, %fd369;
	mov.f64 	%fd371, 0dBFA5815E8FA27607;
	fma.rn.f64 	%fd372, %fd370, %fd400, %fd371;
	mov.f64 	%fd373, 0dBFE4FCF4026AFA4B;
	fma.rn.f64 	%fd374, %fd372, %fd400, %fd373;
	mov.f64 	%fd375, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd376, %fd374, %fd400, %fd375;
	mov.f64 	%fd377, 0d3FF0000000000000;
	fma.rn.f64 	%fd378, %fd376, %fd400, %fd377;
	mul.f64 	%fd379, %fd401, %fd378;
	rcp.rn.f64 	%fd408, %fd379;

BB25_43:
	st.param.f64	[funj_retval0+0], %fd408;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammai(
	.param .b32 ___tgammai_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.u32 	%r1, [___tgammai_param_0];
	cvt.rn.f32.s32	%f1, %r1;
	setp.ltu.f32	%p1, %f1, 0f00000000;
	@%p1 bra 	BB26_5;

	setp.gt.f32	%p2, %f1, 0f42100000;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB26_2;
	bra.uni 	BB26_4;

BB26_2:
	mov.f32 	%f88, %f87;

BB26_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB26_3;

BB26_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB26_12;

BB26_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB26_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB26_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB26_7;

BB26_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB26_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f1, 0fC2280000;
	@%p15 bra 	BB26_12;

	cvt.rzi.s32.f32	%r2, %f1;
	and.b32  	%r3, %r2, 1;
	setp.eq.b32	%p16, %r3, 1;
	@%p16 bra 	BB26_12;

	mov.f32 	%f98, 0f80000000;

BB26_12:
	cvt.rzi.s32.f32	%r4, %f98;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammau(
	.param .b32 ___tgammau_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.u32 	%r1, [___tgammau_param_0];
	cvt.rn.f32.u32	%f1, %r1;
	setp.ltu.f32	%p1, %f1, 0f00000000;
	@%p1 bra 	BB27_5;

	setp.gt.f32	%p2, %f1, 0f42100000;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB27_2;
	bra.uni 	BB27_4;

BB27_2:
	mov.f32 	%f88, %f87;

BB27_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB27_3;

BB27_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB27_12;

BB27_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB27_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB27_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB27_7;

BB27_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB27_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.geu.f32	%p15, %f1, 0fC2280000;
	@%p15 bra 	BB27_12;

	cvt.rzi.s32.f32	%r2, %f1;
	and.b32  	%r3, %r2, 1;
	setp.eq.b32	%p16, %r3, 1;
	@%p16 bra 	BB27_12;

	mov.f32 	%f98, 0f80000000;

BB27_12:
	cvt.rzi.u32.f32	%r4, %f98;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammaj(
	.param .b32 ___tgammaj_param_0
)
{
	.reg .pred 	%p<17>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<99>;


	ld.param.s8 	%rs1, [___tgammaj_param_0];
	cvt.rn.f32.s16	%f1, %rs1;
	setp.lt.s16	%p1, %rs1, 0;
	@%p1 bra 	BB28_5;

	setp.gt.s16	%p2, %rs1, 36;
	selp.f32	%f2, 0f42100000, %f1, %p2;
	setp.gt.f32	%p3, %f2, 0f42081EB8;
	add.f32 	%f3, %f2, 0fBF800000;
	selp.f32	%f86, %f3, %f2, %p3;
	setp.gt.f32	%p4, %f86, 0f3FC00000;
	add.f32 	%f87, %f86, 0fBF800000;
	mov.f32 	%f82, 0f3F800000;
	@%p4 bra 	BB28_2;
	bra.uni 	BB28_4;

BB28_2:
	mov.f32 	%f88, %f87;

BB28_3:
	mov.f32 	%f86, %f88;
	mul.f32 	%f82, %f82, %f86;
	add.f32 	%f88, %f86, 0fBF800000;
	setp.gt.f32	%p5, %f86, 0f3FC00000;
	mov.f32 	%f87, %f88;
	@%p5 bra 	BB28_3;

BB28_4:
	setp.ltu.f32	%p6, %f2, 0f3F000000;
	selp.f32	%f27, %f86, %f87, %p6;
	mov.f32 	%f28, 0f3BE86AA4;
	mov.f32 	%f29, 0fBA8AA19E;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0fBC1E2998;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0fBD2CBE4A;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mov.f32 	%f35, 0f3E2A8A17;
	fma.rn.f32 	%f36, %f34, %f27, %f35;
	mov.f32 	%f37, 0fBD2C0CBB;
	fma.rn.f32 	%f38, %f36, %f27, %f37;
	mov.f32 	%f39, 0fBF27E7A3;
	fma.rn.f32 	%f40, %f38, %f27, %f39;
	mov.f32 	%f41, 0f3F13C468;
	fma.rn.f32 	%f42, %f40, %f27, %f41;
	mov.f32 	%f43, 0f3F800000;
	fma.rn.f32 	%f44, %f42, %f27, %f43;
	mul.f32 	%f45, %f44, %f2;
	setp.lt.f32	%p7, %f2, 0f3F000000;
	selp.f32	%f46, %f45, %f44, %p7;
	div.approx.f32 	%f47, %f82, %f46;
	mul.f32 	%f48, %f47, %f3;
	selp.f32	%f98, %f48, %f47, %p3;
	bra.uni 	BB28_12;

BB28_5:
	cvt.rmi.f32.f32	%f49, %f1;
	setp.eq.f32	%p9, %f49, %f1;
	selp.f32	%f50, 0f7FFFFFFF, %f1, %p9;
	setp.lt.f32	%p10, %f50, 0fC2246666;
	selp.f32	%f14, 0fC2246666, %f50, %p10;
	setp.lt.f32	%p11, %f14, 0fC2081EB8;
	add.f32 	%f51, %f14, 0f40C00000;
	selp.f32	%f95, %f51, %f14, %p11;
	setp.geu.f32	%p12, %f95, 0fBF000000;
	mov.f32 	%f94, %f95;
	@%p12 bra 	BB28_8;

	mov.f32 	%f96, %f95;
	mov.f32 	%f97, %f95;

BB28_7:
	add.f32 	%f96, %f96, 0f3F800000;
	mul.f32 	%f97, %f97, %f96;
	setp.lt.f32	%p13, %f96, 0fBF000000;
	mov.f32 	%f95, %f97;
	mov.f32 	%f94, %f96;
	@%p13 bra 	BB28_7;

BB28_8:
	mov.f32 	%f52, 0f3BE86AA4;
	mov.f32 	%f53, 0fBA8AA19E;
	fma.rn.f32 	%f54, %f53, %f94, %f52;
	mov.f32 	%f55, 0fBC1E2998;
	fma.rn.f32 	%f56, %f54, %f94, %f55;
	mov.f32 	%f57, 0fBD2CBE4A;
	fma.rn.f32 	%f58, %f56, %f94, %f57;
	mov.f32 	%f59, 0f3E2A8A17;
	fma.rn.f32 	%f60, %f58, %f94, %f59;
	mov.f32 	%f61, 0fBD2C0CBB;
	fma.rn.f32 	%f62, %f60, %f94, %f61;
	mov.f32 	%f63, 0fBF27E7A3;
	fma.rn.f32 	%f64, %f62, %f94, %f63;
	mov.f32 	%f65, 0f3F13C468;
	fma.rn.f32 	%f66, %f64, %f94, %f65;
	mov.f32 	%f67, 0f3F800000;
	fma.rn.f32 	%f68, %f66, %f94, %f67;
	mul.f32 	%f69, %f95, %f68;
	rcp.rn.f32 	%f98, %f69;
	setp.geu.f32	%p14, %f14, 0fC2081EB8;
	@%p14 bra 	BB28_12;

	add.f32 	%f70, %f14, 0f3F800000;
	mul.f32 	%f71, %f14, %f70;
	add.f32 	%f72, %f14, 0f40000000;
	mul.f32 	%f73, %f71, %f72;
	add.f32 	%f74, %f14, 0f40400000;
	mul.f32 	%f75, %f73, %f74;
	add.f32 	%f76, %f14, 0f40800000;
	mul.f32 	%f77, %f75, %f76;
	add.f32 	%f78, %f14, 0f40A00000;
	mul.f32 	%f79, %f77, %f78;
	rcp.rn.f32 	%f80, %f79;
	mul.f32 	%f98, %f98, %f80;
	setp.gt.s16	%p15, %rs1, -43;
	@%p15 bra 	BB28_12;

	cvt.rzi.s32.f32	%r1, %f1;
	and.b32  	%r2, %r1, 1;
	setp.eq.b32	%p16, %r2, 1;
	@%p16 bra 	BB28_12;

	mov.f32 	%f98, 0f80000000;

BB28_12:
	cvt.rzi.s32.f32	%r3, %f98;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___tgammav(
	.param .b32 ___tgammav_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<3>;
	.reg .f32 	%f<45>;


	ld.param.u8 	%rs1, [___tgammav_param_0];
	cvt.rn.f32.u16	%f12, %rs1;
	setp.gt.u16	%p1, %rs1, 36;
	selp.f32	%f1, 0f42100000, %f12, %p1;
	setp.gt.f32	%p2, %f1, 0f42081EB8;
	add.f32 	%f2, %f1, 0fBF800000;
	selp.f32	%f42, %f2, %f1, %p2;
	setp.gt.f32	%p3, %f42, 0f3FC00000;
	add.f32 	%f43, %f42, 0fBF800000;
	mov.f32 	%f38, 0f3F800000;
	@%p3 bra 	BB29_1;
	bra.uni 	BB29_3;

BB29_1:
	mov.f32 	%f44, %f43;

BB29_2:
	mov.f32 	%f42, %f44;
	mul.f32 	%f38, %f38, %f42;
	add.f32 	%f44, %f42, 0fBF800000;
	setp.gt.f32	%p4, %f42, 0f3FC00000;
	mov.f32 	%f43, %f44;
	@%p4 bra 	BB29_2;

BB29_3:
	setp.ltu.f32	%p5, %f1, 0f3F000000;
	selp.f32	%f15, %f42, %f43, %p5;
	mov.f32 	%f16, 0f3BE86AA4;
	mov.f32 	%f17, 0fBA8AA19E;
	fma.rn.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0fBC1E2998;
	fma.rn.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBD2CBE4A;
	fma.rn.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f3E2A8A17;
	fma.rn.f32 	%f24, %f22, %f15, %f23;
	mov.f32 	%f25, 0fBD2C0CBB;
	fma.rn.f32 	%f26, %f24, %f15, %f25;
	mov.f32 	%f27, 0fBF27E7A3;
	fma.rn.f32 	%f28, %f26, %f15, %f27;
	mov.f32 	%f29, 0f3F13C468;
	fma.rn.f32 	%f30, %f28, %f15, %f29;
	mov.f32 	%f31, 0f3F800000;
	fma.rn.f32 	%f32, %f30, %f15, %f31;
	mul.f32 	%f33, %f32, %f1;
	setp.lt.f32	%p6, %f1, 0f3F000000;
	selp.f32	%f34, %f33, %f32, %p6;
	div.approx.f32 	%f35, %f38, %f34;
	mul.f32 	%f36, %f35, %f2;
	selp.f32	%f37, %f36, %f35, %p2;
	cvt.rzi.u32.f32	%r1, %f37;
	and.b32  	%r2, %r1, 255;
	st.param.b32	[funj_retval0+0], %r2;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammas(
	.param .b32 ___lgammas_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<20>;
	.reg .f32 	%f<269>;


	ld.param.f32 	%f39, [___lgammas_param_0];
	abs.f32 	%f1, %f39;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB30_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB30_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f1;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB30_4;

	lg2.approx.f32 	%f261, %f1;
	bra.uni 	BB30_5;

BB30_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f51, %f1, 0f4B800000;
	selp.f32	%f52, %f51, %f1, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r3, %f52;
	and.b32  	%r4, %r3, 8388607;
	or.b32  	%r5, %r4, 1065353216;
	mov.b32 	 %f54, %r5;
	shr.u32 	%r6, %r3, 23;
	cvt.rn.f32.u32	%f55, %r6;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB30_5:
	add.f32 	%f76, %f1, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f1;
	add.rn.f32 	%f80, %f78, %f2;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f268, %f1, %f81, %p8;
	bra.uni 	BB30_15;

BB30_6:
	add.f32 	%f84, %f1, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB30_15;

BB30_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB30_9;

	add.f32 	%f100, %f1, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB30_15;

BB30_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB30_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f1;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB30_15;

BB30_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f1, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f1, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f1, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f1, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f1, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f1, %f154;
	mul.f32 	%f156, %f155, %f1;
	fma.rn.f32 	%f10, %f156, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB30_13;

	lg2.approx.f32 	%f262, %f10;
	bra.uni 	BB30_14;

BB30_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f159, %f10, 0f4B800000;
	selp.f32	%f160, %f159, %f10, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r7, %f160;
	and.b32  	%r8, %r7, 8388607;
	or.b32  	%r9, %r8, 1065353216;
	mov.b32 	 %f162, %r9;
	shr.u32 	%r10, %r7, 23;
	cvt.rn.f32.u32	%f163, %r10;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB30_14:
	neg.f32 	%f268, %f262;

BB30_15:
	setp.ge.f32	%p16, %f39, 0f00000000;
	@%p16 bra 	BB30_37;

	cvt.rmi.f32.f32	%f184, %f1;
	setp.neu.f32	%p17, %f1, %f184;
	@%p17 bra 	BB30_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB30_37;

BB30_18:
	setp.lt.f32	%p18, %f1, 0f1FEC1E4A;
	@%p18 bra 	BB30_33;

	add.f32 	%f185, %f1, %f1;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f1;
	mul.f32 	%f16, %f189, 0f40490FDB;
	mul.rn.f32 	%f17, %f16, %f16;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB30_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f17, %f190;
	bra.uni 	BB30_22;

BB30_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f17, %f192;

BB30_22:
	@%p19 bra 	BB30_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f17, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f17, %f196;
	bra.uni 	BB30_25;

BB30_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f17, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f17, %f199;

BB30_25:
	fma.rn.f32 	%f265, %f264, %f16, %f16;
	@%p19 bra 	BB30_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f17, %f200;

BB30_27:
	and.b32  	%r11, %r1, 2;
	setp.eq.s32	%p22, %r11, 0;
	@%p22 bra 	BB30_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB30_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f29, %f203, %f1;
	setp.gt.f32	%p23, %f29, 0f00000000;
	setp.lt.f32	%p24, %f29, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB30_31;

	lg2.approx.f32 	%f266, %f29;
	bra.uni 	BB30_32;

BB30_31:
	setp.lt.f32	%p26, %f29, 0f00800000;
	mul.f32 	%f206, %f29, 0f4B800000;
	selp.f32	%f207, %f206, %f29, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r12, %f207;
	and.b32  	%r13, %r12, 8388607;
	or.b32  	%r14, %r13, 1065353216;
	mov.b32 	 %f209, %r14;
	shr.u32 	%r15, %r12, 23;
	cvt.rn.f32.u32	%f210, %r15;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB30_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB30_37;

BB30_33:
	setp.gt.f32	%p28, %f1, 0f00000000;
	setp.lt.f32	%p29, %f1, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB30_35;

	lg2.approx.f32 	%f267, %f1;
	bra.uni 	BB30_36;

BB30_35:
	setp.lt.f32	%p31, %f1, 0f00800000;
	mul.f32 	%f235, %f1, 0f4B800000;
	selp.f32	%f236, %f235, %f1, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r16, %f236;
	and.b32  	%r17, %r16, 8388607;
	or.b32  	%r18, %r17, 1065353216;
	mov.b32 	 %f238, %r18;
	shr.u32 	%r19, %r16, 23;
	cvt.rn.f32.u32	%f239, %r19;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB30_36:
	neg.f32 	%f268, %f267;

BB30_37:
	st.param.f32	[funj_retval0+0], %f268;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___lgammad(
	.param .b64 ___lgammad_param_0
)
{
	.reg .pred 	%p<16>;
	.reg .s32 	%r<34>;
	.reg .s64 	%rd<5>;
	.reg .f64 	%fd<105>;


	ld.param.f64 	%fd22, [___lgammad_param_0];
	abs.f64 	%fd101, %fd22;
	setp.gtu.f64	%p1, %fd101, 0d7FF0000000000000;
	@%p1 bra 	BB31_23;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd101;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_lgamma_pos, 
	(
	param0
	);
	ld.param.f64	%fd104, [retval0+0];
	}
	// Callseq End 1
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd22;
	}
	setp.gt.s32	%p2, %r13, -1;
	@%p2 bra 	BB31_24;

	cvt.rzi.f64.f64	%fd23, %fd101;
	setp.neu.f64	%p3, %fd101, %fd23;
	@%p3 bra 	BB31_4;

	mov.f64 	%fd104, 0d7FF0000000000000;
	bra.uni 	BB31_24;

BB31_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd101;
	}
	setp.lt.s32	%p4, %r1, 1006632960;
	@%p4 bra 	BB31_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd101;
	}
	add.s32 	%r15, %r1, 1048576;
	mov.b64 	%fd24, {%r14, %r15};
	cvt.rni.f64.f64	%fd25, %fd24;
	cvt.rzi.s64.f64	%rd1, %fd25;
	cvt.u32.u64	%r2, %rd1;
	neg.f64 	%fd26, %fd25;
	mov.f64 	%fd27, 0d3FE0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd101;
	mul.f64 	%fd29, %fd28, 0d3CA1A62633145C07;
	mov.f64 	%fd30, 0d400921FB54442D18;
	fma.rn.f64 	%fd31, %fd28, %fd30, %fd29;
	and.b32  	%r16, %r2, 1;
	shl.b32 	%r17, %r16, 3;
	mul.rn.f64 	%fd3, %fd31, %fd31;
	setp.eq.s32	%p5, %r16, 0;
	selp.f64	%fd32, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	mul.wide.u32 	%rd2, %r17, 8;
	mov.u64 	%rd3, __cudart_sin_cos_coeffs;
	add.s64 	%rd4, %rd2, %rd3;
	ld.const.f64 	%fd33, [%rd4+8];
	fma.rn.f64 	%fd34, %fd32, %fd3, %fd33;
	ld.const.f64 	%fd35, [%rd4+16];
	fma.rn.f64 	%fd36, %fd34, %fd3, %fd35;
	ld.const.f64 	%fd37, [%rd4+24];
	fma.rn.f64 	%fd38, %fd36, %fd3, %fd37;
	ld.const.f64 	%fd39, [%rd4+32];
	fma.rn.f64 	%fd40, %fd38, %fd3, %fd39;
	ld.const.f64 	%fd41, [%rd4+40];
	fma.rn.f64 	%fd42, %fd40, %fd3, %fd41;
	ld.const.f64 	%fd43, [%rd4+48];
	fma.rn.f64 	%fd4, %fd42, %fd3, %fd43;
	fma.rn.f64 	%fd100, %fd4, %fd31, %fd31;
	@%p5 bra 	BB31_7;

	mov.f64 	%fd44, 0d3FF0000000000000;
	fma.rn.f64 	%fd100, %fd4, %fd3, %fd44;

BB31_7:
	and.b32  	%r18, %r2, 2;
	setp.eq.s32	%p6, %r18, 0;
	@%p6 bra 	BB31_9;

	mov.f64 	%fd45, 0d0000000000000000;
	mov.f64 	%fd46, 0dBFF0000000000000;
	fma.rn.f64 	%fd100, %fd100, %fd46, %fd45;

BB31_9:
	abs.f64 	%fd47, %fd100;
	mul.f64 	%fd48, %fd47, %fd101;
	div.rn.f64 	%fd101, %fd30, %fd48;

BB31_10:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd101;
	}
	setp.lt.s32	%p7, %r30, 2146435072;
	setp.gt.f64	%p8, %fd101, 0d0000000000000000;
	and.pred  	%p9, %p8, %p7;
	@%p9 bra 	BB31_16;

	abs.f64 	%fd50, %fd101;
	setp.gtu.f64	%p10, %fd50, 0d7FF0000000000000;
	@%p10 bra 	BB31_15;

	setp.neu.f64	%p11, %fd101, 0d0000000000000000;
	@%p11 bra 	BB31_14;

	mov.f64 	%fd103, 0dFFF0000000000000;
	bra.uni 	BB31_22;

BB31_14:
	setp.eq.f64	%p12, %fd101, 0d7FF0000000000000;
	selp.f64	%fd103, %fd101, 0dFFF8000000000000, %p12;
	bra.uni 	BB31_22;

BB31_15:
	add.f64 	%fd103, %fd101, %fd101;
	bra.uni 	BB31_22;

BB31_16:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd101;
	}
	setp.lt.s32	%p13, %r30, 1048576;
	@%p13 bra 	BB31_18;

	mov.u32 	%r32, -1023;
	bra.uni 	BB31_19;

BB31_18:
	mul.f64 	%fd52, %fd101, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd52;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd52;
	}
	mov.u32 	%r32, -1077;

BB31_19:
	shr.u32 	%r21, %r30, 20;
	add.s32 	%r33, %r32, %r21;
	and.b32  	%r22, %r30, -2146435073;
	or.b32  	%r23, %r22, 1072693248;
	mov.b64 	%fd102, {%r31, %r23};
	setp.lt.s32	%p14, %r23, 1073127583;
	@%p14 bra 	BB31_21;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd102;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd102;
	}
	add.s32 	%r26, %r25, -1048576;
	mov.b64 	%fd102, {%r24, %r26};
	add.s32 	%r33, %r33, 1;

BB31_21:
	add.f64 	%fd54, %fd102, 0d3FF0000000000000;
	mov.f64 	%fd55, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd53,%fd54;
	// inline asm
	neg.f64 	%fd56, %fd54;
	fma.rn.f64 	%fd57, %fd56, %fd53, %fd55;
	fma.rn.f64 	%fd58, %fd57, %fd57, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd53, %fd53;
	add.f64 	%fd60, %fd102, 0dBFF0000000000000;
	mul.f64 	%fd61, %fd60, %fd59;
	fma.rn.f64 	%fd62, %fd60, %fd59, %fd61;
	mul.f64 	%fd63, %fd62, %fd62;
	mov.f64 	%fd64, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd65, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0d3F624924923BE72D;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0d3FB5555555555554;
	fma.rn.f64 	%fd78, %fd76, %fd63, %fd77;
	sub.f64 	%fd79, %fd60, %fd62;
	add.f64 	%fd80, %fd79, %fd79;
	neg.f64 	%fd81, %fd62;
	fma.rn.f64 	%fd82, %fd81, %fd60, %fd80;
	mul.f64 	%fd83, %fd59, %fd82;
	mul.f64 	%fd84, %fd78, %fd63;
	fma.rn.f64 	%fd85, %fd84, %fd62, %fd83;
	xor.b32  	%r27, %r33, -2147483648;
	mov.u32 	%r28, -2147483648;
	mov.u32 	%r29, 1127219200;
	mov.b64 	%fd86, {%r27, %r29};
	mov.b64 	%fd87, {%r28, %r29};
	sub.f64 	%fd88, %fd86, %fd87;
	mov.f64 	%fd89, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd90, %fd88, %fd89, %fd62;
	neg.f64 	%fd91, %fd88;
	fma.rn.f64 	%fd92, %fd91, %fd89, %fd90;
	sub.f64 	%fd93, %fd92, %fd62;
	sub.f64 	%fd94, %fd85, %fd93;
	mov.f64 	%fd95, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd96, %fd88, %fd95, %fd94;
	add.f64 	%fd103, %fd90, %fd96;

BB31_22:
	sub.f64 	%fd97, %fd103, %fd104;
	neg.f64 	%fd98, %fd103;
	selp.f64	%fd104, %fd98, %fd97, %p4;
	bra.uni 	BB31_24;

BB31_23:
	add.f64 	%fd104, %fd22, %fd22;

BB31_24:
	st.param.f64	[funj_retval0+0], %fd104;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammai(
	.param .b32 ___lgammai_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.u32 	%r3, [___lgammai_param_0];
	cvt.rn.f32.s32	%f1, %r3;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p1, %f2, 0f40400000;
	@%p1 bra 	BB32_7;

	setp.ltu.f32	%p2, %f2, 0f40F9999A;
	@%p2 bra 	BB32_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f2;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f3, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f2, 0f7F800000;
	setp.gt.f32	%p4, %f2, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB32_4;

	lg2.approx.f32 	%f261, %f2;
	bra.uni 	BB32_5;

BB32_4:
	setp.lt.f32	%p6, %f2, 0f00800000;
	mul.f32 	%f51, %f2, 0f4B800000;
	selp.f32	%f52, %f51, %f2, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r4, %f52;
	and.b32  	%r5, %r4, 8388607;
	or.b32  	%r6, %r5, 1065353216;
	mov.b32 	 %f54, %r6;
	shr.u32 	%r7, %r4, 23;
	cvt.rn.f32.u32	%f55, %r7;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB32_5:
	add.f32 	%f76, %f2, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f2;
	add.rn.f32 	%f80, %f78, %f3;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f2, 0f7F800000;
	selp.f32	%f268, %f2, %f81, %p8;
	bra.uni 	BB32_15;

BB32_6:
	add.f32 	%f84, %f2, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB32_15;

BB32_7:
	setp.ltu.f32	%p9, %f2, 0f3FC00000;
	@%p9 bra 	BB32_9;

	add.f32 	%f100, %f2, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB32_15;

BB32_9:
	setp.ltu.f32	%p10, %f2, 0f3F333333;
	@%p10 bra 	BB32_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f2;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB32_15;

BB32_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f2, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f2, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f2, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f2, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f2, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f2, %f154;
	mul.f32 	%f156, %f155, %f2;
	fma.rn.f32 	%f11, %f156, %f2, %f2;
	setp.gt.f32	%p11, %f11, 0f00000000;
	setp.lt.f32	%p12, %f11, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB32_13;

	lg2.approx.f32 	%f262, %f11;
	bra.uni 	BB32_14;

BB32_13:
	setp.lt.f32	%p14, %f11, 0f00800000;
	mul.f32 	%f159, %f11, 0f4B800000;
	selp.f32	%f160, %f159, %f11, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r8, %f160;
	and.b32  	%r9, %r8, 8388607;
	or.b32  	%r10, %r9, 1065353216;
	mov.b32 	 %f162, %r10;
	shr.u32 	%r11, %r8, 23;
	cvt.rn.f32.u32	%f163, %r11;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB32_14:
	neg.f32 	%f268, %f262;

BB32_15:
	setp.ge.f32	%p16, %f1, 0f00000000;
	@%p16 bra 	BB32_37;

	cvt.rmi.f32.f32	%f184, %f2;
	setp.neu.f32	%p17, %f2, %f184;
	@%p17 bra 	BB32_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB32_37;

BB32_18:
	setp.lt.f32	%p18, %f2, 0f1FEC1E4A;
	@%p18 bra 	BB32_33;

	add.f32 	%f185, %f2, %f2;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f2;
	mul.f32 	%f17, %f189, 0f40490FDB;
	mul.rn.f32 	%f18, %f17, %f17;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB32_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f18, %f190;
	bra.uni 	BB32_22;

BB32_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f18, %f192;

BB32_22:
	@%p19 bra 	BB32_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f18, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f18, %f196;
	bra.uni 	BB32_25;

BB32_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f18, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f18, %f199;

BB32_25:
	fma.rn.f32 	%f265, %f264, %f17, %f17;
	@%p19 bra 	BB32_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f18, %f200;

BB32_27:
	and.b32  	%r12, %r1, 2;
	setp.eq.s32	%p22, %r12, 0;
	@%p22 bra 	BB32_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB32_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f30, %f203, %f2;
	setp.gt.f32	%p23, %f30, 0f00000000;
	setp.lt.f32	%p24, %f30, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB32_31;

	lg2.approx.f32 	%f266, %f30;
	bra.uni 	BB32_32;

BB32_31:
	setp.lt.f32	%p26, %f30, 0f00800000;
	mul.f32 	%f206, %f30, 0f4B800000;
	selp.f32	%f207, %f206, %f30, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r13, %f207;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	 %f209, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32	%f210, %r16;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB32_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB32_37;

BB32_33:
	setp.gt.f32	%p28, %f2, 0f00000000;
	setp.lt.f32	%p29, %f2, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB32_35;

	lg2.approx.f32 	%f267, %f2;
	bra.uni 	BB32_36;

BB32_35:
	setp.lt.f32	%p31, %f2, 0f00800000;
	mul.f32 	%f235, %f2, 0f4B800000;
	selp.f32	%f236, %f235, %f2, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r17, %f236;
	and.b32  	%r18, %r17, 8388607;
	or.b32  	%r19, %r18, 1065353216;
	mov.b32 	 %f238, %r19;
	shr.u32 	%r20, %r17, 23;
	cvt.rn.f32.u32	%f239, %r20;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB32_36:
	neg.f32 	%f268, %f267;

BB32_37:
	cvt.rzi.s32.f32	%r21, %f268;
	st.param.b32	[funj_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammau(
	.param .b32 ___lgammau_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.u32 	%r3, [___lgammau_param_0];
	cvt.rn.f32.u32	%f1, %r3;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p1, %f2, 0f40400000;
	@%p1 bra 	BB33_7;

	setp.ltu.f32	%p2, %f2, 0f40F9999A;
	@%p2 bra 	BB33_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f2;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f3, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f2, 0f7F800000;
	setp.gt.f32	%p4, %f2, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB33_4;

	lg2.approx.f32 	%f261, %f2;
	bra.uni 	BB33_5;

BB33_4:
	setp.lt.f32	%p6, %f2, 0f00800000;
	mul.f32 	%f51, %f2, 0f4B800000;
	selp.f32	%f52, %f51, %f2, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r4, %f52;
	and.b32  	%r5, %r4, 8388607;
	or.b32  	%r6, %r5, 1065353216;
	mov.b32 	 %f54, %r6;
	shr.u32 	%r7, %r4, 23;
	cvt.rn.f32.u32	%f55, %r7;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB33_5:
	add.f32 	%f76, %f2, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f2;
	add.rn.f32 	%f80, %f78, %f3;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f2, 0f7F800000;
	selp.f32	%f268, %f2, %f81, %p8;
	bra.uni 	BB33_15;

BB33_6:
	add.f32 	%f84, %f2, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB33_15;

BB33_7:
	setp.ltu.f32	%p9, %f2, 0f3FC00000;
	@%p9 bra 	BB33_9;

	add.f32 	%f100, %f2, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB33_15;

BB33_9:
	setp.ltu.f32	%p10, %f2, 0f3F333333;
	@%p10 bra 	BB33_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f2;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB33_15;

BB33_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f2, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f2, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f2, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f2, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f2, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f2, %f154;
	mul.f32 	%f156, %f155, %f2;
	fma.rn.f32 	%f11, %f156, %f2, %f2;
	setp.gt.f32	%p11, %f11, 0f00000000;
	setp.lt.f32	%p12, %f11, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB33_13;

	lg2.approx.f32 	%f262, %f11;
	bra.uni 	BB33_14;

BB33_13:
	setp.lt.f32	%p14, %f11, 0f00800000;
	mul.f32 	%f159, %f11, 0f4B800000;
	selp.f32	%f160, %f159, %f11, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r8, %f160;
	and.b32  	%r9, %r8, 8388607;
	or.b32  	%r10, %r9, 1065353216;
	mov.b32 	 %f162, %r10;
	shr.u32 	%r11, %r8, 23;
	cvt.rn.f32.u32	%f163, %r11;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB33_14:
	neg.f32 	%f268, %f262;

BB33_15:
	setp.ge.f32	%p16, %f1, 0f00000000;
	@%p16 bra 	BB33_37;

	cvt.rmi.f32.f32	%f184, %f2;
	setp.neu.f32	%p17, %f2, %f184;
	@%p17 bra 	BB33_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB33_37;

BB33_18:
	setp.lt.f32	%p18, %f2, 0f1FEC1E4A;
	@%p18 bra 	BB33_33;

	add.f32 	%f185, %f2, %f2;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f2;
	mul.f32 	%f17, %f189, 0f40490FDB;
	mul.rn.f32 	%f18, %f17, %f17;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB33_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f18, %f190;
	bra.uni 	BB33_22;

BB33_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f18, %f192;

BB33_22:
	@%p19 bra 	BB33_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f18, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f18, %f196;
	bra.uni 	BB33_25;

BB33_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f18, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f18, %f199;

BB33_25:
	fma.rn.f32 	%f265, %f264, %f17, %f17;
	@%p19 bra 	BB33_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f18, %f200;

BB33_27:
	and.b32  	%r12, %r1, 2;
	setp.eq.s32	%p22, %r12, 0;
	@%p22 bra 	BB33_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB33_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f30, %f203, %f2;
	setp.gt.f32	%p23, %f30, 0f00000000;
	setp.lt.f32	%p24, %f30, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB33_31;

	lg2.approx.f32 	%f266, %f30;
	bra.uni 	BB33_32;

BB33_31:
	setp.lt.f32	%p26, %f30, 0f00800000;
	mul.f32 	%f206, %f30, 0f4B800000;
	selp.f32	%f207, %f206, %f30, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r13, %f207;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	 %f209, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32	%f210, %r16;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB33_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB33_37;

BB33_33:
	setp.gt.f32	%p28, %f2, 0f00000000;
	setp.lt.f32	%p29, %f2, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB33_35;

	lg2.approx.f32 	%f267, %f2;
	bra.uni 	BB33_36;

BB33_35:
	setp.lt.f32	%p31, %f2, 0f00800000;
	mul.f32 	%f235, %f2, 0f4B800000;
	selp.f32	%f236, %f235, %f2, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r17, %f236;
	and.b32  	%r18, %r17, 8388607;
	or.b32  	%r19, %r18, 1065353216;
	mov.b32 	 %f238, %r19;
	shr.u32 	%r20, %r17, 23;
	cvt.rn.f32.u32	%f239, %r20;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB33_36:
	neg.f32 	%f268, %f267;

BB33_37:
	cvt.rzi.u32.f32	%r21, %f268;
	st.param.b32	[funj_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammaj(
	.param .b32 ___lgammaj_param_0
)
{
	.reg .pred 	%p<33>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<22>;
	.reg .f32 	%f<269>;


	ld.param.s8 	%rs1, [___lgammaj_param_0];
	cvt.rn.f32.s16	%f39, %rs1;
	abs.f32 	%f1, %f39;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB34_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB34_6;

	// inline asm
	rcp.approx.ftz.f32 %f40,%f1;
	// inline asm
	mul.f32 	%f42, %f40, %f40;
	mov.f32 	%f43, 0fBB360953;
	mov.f32 	%f44, 0f3A4BE755;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0f3DAAAAA3;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f47, %f40, %f48;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB34_4;

	lg2.approx.f32 	%f261, %f1;
	bra.uni 	BB34_5;

BB34_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f51, %f1, 0f4B800000;
	selp.f32	%f52, %f51, %f1, %p6;
	selp.f32	%f53, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r3, %f52;
	and.b32  	%r4, %r3, 8388607;
	or.b32  	%r5, %r4, 1065353216;
	mov.b32 	 %f54, %r5;
	shr.u32 	%r6, %r3, 23;
	cvt.rn.f32.u32	%f55, %r6;
	add.f32 	%f56, %f53, %f55;
	setp.gt.f32	%p7, %f54, 0f3FAE147B;
	mul.f32 	%f57, %f54, 0f3F000000;
	add.f32 	%f58, %f56, 0f3F800000;
	selp.f32	%f59, %f57, %f54, %p7;
	selp.f32	%f60, %f58, %f56, %p7;
	add.f32 	%f50, %f59, 0f3F800000;
	add.f32 	%f61, %f59, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f49,%f50;
	// inline asm
	neg.f32 	%f62, %f61;
	mul.f32 	%f63, %f61, %f62;
	mul.rn.f32 	%f64, %f49, %f63;
	add.rn.f32 	%f65, %f61, %f64;
	mul.f32 	%f66, %f65, %f65;
	mov.f32 	%f67, 0f3C4C6A36;
	mov.f32 	%f68, 0f3B1E94E6;
	fma.rn.f32 	%f69, %f68, %f66, %f67;
	mov.f32 	%f70, 0f3DAAAB1A;
	fma.rn.f32 	%f71, %f69, %f66, %f70;
	mul.f32 	%f72, %f71, %f66;
	fma.rn.f32 	%f73, %f72, %f65, %f64;
	add.f32 	%f74, %f73, %f61;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f261, %f60, %f75, %f74;

BB34_5:
	add.f32 	%f76, %f1, 0fBF000000;
	mul.f32 	%f77, %f261, 0f3F000000;
	mul.rn.f32 	%f78, %f77, %f76;
	sub.f32 	%f79, %f78, %f1;
	add.rn.f32 	%f80, %f78, %f2;
	add.f32 	%f81, %f79, %f80;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f268, %f1, %f81, %p8;
	bra.uni 	BB34_15;

BB34_6:
	add.f32 	%f84, %f1, 0fC0400000;
	mov.f32 	%f85, 0fC640F6F8;
	mov.f32 	%f86, 0fC43B38FB;
	fma.rn.f32 	%f87, %f86, %f84, %f85;
	mov.f32 	%f88, 0fC7206560;
	fma.rn.f32 	%f89, %f87, %f84, %f88;
	mov.f32 	%f90, 0fC73CB6AA;
	fma.rn.f32 	%f91, %f89, %f84, %f90;
	mov.f32 	%f92, 0fC80BAE5A;
	fma.rn.f32 	%f93, %f91, %f84, %f92;
	add.f32 	%f94, %f84, 0fC381A020;
	mov.f32 	%f95, 0fC62864B8;
	fma.rn.f32 	%f96, %f94, %f84, %f95;
	mov.f32 	%f97, 0fC7B50686;
	fma.rn.f32 	%f98, %f96, %f84, %f97;
	mov.f32 	%f99, 0fC8498465;
	fma.rn.f32 	%f83, %f98, %f84, %f99;
	// inline asm
	rcp.approx.ftz.f32 %f82,%f83;
	// inline asm
	fma.rn.f32 	%f268, %f93, %f82, %f84;
	bra.uni 	BB34_15;

BB34_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB34_9;

	add.f32 	%f100, %f1, 0fC0000000;
	mov.f32 	%f101, 0fB967A002;
	mov.f32 	%f102, 0f385007FA;
	fma.rn.f32 	%f103, %f102, %f100, %f101;
	mov.f32 	%f104, 0f3A0DE6FC;
	fma.rn.f32 	%f105, %f103, %f100, %f104;
	mov.f32 	%f106, 0fBA9DE0E2;
	fma.rn.f32 	%f107, %f105, %f100, %f106;
	mov.f32 	%f108, 0f3B3D05B7;
	fma.rn.f32 	%f109, %f107, %f100, %f108;
	mov.f32 	%f110, 0fBBF1EB10;
	fma.rn.f32 	%f111, %f109, %f100, %f110;
	mov.f32 	%f112, 0f3CA89A28;
	fma.rn.f32 	%f113, %f111, %f100, %f112;
	mov.f32 	%f114, 0fBD89F01A;
	fma.rn.f32 	%f115, %f113, %f100, %f114;
	mov.f32 	%f116, 0f3EA51A66;
	fma.rn.f32 	%f117, %f115, %f100, %f116;
	mov.f32 	%f118, 0f3ED87730;
	fma.rn.f32 	%f119, %f117, %f100, %f118;
	mul.f32 	%f268, %f119, %f100;
	bra.uni 	BB34_15;

BB34_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB34_11;

	mov.f32 	%f120, 0f3F800000;
	sub.f32 	%f121, %f120, %f1;
	mov.f32 	%f122, 0f3DD47577;
	mov.f32 	%f123, 0f3D3BEF76;
	fma.rn.f32 	%f124, %f123, %f121, %f122;
	mov.f32 	%f125, 0f3DFB8079;
	fma.rn.f32 	%f126, %f124, %f121, %f125;
	mov.f32 	%f127, 0f3E0295B5;
	fma.rn.f32 	%f128, %f126, %f121, %f127;
	mov.f32 	%f129, 0f3E12A765;
	fma.rn.f32 	%f130, %f128, %f121, %f129;
	mov.f32 	%f131, 0f3E2D6867;
	fma.rn.f32 	%f132, %f130, %f121, %f131;
	mov.f32 	%f133, 0f3E5462BF;
	fma.rn.f32 	%f134, %f132, %f121, %f133;
	mov.f32 	%f135, 0f3E8A8A72;
	fma.rn.f32 	%f136, %f134, %f121, %f135;
	mov.f32 	%f137, 0f3ECD26A4;
	fma.rn.f32 	%f138, %f136, %f121, %f137;
	mov.f32 	%f139, 0f3F528D32;
	fma.rn.f32 	%f140, %f138, %f121, %f139;
	mov.f32 	%f141, 0f3F13C468;
	fma.rn.f32 	%f142, %f140, %f121, %f141;
	mul.f32 	%f268, %f142, %f121;
	bra.uni 	BB34_15;

BB34_11:
	mov.f32 	%f143, 0fBBB34878;
	mov.f32 	%f144, 0f3B6B1C86;
	fma.rn.f32 	%f145, %f144, %f1, %f143;
	mov.f32 	%f146, 0fBD36CAEF;
	fma.rn.f32 	%f147, %f145, %f1, %f146;
	mov.f32 	%f148, 0f3E2B5555;
	fma.rn.f32 	%f149, %f147, %f1, %f148;
	mov.f32 	%f150, 0fBD2C96C7;
	fma.rn.f32 	%f151, %f149, %f1, %f150;
	mov.f32 	%f152, 0fBF27E6EB;
	fma.rn.f32 	%f153, %f151, %f1, %f152;
	mov.f32 	%f154, 0f3F13C463;
	fma.rn.f32 	%f155, %f153, %f1, %f154;
	mul.f32 	%f156, %f155, %f1;
	fma.rn.f32 	%f10, %f156, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB34_13;

	lg2.approx.f32 	%f262, %f10;
	bra.uni 	BB34_14;

BB34_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f159, %f10, 0f4B800000;
	selp.f32	%f160, %f159, %f10, %p14;
	selp.f32	%f161, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r7, %f160;
	and.b32  	%r8, %r7, 8388607;
	or.b32  	%r9, %r8, 1065353216;
	mov.b32 	 %f162, %r9;
	shr.u32 	%r10, %r7, 23;
	cvt.rn.f32.u32	%f163, %r10;
	add.f32 	%f164, %f161, %f163;
	setp.gt.f32	%p15, %f162, 0f3FAE147B;
	mul.f32 	%f165, %f162, 0f3F000000;
	add.f32 	%f166, %f164, 0f3F800000;
	selp.f32	%f167, %f165, %f162, %p15;
	selp.f32	%f168, %f166, %f164, %p15;
	add.f32 	%f158, %f167, 0f3F800000;
	add.f32 	%f169, %f167, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f157,%f158;
	// inline asm
	neg.f32 	%f170, %f169;
	mul.f32 	%f171, %f169, %f170;
	mul.rn.f32 	%f172, %f157, %f171;
	add.rn.f32 	%f173, %f169, %f172;
	mul.f32 	%f174, %f173, %f173;
	mov.f32 	%f175, 0f3C4C6A36;
	mov.f32 	%f176, 0f3B1E94E6;
	fma.rn.f32 	%f177, %f176, %f174, %f175;
	mov.f32 	%f178, 0f3DAAAB1A;
	fma.rn.f32 	%f179, %f177, %f174, %f178;
	mul.f32 	%f180, %f179, %f174;
	fma.rn.f32 	%f181, %f180, %f173, %f172;
	add.f32 	%f182, %f181, %f169;
	mov.f32 	%f183, 0f3F317218;
	fma.rn.f32 	%f262, %f168, %f183, %f182;

BB34_14:
	neg.f32 	%f268, %f262;

BB34_15:
	setp.gt.s16	%p16, %rs1, -1;
	@%p16 bra 	BB34_37;

	cvt.rmi.f32.f32	%f184, %f1;
	setp.neu.f32	%p17, %f1, %f184;
	@%p17 bra 	BB34_18;

	mov.f32 	%f268, 0f7F800000;
	bra.uni 	BB34_37;

BB34_18:
	setp.lt.f32	%p18, %f1, 0f1FEC1E4A;
	@%p18 bra 	BB34_33;

	add.f32 	%f185, %f1, %f1;
	cvt.rni.f32.f32	%f186, %f185;
	cvt.rzi.s32.f32	%r1, %f186;
	neg.f32 	%f187, %f186;
	mov.f32 	%f188, 0f3F000000;
	fma.rn.f32 	%f189, %f187, %f188, %f1;
	mul.f32 	%f16, %f189, 0f40490FDB;
	mul.rn.f32 	%f17, %f16, %f16;
	and.b32  	%r2, %r1, 1;
	setp.eq.s32	%p19, %r2, 0;
	@%p19 bra 	BB34_21;

	mov.f32 	%f190, 0fBAB6061A;
	mov.f32 	%f191, 0f37CCF5CE;
	fma.rn.f32 	%f263, %f191, %f17, %f190;
	bra.uni 	BB34_22;

BB34_21:
	mov.f32 	%f192, 0f3C08839E;
	mov.f32 	%f193, 0fB94CA1F9;
	fma.rn.f32 	%f263, %f193, %f17, %f192;

BB34_22:
	@%p19 bra 	BB34_24;

	mov.f32 	%f194, 0f3D2AAAA5;
	fma.rn.f32 	%f195, %f263, %f17, %f194;
	mov.f32 	%f196, 0fBF000000;
	fma.rn.f32 	%f264, %f195, %f17, %f196;
	bra.uni 	BB34_25;

BB34_24:
	mov.f32 	%f197, 0fBE2AAAA3;
	fma.rn.f32 	%f198, %f263, %f17, %f197;
	mov.f32 	%f199, 0f00000000;
	fma.rn.f32 	%f264, %f198, %f17, %f199;

BB34_25:
	fma.rn.f32 	%f265, %f264, %f16, %f16;
	@%p19 bra 	BB34_27;

	mov.f32 	%f200, 0f3F800000;
	fma.rn.f32 	%f265, %f264, %f17, %f200;

BB34_27:
	and.b32  	%r11, %r1, 2;
	setp.eq.s32	%p22, %r11, 0;
	@%p22 bra 	BB34_29;

	mov.f32 	%f201, 0f00000000;
	mov.f32 	%f202, 0fBF800000;
	fma.rn.f32 	%f265, %f265, %f202, %f201;

BB34_29:
	abs.f32 	%f203, %f265;
	mul.f32 	%f29, %f203, %f1;
	setp.gt.f32	%p23, %f29, 0f00000000;
	setp.lt.f32	%p24, %f29, 0f7F800000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB34_31;

	lg2.approx.f32 	%f266, %f29;
	bra.uni 	BB34_32;

BB34_31:
	setp.lt.f32	%p26, %f29, 0f00800000;
	mul.f32 	%f206, %f29, 0f4B800000;
	selp.f32	%f207, %f206, %f29, %p26;
	selp.f32	%f208, 0fC3170000, 0fC2FE0000, %p26;
	mov.b32 	 %r12, %f207;
	and.b32  	%r13, %r12, 8388607;
	or.b32  	%r14, %r13, 1065353216;
	mov.b32 	 %f209, %r14;
	shr.u32 	%r15, %r12, 23;
	cvt.rn.f32.u32	%f210, %r15;
	add.f32 	%f211, %f208, %f210;
	setp.gt.f32	%p27, %f209, 0f3FAE147B;
	mul.f32 	%f212, %f209, 0f3F000000;
	add.f32 	%f213, %f211, 0f3F800000;
	selp.f32	%f214, %f212, %f209, %p27;
	selp.f32	%f215, %f213, %f211, %p27;
	add.f32 	%f205, %f214, 0f3F800000;
	add.f32 	%f216, %f214, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f204,%f205;
	// inline asm
	neg.f32 	%f217, %f216;
	mul.f32 	%f218, %f216, %f217;
	mul.rn.f32 	%f219, %f204, %f218;
	add.rn.f32 	%f220, %f216, %f219;
	mul.f32 	%f221, %f220, %f220;
	mov.f32 	%f222, 0f3C4C6A36;
	mov.f32 	%f223, 0f3B1E94E6;
	fma.rn.f32 	%f224, %f223, %f221, %f222;
	mov.f32 	%f225, 0f3DAAAB1A;
	fma.rn.f32 	%f226, %f224, %f221, %f225;
	mul.f32 	%f227, %f226, %f221;
	fma.rn.f32 	%f228, %f227, %f220, %f219;
	add.f32 	%f229, %f228, %f216;
	mov.f32 	%f230, 0f3F317218;
	fma.rn.f32 	%f266, %f215, %f230, %f229;

BB34_32:
	mov.f32 	%f231, 0f3F928682;
	sub.f32 	%f232, %f231, %f266;
	sub.f32 	%f268, %f232, %f268;
	bra.uni 	BB34_37;

BB34_33:
	setp.gt.f32	%p28, %f1, 0f00000000;
	setp.lt.f32	%p29, %f1, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB34_35;

	lg2.approx.f32 	%f267, %f1;
	bra.uni 	BB34_36;

BB34_35:
	setp.lt.f32	%p31, %f1, 0f00800000;
	mul.f32 	%f235, %f1, 0f4B800000;
	selp.f32	%f236, %f235, %f1, %p31;
	selp.f32	%f237, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r16, %f236;
	and.b32  	%r17, %r16, 8388607;
	or.b32  	%r18, %r17, 1065353216;
	mov.b32 	 %f238, %r18;
	shr.u32 	%r19, %r16, 23;
	cvt.rn.f32.u32	%f239, %r19;
	add.f32 	%f240, %f237, %f239;
	setp.gt.f32	%p32, %f238, 0f3FAE147B;
	mul.f32 	%f241, %f238, 0f3F000000;
	add.f32 	%f242, %f240, 0f3F800000;
	selp.f32	%f243, %f241, %f238, %p32;
	selp.f32	%f244, %f242, %f240, %p32;
	add.f32 	%f234, %f243, 0f3F800000;
	add.f32 	%f245, %f243, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f233,%f234;
	// inline asm
	neg.f32 	%f246, %f245;
	mul.f32 	%f247, %f245, %f246;
	mul.rn.f32 	%f248, %f233, %f247;
	add.rn.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f249, %f249;
	mov.f32 	%f251, 0f3C4C6A36;
	mov.f32 	%f252, 0f3B1E94E6;
	fma.rn.f32 	%f253, %f252, %f250, %f251;
	mov.f32 	%f254, 0f3DAAAB1A;
	fma.rn.f32 	%f255, %f253, %f250, %f254;
	mul.f32 	%f256, %f255, %f250;
	fma.rn.f32 	%f257, %f256, %f249, %f248;
	add.f32 	%f258, %f257, %f245;
	mov.f32 	%f259, 0f3F317218;
	fma.rn.f32 	%f267, %f244, %f259, %f258;

BB34_36:
	neg.f32 	%f268, %f267;

BB34_37:
	cvt.rzi.s32.f32	%r20, %f268;
	cvt.s32.s8 	%r21, %r20;
	st.param.b32	[funj_retval0+0], %r21;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___lgammav(
	.param .b32 ___lgammav_param_0
)
{
	.reg .pred 	%p<16>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<11>;
	.reg .f32 	%f<164>;


	ld.param.u8 	%rs1, [___lgammav_param_0];
	cvt.rn.f32.u16	%f16, %rs1;
	abs.f32 	%f1, %f16;
	setp.ltu.f32	%p1, %f1, 0f40400000;
	@%p1 bra 	BB35_7;

	setp.ltu.f32	%p2, %f1, 0f40F9999A;
	@%p2 bra 	BB35_6;

	// inline asm
	rcp.approx.ftz.f32 %f17,%f1;
	// inline asm
	mul.f32 	%f19, %f17, %f17;
	mov.f32 	%f20, 0fBB360953;
	mov.f32 	%f21, 0f3A4BE755;
	fma.rn.f32 	%f22, %f21, %f19, %f20;
	mov.f32 	%f23, 0f3DAAAAA3;
	fma.rn.f32 	%f24, %f22, %f19, %f23;
	mov.f32 	%f25, 0f3F6B3F8E;
	fma.rn.f32 	%f2, %f24, %f17, %f25;
	setp.lt.f32	%p3, %f1, 0f7F800000;
	setp.gt.f32	%p4, %f1, 0f00000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB35_4;

	lg2.approx.f32 	%f161, %f1;
	bra.uni 	BB35_5;

BB35_4:
	setp.lt.f32	%p6, %f1, 0f00800000;
	mul.f32 	%f28, %f1, 0f4B800000;
	selp.f32	%f29, %f28, %f1, %p6;
	selp.f32	%f30, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	 %r1, %f29;
	and.b32  	%r2, %r1, 8388607;
	or.b32  	%r3, %r2, 1065353216;
	mov.b32 	 %f31, %r3;
	shr.u32 	%r4, %r1, 23;
	cvt.rn.f32.u32	%f32, %r4;
	add.f32 	%f33, %f30, %f32;
	setp.gt.f32	%p7, %f31, 0f3FAE147B;
	mul.f32 	%f34, %f31, 0f3F000000;
	add.f32 	%f35, %f33, 0f3F800000;
	selp.f32	%f36, %f34, %f31, %p7;
	selp.f32	%f37, %f35, %f33, %p7;
	add.f32 	%f27, %f36, 0f3F800000;
	add.f32 	%f38, %f36, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f26,%f27;
	// inline asm
	neg.f32 	%f39, %f38;
	mul.f32 	%f40, %f38, %f39;
	mul.rn.f32 	%f41, %f26, %f40;
	add.rn.f32 	%f42, %f38, %f41;
	mul.f32 	%f43, %f42, %f42;
	mov.f32 	%f44, 0f3C4C6A36;
	mov.f32 	%f45, 0f3B1E94E6;
	fma.rn.f32 	%f46, %f45, %f43, %f44;
	mov.f32 	%f47, 0f3DAAAB1A;
	fma.rn.f32 	%f48, %f46, %f43, %f47;
	mul.f32 	%f49, %f48, %f43;
	fma.rn.f32 	%f50, %f49, %f42, %f41;
	add.f32 	%f51, %f50, %f38;
	mov.f32 	%f52, 0f3F317218;
	fma.rn.f32 	%f161, %f37, %f52, %f51;

BB35_5:
	add.f32 	%f53, %f1, 0fBF000000;
	mul.f32 	%f54, %f161, 0f3F000000;
	mul.rn.f32 	%f55, %f54, %f53;
	sub.f32 	%f56, %f55, %f1;
	add.rn.f32 	%f57, %f55, %f2;
	add.f32 	%f58, %f56, %f57;
	setp.eq.f32	%p8, %f1, 0f7F800000;
	selp.f32	%f163, %f1, %f58, %p8;
	bra.uni 	BB35_15;

BB35_6:
	add.f32 	%f61, %f1, 0fC0400000;
	mov.f32 	%f62, 0fC640F6F8;
	mov.f32 	%f63, 0fC43B38FB;
	fma.rn.f32 	%f64, %f63, %f61, %f62;
	mov.f32 	%f65, 0fC7206560;
	fma.rn.f32 	%f66, %f64, %f61, %f65;
	mov.f32 	%f67, 0fC73CB6AA;
	fma.rn.f32 	%f68, %f66, %f61, %f67;
	mov.f32 	%f69, 0fC80BAE5A;
	fma.rn.f32 	%f70, %f68, %f61, %f69;
	add.f32 	%f71, %f61, 0fC381A020;
	mov.f32 	%f72, 0fC62864B8;
	fma.rn.f32 	%f73, %f71, %f61, %f72;
	mov.f32 	%f74, 0fC7B50686;
	fma.rn.f32 	%f75, %f73, %f61, %f74;
	mov.f32 	%f76, 0fC8498465;
	fma.rn.f32 	%f60, %f75, %f61, %f76;
	// inline asm
	rcp.approx.ftz.f32 %f59,%f60;
	// inline asm
	fma.rn.f32 	%f163, %f70, %f59, %f61;
	bra.uni 	BB35_15;

BB35_7:
	setp.ltu.f32	%p9, %f1, 0f3FC00000;
	@%p9 bra 	BB35_9;

	add.f32 	%f77, %f1, 0fC0000000;
	mov.f32 	%f78, 0fB967A002;
	mov.f32 	%f79, 0f385007FA;
	fma.rn.f32 	%f80, %f79, %f77, %f78;
	mov.f32 	%f81, 0f3A0DE6FC;
	fma.rn.f32 	%f82, %f80, %f77, %f81;
	mov.f32 	%f83, 0fBA9DE0E2;
	fma.rn.f32 	%f84, %f82, %f77, %f83;
	mov.f32 	%f85, 0f3B3D05B7;
	fma.rn.f32 	%f86, %f84, %f77, %f85;
	mov.f32 	%f87, 0fBBF1EB10;
	fma.rn.f32 	%f88, %f86, %f77, %f87;
	mov.f32 	%f89, 0f3CA89A28;
	fma.rn.f32 	%f90, %f88, %f77, %f89;
	mov.f32 	%f91, 0fBD89F01A;
	fma.rn.f32 	%f92, %f90, %f77, %f91;
	mov.f32 	%f93, 0f3EA51A66;
	fma.rn.f32 	%f94, %f92, %f77, %f93;
	mov.f32 	%f95, 0f3ED87730;
	fma.rn.f32 	%f96, %f94, %f77, %f95;
	mul.f32 	%f163, %f96, %f77;
	bra.uni 	BB35_15;

BB35_9:
	setp.ltu.f32	%p10, %f1, 0f3F333333;
	@%p10 bra 	BB35_11;

	mov.f32 	%f97, 0f3F800000;
	sub.f32 	%f98, %f97, %f1;
	mov.f32 	%f99, 0f3DD47577;
	mov.f32 	%f100, 0f3D3BEF76;
	fma.rn.f32 	%f101, %f100, %f98, %f99;
	mov.f32 	%f102, 0f3DFB8079;
	fma.rn.f32 	%f103, %f101, %f98, %f102;
	mov.f32 	%f104, 0f3E0295B5;
	fma.rn.f32 	%f105, %f103, %f98, %f104;
	mov.f32 	%f106, 0f3E12A765;
	fma.rn.f32 	%f107, %f105, %f98, %f106;
	mov.f32 	%f108, 0f3E2D6867;
	fma.rn.f32 	%f109, %f107, %f98, %f108;
	mov.f32 	%f110, 0f3E5462BF;
	fma.rn.f32 	%f111, %f109, %f98, %f110;
	mov.f32 	%f112, 0f3E8A8A72;
	fma.rn.f32 	%f113, %f111, %f98, %f112;
	mov.f32 	%f114, 0f3ECD26A4;
	fma.rn.f32 	%f115, %f113, %f98, %f114;
	mov.f32 	%f116, 0f3F528D32;
	fma.rn.f32 	%f117, %f115, %f98, %f116;
	mov.f32 	%f118, 0f3F13C468;
	fma.rn.f32 	%f119, %f117, %f98, %f118;
	mul.f32 	%f163, %f119, %f98;
	bra.uni 	BB35_15;

BB35_11:
	mov.f32 	%f120, 0fBBB34878;
	mov.f32 	%f121, 0f3B6B1C86;
	fma.rn.f32 	%f122, %f121, %f1, %f120;
	mov.f32 	%f123, 0fBD36CAEF;
	fma.rn.f32 	%f124, %f122, %f1, %f123;
	mov.f32 	%f125, 0f3E2B5555;
	fma.rn.f32 	%f126, %f124, %f1, %f125;
	mov.f32 	%f127, 0fBD2C96C7;
	fma.rn.f32 	%f128, %f126, %f1, %f127;
	mov.f32 	%f129, 0fBF27E6EB;
	fma.rn.f32 	%f130, %f128, %f1, %f129;
	mov.f32 	%f131, 0f3F13C463;
	fma.rn.f32 	%f132, %f130, %f1, %f131;
	mul.f32 	%f133, %f132, %f1;
	fma.rn.f32 	%f10, %f133, %f1, %f1;
	setp.gt.f32	%p11, %f10, 0f00000000;
	setp.lt.f32	%p12, %f10, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB35_13;

	lg2.approx.f32 	%f162, %f10;
	bra.uni 	BB35_14;

BB35_13:
	setp.lt.f32	%p14, %f10, 0f00800000;
	mul.f32 	%f136, %f10, 0f4B800000;
	selp.f32	%f137, %f136, %f10, %p14;
	selp.f32	%f138, 0fC3170000, 0fC2FE0000, %p14;
	mov.b32 	 %r5, %f137;
	and.b32  	%r6, %r5, 8388607;
	or.b32  	%r7, %r6, 1065353216;
	mov.b32 	 %f139, %r7;
	shr.u32 	%r8, %r5, 23;
	cvt.rn.f32.u32	%f140, %r8;
	add.f32 	%f141, %f138, %f140;
	setp.gt.f32	%p15, %f139, 0f3FAE147B;
	mul.f32 	%f142, %f139, 0f3F000000;
	add.f32 	%f143, %f141, 0f3F800000;
	selp.f32	%f144, %f142, %f139, %p15;
	selp.f32	%f145, %f143, %f141, %p15;
	add.f32 	%f135, %f144, 0f3F800000;
	add.f32 	%f146, %f144, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f134,%f135;
	// inline asm
	neg.f32 	%f147, %f146;
	mul.f32 	%f148, %f146, %f147;
	mul.rn.f32 	%f149, %f134, %f148;
	add.rn.f32 	%f150, %f146, %f149;
	mul.f32 	%f151, %f150, %f150;
	mov.f32 	%f152, 0f3C4C6A36;
	mov.f32 	%f153, 0f3B1E94E6;
	fma.rn.f32 	%f154, %f153, %f151, %f152;
	mov.f32 	%f155, 0f3DAAAB1A;
	fma.rn.f32 	%f156, %f154, %f151, %f155;
	mul.f32 	%f157, %f156, %f151;
	fma.rn.f32 	%f158, %f157, %f150, %f149;
	add.f32 	%f159, %f158, %f146;
	mov.f32 	%f160, 0f3F317218;
	fma.rn.f32 	%f162, %f145, %f160, %f159;

BB35_14:
	neg.f32 	%f163, %f162;

BB35_15:
	cvt.rzi.u32.f32	%r9, %f163;
	and.b32  	%r10, %r9, 255;
	st.param.b32	[funj_retval0+0], %r10;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___absc(
	.param .align 8 .b8 ___absc_param_0[8]
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<13>;


	ld.param.f32 	%f1, [___absc_param_0+4];
	ld.param.f32 	%f2, [___absc_param_0];
	abs.f32 	%f3, %f2;
	abs.f32 	%f4, %f1;
	setp.gt.f32	%p1, %f3, %f4;
	selp.f32	%f5, %f3, %f4, %p1;
	selp.f32	%f6, %f4, %f3, %p1;
	div.rn.f32 	%f7, %f6, %f5;
	fma.rn.f32 	%f8, %f7, %f7, 0f3F800000;
	sqrt.rn.f32 	%f9, %f8;
	mul.f32 	%f10, %f5, %f9;
	setp.eq.f32	%p2, %f5, 0f00000000;
	setp.gt.f32	%p3, %f5, 0f7F7FFFFF;
	or.pred  	%p4, %p2, %p3;
	setp.gt.f32	%p5, %f6, 0f7F7FFFFF;
	or.pred  	%p6, %p4, %p5;
	add.f32 	%f11, %f5, %f6;
	selp.f32	%f12, %f11, %f10, %p6;
	st.param.f32	[funj_retval0+0], %f12;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___absz(
	.param .align 16 .b8 ___absz_param_0[16]
)
{
	.reg .pred 	%p<7>;
	.reg .f64 	%fd<13>;


	ld.param.f64 	%fd1, [___absz_param_0+8];
	ld.param.f64 	%fd2, [___absz_param_0];
	abs.f64 	%fd3, %fd2;
	abs.f64 	%fd4, %fd1;
	setp.gt.f64	%p1, %fd3, %fd4;
	selp.f64	%fd5, %fd3, %fd4, %p1;
	selp.f64	%fd6, %fd4, %fd3, %p1;
	div.rn.f64 	%fd7, %fd6, %fd5;
	fma.rn.f64 	%fd8, %fd7, %fd7, 0d3FF0000000000000;
	sqrt.rn.f64 	%fd9, %fd8;
	mul.f64 	%fd10, %fd5, %fd9;
	setp.eq.f64	%p2, %fd5, 0d0000000000000000;
	setp.gt.f64	%p3, %fd5, 0d7FEFFFFFFFFFFFFF;
	or.pred  	%p4, %p2, %p3;
	setp.gt.f64	%p5, %fd6, 0d7FEFFFFFFFFFFFFF;
	or.pred  	%p6, %p4, %p5;
	add.f64 	%fd11, %fd5, %fd6;
	selp.f64	%fd12, %fd11, %fd10, %p6;
	st.param.f64	[funj_retval0+0], %fd12;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3remss(
	.param .b32 _Z3remss_param_0,
	.param .b32 _Z3remss_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<23>;
	.reg .f32 	%f<54>;


	ld.param.f32 	%f21, [_Z3remss_param_0];
	ld.param.f32 	%f22, [_Z3remss_param_1];
	abs.f32 	%f51, %f21;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	abs.f32 	%f2, %f22;
	setp.gtu.f32	%p2, %f2, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB38_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f2, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB38_3;
	bra.uni 	BB38_2;

BB38_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB38_21;

BB38_3:
	setp.ge.f32	%p7, %f51, %f2;
	@%p7 bra 	BB38_5;

	mov.u32 	%r22, 0;
	bra.uni 	BB38_16;

BB38_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f2;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p8, %f3, 0f00000000;
	setp.eq.f32	%p9, %f3, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB38_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB38_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB38_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f2, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB38_12;

BB38_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f2, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB38_12;

BB38_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f2, %f37;
	bra.uni 	BB38_12;

BB38_11:
	setp.leu.f32	%p15, %f3, 0f00000000;
	add.f32 	%f39, %f2, %f2;
	selp.f32	%f44, %f39, %f2, %p15;

BB38_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f2;
	@%p17 bra 	BB38_13;
	bra.uni 	BB38_22;

BB38_13:
	mov.f32 	%f52, %f51;

BB38_14:
	mov.f32 	%f10, %f52;
	mov.f32 	%f11, %f45;
	sub.f32 	%f42, %f10, %f11;
	setp.ltu.f32	%p18, %f10, %f11;
	selp.f32	%f52, %f10, %f42, %p18;
	mul.f32 	%f45, %f11, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f2;
	@%p19 bra 	BB38_14;

	setp.ge.f32	%p20, %f10, %f11;
	selp.u32	%r22, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB38_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f15, %f50, %f50;
	setp.gt.f32	%p21, %f15, %f2;
	@%p21 bra 	BB38_18;

	setp.eq.f32	%p22, %f15, %f2;
	setp.ne.s32	%p23, %r22, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB38_19;
	bra.uni 	BB38_18;

BB38_18:
	sub.f32 	%f50, %f50, %f2;

BB38_19:
	mov.b32 	 %r18, %f21;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB38_21;

BB38_20:
	add.f32 	%f53, %f21, %f22;

BB38_21:
	st.param.f32	[funj_retval0+0], %f53;
	ret;

BB38_22:
	mov.u32 	%r22, 0;
	bra.uni 	BB38_16;
}

.visible .func  (.param .b64 funj_retval0) _Z3remdd(
	.param .b64 _Z3remdd_param_0,
	.param .b64 _Z3remdd_param_1
)
{
	.reg .pred 	%p<24>;
	.reg .s32 	%r<35>;
	.reg .f64 	%fd<39>;


	ld.param.f64 	%fd22, [_Z3remdd_param_0];
	ld.param.f64 	%fd23, [_Z3remdd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd22;
	}
	and.b32  	%r12, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd23;
	}
	and.b32  	%r31, %r13, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd22;
	}
	mov.b64 	%fd37, {%r14, %r12};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd23;
	}
	mov.b64 	%fd2, {%r32, %r31};
	setp.gt.u32	%p1, %r12, 2146435071;
	setp.gt.u32	%p2, %r31, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB39_22;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB39_3;

	mov.f64 	%fd38, 0dFFF8000000000000;
	bra.uni 	BB39_25;

BB39_3:
	setp.ge.f64	%p5, %fd37, %fd2;
	@%p5 bra 	BB39_5;

	mov.u32 	%r34, 0;
	bra.uni 	BB39_18;

BB39_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.s32	%p6, %r15, 1048576;
	@%p6 bra 	BB39_7;

	mov.f64 	%fd33, 0d0000000000000000;
	bra.uni 	BB39_11;

BB39_7:
	setp.geu.f64	%p7, %fd2, %fd37;
	mov.f64 	%fd34, %fd2;
	@%p7 bra 	BB39_10;

	mov.f64 	%fd35, %fd2;

BB39_9:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p8, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd35;
	}
	setp.lt.s32	%p9, %r16, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p10 bra 	BB39_9;

BB39_10:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd33;
	}

BB39_11:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd37;
	}
	setp.lt.s32	%p11, %r17, 1048576;
	@%p11 bra 	BB39_13;

	and.b32  	%r18, %r31, 1048575;
	and.b32  	%r19, %r1, 2146435072;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd32, {%r32, %r20};

BB39_13:
	mul.f64 	%fd25, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd32, %fd37;
	selp.f64	%fd36, %fd25, %fd32, %p12;
	setp.ge.f64	%p13, %fd36, %fd2;
	@%p13 bra 	BB39_15;

	mov.u32 	%r34, 0;
	bra.uni 	BB39_18;

BB39_15:
	mov.u32 	%r33, -1;

BB39_16:
	setp.ltu.f64	%p14, %fd37, %fd36;
	selp.u32	%r22, 1, 0, %p14;
	shl.b32 	%r23, %r33, 1;
	add.s32 	%r33, %r22, %r23;
	sub.f64 	%fd26, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd26, %p14;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd36, %fd2;
	@%p15 bra 	BB39_16;

	not.b32 	%r24, %r33;
	and.b32  	%r34, %r24, 1;

BB39_18:
	add.f64 	%fd15, %fd37, %fd37;
	setp.gt.f64	%p16, %fd15, %fd2;
	@%p16 bra 	BB39_20;

	setp.eq.f64	%p17, %fd15, %fd2;
	setp.ne.s32	%p18, %r34, 0;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB39_21;
	bra.uni 	BB39_20;

BB39_20:
	sub.f64 	%fd37, %fd37, %fd2;

BB39_21:
	and.b32  	%r27, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd37;
	}
	xor.b32  	%r29, %r28, %r27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd37;
	}
	mov.b64 	%fd38, {%r30, %r29};
	bra.uni 	BB39_25;

BB39_22:
	setp.gtu.f64	%p20, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p21, %fd2, 0d7FF0000000000000;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	BB39_24;

	setp.eq.f64	%p23, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd22, %p23;
	bra.uni 	BB39_25;

BB39_24:
	add.f64 	%fd38, %fd22, %fd23;

BB39_25:
	st.param.f64	[funj_retval0+0], %fd38;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modss(
	.param .b32 _Z3modss_param_0,
	.param .b32 _Z3modss_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<18>;
	.reg .f32 	%f<48>;


	ld.param.f32 	%f18, [_Z3modss_param_0];
	ld.param.f32 	%f19, [_Z3modss_param_1];
	abs.f32 	%f46, %f18;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	abs.f32 	%f2, %f19;
	setp.eq.f32	%p2, %f2, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB40_2;
	bra.uni 	BB40_1;

BB40_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB40_14;

BB40_2:
	setp.ltu.f32	%p4, %f46, %f2;
	@%p4 bra 	BB40_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f2;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p5, %f3, 0f00000000;
	setp.eq.f32	%p6, %f3, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB40_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB40_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB40_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f2, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB40_10;

BB40_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f2, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB40_10;

BB40_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f2, %f34;
	bra.uni 	BB40_10;

BB40_9:
	setp.leu.f32	%p12, %f3, 0f00000000;
	add.f32 	%f36, %f2, %f2;
	selp.f32	%f44, %f36, %f2, %p12;

BB40_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f2;
	@%p14 bra 	BB40_12;

BB40_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f2;
	@%p16 bra 	BB40_11;

BB40_12:
	mov.b32 	 %r14, %f18;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB40_14;

BB40_13:
	setp.gtu.f32	%p17, %f2, 0f7F800000;
	add.f32 	%f40, %f18, %f19;
	selp.f32	%f41, %f40, %f18, %p17;
	add.f32 	%f42, %f41, %f18;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB40_14:
	st.param.f32	[funj_retval0+0], %f47;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3moddd(
	.param .b64 _Z3moddd_param_0,
	.param .b64 _Z3moddd_param_1
)
{
	.reg .pred 	%p<20>;
	.reg .s32 	%r<23>;
	.reg .f64 	%fd<36>;


	ld.param.f64 	%fd35, [_Z3moddd_param_0];
	ld.param.f64 	%fd20, [_Z3moddd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd35;
	}
	and.b32  	%r8, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd20;
	}
	and.b32  	%r21, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd35;
	}
	mov.b64 	%fd34, {%r10, %r8};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd20;
	}
	mov.b64 	%fd2, {%r22, %r21};
	setp.gt.u32	%p1, %r8, 2146435071;
	setp.gt.u32	%p2, %r21, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB41_15;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB41_3;

	mov.f64 	%fd35, 0dFFF8000000000000;
	bra.uni 	BB41_18;

BB41_3:
	setp.ltu.f64	%p5, %fd34, %fd2;
	@%p5 bra 	BB41_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd2;
	}
	setp.lt.s32	%p6, %r11, 1048576;
	@%p6 bra 	BB41_6;

	mov.f64 	%fd30, 0d0000000000000000;
	bra.uni 	BB41_10;

BB41_6:
	setp.geu.f64	%p7, %fd2, %fd34;
	mov.f64 	%fd31, %fd2;
	@%p7 bra 	BB41_9;

	mov.f64 	%fd32, %fd2;

BB41_8:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p8, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd32;
	}
	setp.lt.s32	%p9, %r12, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p10 bra 	BB41_8;

BB41_9:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd30;
	}

BB41_10:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd34;
	}
	setp.lt.s32	%p11, %r13, 1048576;
	@%p11 bra 	BB41_12;

	and.b32  	%r14, %r21, 1048575;
	and.b32  	%r15, %r1, 2146435072;
	or.b32  	%r16, %r14, %r15;
	mov.b64 	%fd29, {%r22, %r16};

BB41_12:
	mul.f64 	%fd22, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd29, %fd34;
	selp.f64	%fd33, %fd22, %fd29, %p12;
	setp.ltu.f64	%p13, %fd33, %fd2;
	@%p13 bra 	BB41_14;

BB41_13:
	sub.f64 	%fd23, %fd34, %fd33;
	setp.ltu.f64	%p14, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd23, %p14;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd33, %fd2;
	@%p15 bra 	BB41_13;

BB41_14:
	and.b32  	%r17, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd34;
	}
	or.b32  	%r19, %r18, %r17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r20, %r19};
	bra.uni 	BB41_18;

BB41_15:
	setp.gtu.f64	%p16, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p17, %fd2, 0d7FF0000000000000;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	BB41_17;

	setp.eq.f64	%p19, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd35, %p19;
	bra.uni 	BB41_18;

BB41_17:
	add.f64 	%fd35, %fd35, %fd20;

BB41_18:
	st.param.f64	[funj_retval0+0], %fd35;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minss(
	.param .b32 ___minss_param_0,
	.param .b32 ___minss_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___minss_param_0];
	ld.param.f32 	%f2, [___minss_param_1];
	min.f32 	%f3, %f1, %f2;
	st.param.f32	[funj_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minii(
	.param .b32 ___minii_param_0,
	.param .b32 ___minii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___minii_param_0];
	ld.param.u32 	%r2, [___minii_param_1];
	min.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minuu(
	.param .b32 ___minuu_param_0,
	.param .b32 ___minuu_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___minuu_param_0];
	ld.param.u32 	%r2, [___minuu_param_1];
	min.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___minxx(
	.param .b64 ___minxx_param_0,
	.param .b64 ___minxx_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___minxx_param_0];
	ld.param.u64 	%rd2, [___minxx_param_1];
	min.s64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___minyy(
	.param .b64 ___minyy_param_0,
	.param .b64 ___minyy_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___minyy_param_0];
	ld.param.u64 	%rd2, [___minyy_param_1];
	min.u64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minjj(
	.param .b32 ___minjj_param_0,
	.param .b32 ___minjj_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___minjj_param_0];
	ld.param.s8 	%rs2, [___minjj_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	min.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___minvv(
	.param .b32 ___minvv_param_0,
	.param .b32 ___minvv_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___minvv_param_0];
	ld.param.u8 	%rs2, [___minvv_param_1];
	cvt.u32.u16	%r1, %rs2;
	cvt.u32.u16	%r2, %rs1;
	min.s32 	%r3, %r2, %r1;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___mindd(
	.param .b64 ___mindd_param_0,
	.param .b64 ___mindd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___mindd_param_0];
	ld.param.f64 	%fd2, [___mindd_param_1];
	min.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[funj_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxss(
	.param .b32 ___maxss_param_0,
	.param .b32 ___maxss_param_1
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [___maxss_param_0];
	ld.param.f32 	%f2, [___maxss_param_1];
	max.f32 	%f3, %f1, %f2;
	st.param.f32	[funj_retval0+0], %f3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxii(
	.param .b32 ___maxii_param_0,
	.param .b32 ___maxii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___maxii_param_0];
	ld.param.u32 	%r2, [___maxii_param_1];
	max.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxuu(
	.param .b32 ___maxuu_param_0,
	.param .b32 ___maxuu_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___maxuu_param_0];
	ld.param.u32 	%r2, [___maxuu_param_1];
	max.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___maxxx(
	.param .b64 ___maxxx_param_0,
	.param .b64 ___maxxx_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___maxxx_param_0];
	ld.param.u64 	%rd2, [___maxxx_param_1];
	max.s64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___maxyy(
	.param .b64 ___maxyy_param_0,
	.param .b64 ___maxyy_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___maxyy_param_0];
	ld.param.u64 	%rd2, [___maxyy_param_1];
	max.u64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxjj(
	.param .b32 ___maxjj_param_0,
	.param .b32 ___maxjj_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___maxjj_param_0];
	ld.param.s8 	%rs2, [___maxjj_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	max.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___maxvv(
	.param .b32 ___maxvv_param_0,
	.param .b32 ___maxvv_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.u8 	%rs1, [___maxvv_param_0];
	ld.param.u8 	%rs2, [___maxvv_param_1];
	cvt.u32.u16	%r1, %rs2;
	cvt.u32.u16	%r2, %rs1;
	max.s32 	%r3, %r2, %r1;
	and.b32  	%r4, %r3, 255;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___maxdd(
	.param .b64 ___maxdd_param_0,
	.param .b64 ___maxdd_param_1
)
{
	.reg .f64 	%fd<4>;


	ld.param.f64 	%fd1, [___maxdd_param_0];
	ld.param.f64 	%fd2, [___maxdd_param_1];
	max.f64 	%fd3, %fd1, %fd2;
	st.param.f64	[funj_retval0+0], %fd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modss(
	.param .b32 ___modss_param_0,
	.param .b32 ___modss_param_1
)
{
	.reg .pred 	%p<19>;
	.reg .s32 	%r<18>;
	.reg .f32 	%f<48>;


	ld.param.f32 	%f18, [___modss_param_0];
	ld.param.f32 	%f19, [___modss_param_1];
	abs.f32 	%f46, %f18;
	setp.eq.f32	%p1, %f46, 0f7F800000;
	abs.f32 	%f2, %f19;
	setp.eq.f32	%p2, %f2, 0f00000000;
	or.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB58_2;
	bra.uni 	BB58_1;

BB58_1:
	mov.f32 	%f47, 0f7FFFFFFF;
	bra.uni 	BB58_14;

BB58_2:
	setp.ltu.f32	%p4, %f46, %f2;
	@%p4 bra 	BB58_13;

	lg2.approx.f32 	%f20, %f46;
	cvt.rzi.s32.f32	%r3, %f20;
	lg2.approx.f32 	%f21, %f2;
	cvt.rzi.s32.f32	%r4, %f21;
	sub.s32 	%r1, %r3, %r4;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p5, %f3, 0f00000000;
	setp.eq.f32	%p6, %f3, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	setp.eq.s32	%p8, %r3, %r4;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB58_9;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p10, %r2, 126;
	@%p10 bra 	BB58_8;

	setp.lt.s32	%p11, %r2, 252;
	@%p11 bra 	BB58_7;

	shr.s32 	%r5, %r1, 31;
	shr.u32 	%r6, %r5, 30;
	add.s32 	%r7, %r1, %r6;
	shr.s32 	%r8, %r7, 2;
	cvt.rn.f32.s32	%f23, %r8;
	// inline asm
	ex2.approx.ftz.f32 %f22,%f23;
	// inline asm
	mul.f32 	%f26, %f2, %f22;
	mul.f32 	%f27, %f26, %f22;
	mul.f32 	%f28, %f27, %f22;
	mad.lo.s32 	%r9, %r8, -3, %r1;
	cvt.rn.f32.s32	%f25, %r9;
	// inline asm
	ex2.approx.ftz.f32 %f24,%f25;
	// inline asm
	mul.f32 	%f44, %f28, %f24;
	bra.uni 	BB58_10;

BB58_7:
	shr.u32 	%r10, %r1, 31;
	add.s32 	%r11, %r1, %r10;
	shr.s32 	%r12, %r11, 1;
	cvt.rn.f32.s32	%f30, %r12;
	// inline asm
	ex2.approx.ftz.f32 %f29,%f30;
	// inline asm
	mul.f32 	%f33, %f2, %f29;
	sub.s32 	%r13, %r1, %r12;
	cvt.rn.f32.s32	%f32, %r13;
	// inline asm
	ex2.approx.ftz.f32 %f31,%f32;
	// inline asm
	mul.f32 	%f44, %f33, %f31;
	bra.uni 	BB58_10;

BB58_8:
	cvt.rn.f32.s32	%f35, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f2, %f34;
	bra.uni 	BB58_10;

BB58_9:
	setp.leu.f32	%p12, %f3, 0f00000000;
	add.f32 	%f36, %f2, %f2;
	selp.f32	%f44, %f36, %f2, %p12;

BB58_10:
	mul.f32 	%f37, %f46, 0f3F000000;
	setp.gtu.f32	%p13, %f44, %f37;
	add.f32 	%f38, %f44, %f44;
	selp.f32	%f45, %f44, %f38, %p13;
	setp.ltu.f32	%p14, %f45, %f2;
	@%p14 bra 	BB58_12;

BB58_11:
	sub.f32 	%f39, %f46, %f45;
	setp.ltu.f32	%p15, %f46, %f45;
	selp.f32	%f46, %f46, %f39, %p15;
	mul.f32 	%f45, %f45, 0f3F000000;
	setp.ge.f32	%p16, %f45, %f2;
	@%p16 bra 	BB58_11;

BB58_12:
	mov.b32 	 %r14, %f18;
	and.b32  	%r15, %r14, -2147483648;
	mov.b32 	 %r16, %f46;
	or.b32  	%r17, %r16, %r15;
	mov.b32 	 %f47, %r17;
	bra.uni 	BB58_14;

BB58_13:
	setp.gtu.f32	%p17, %f2, 0f7F800000;
	add.f32 	%f40, %f18, %f19;
	selp.f32	%f41, %f40, %f18, %p17;
	add.f32 	%f42, %f41, %f18;
	setp.leu.f32	%p18, %f46, 0f00000000;
	selp.f32	%f47, %f42, %f41, %p18;

BB58_14:
	st.param.f32	[funj_retval0+0], %f47;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modii(
	.param .b32 ___modii_param_0,
	.param .b32 ___modii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___modii_param_0];
	ld.param.u32 	%r2, [___modii_param_1];
	rem.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIiET_S0_S0_(
	.param .b32 _Z3modIiET_S0_S0__param_0,
	.param .b32 _Z3modIiET_S0_S0__param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [_Z3modIiET_S0_S0__param_0];
	ld.param.u32 	%r2, [_Z3modIiET_S0_S0__param_1];
	rem.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___moduu(
	.param .b32 ___moduu_param_0,
	.param .b32 ___moduu_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___moduu_param_0];
	ld.param.u32 	%r2, [___moduu_param_1];
	rem.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIjET_S0_S0_(
	.param .b32 _Z3modIjET_S0_S0__param_0,
	.param .b32 _Z3modIjET_S0_S0__param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [_Z3modIjET_S0_S0__param_0];
	ld.param.u32 	%r2, [_Z3modIjET_S0_S0__param_1];
	rem.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___modxx(
	.param .b64 ___modxx_param_0,
	.param .b64 ___modxx_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___modxx_param_0];
	ld.param.u64 	%rd2, [___modxx_param_1];
	rem.s64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3modIxET_S0_S0_(
	.param .b64 _Z3modIxET_S0_S0__param_0,
	.param .b64 _Z3modIxET_S0_S0__param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z3modIxET_S0_S0__param_0];
	ld.param.u64 	%rd2, [_Z3modIxET_S0_S0__param_1];
	rem.s64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___modyy(
	.param .b64 ___modyy_param_0,
	.param .b64 ___modyy_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___modyy_param_0];
	ld.param.u64 	%rd2, [___modyy_param_1];
	rem.u64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3modIyET_S0_S0_(
	.param .b64 _Z3modIyET_S0_S0__param_0,
	.param .b64 _Z3modIyET_S0_S0__param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z3modIyET_S0_S0__param_0];
	ld.param.u64 	%rd2, [_Z3modIyET_S0_S0__param_1];
	rem.u64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modjj(
	.param .b32 ___modjj_param_0,
	.param .b32 ___modjj_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___modjj_param_0];
	ld.param.s8 	%rs2, [___modjj_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	rem.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIcET_S0_S0_(
	.param .b32 _Z3modIcET_S0_S0__param_0,
	.param .b32 _Z3modIcET_S0_S0__param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [_Z3modIcET_S0_S0__param_0];
	ld.param.s8 	%rs2, [_Z3modIcET_S0_S0__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	rem.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___modvv(
	.param .b32 ___modvv_param_0,
	.param .b32 ___modvv_param_1
)
{
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___modvv_param_1];
	ld.param.u8 	%rs2, [___modvv_param_0];
	rem.u16 	%rs3, %rs2, %rs1;
	cvt.u32.u16	%r1, %rs3;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3modIhET_S0_S0_(
	.param .b32 _Z3modIhET_S0_S0__param_0,
	.param .b32 _Z3modIhET_S0_S0__param_1
)
{
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [_Z3modIhET_S0_S0__param_1];
	ld.param.u8 	%rs2, [_Z3modIhET_S0_S0__param_0];
	rem.u16 	%rs3, %rs2, %rs1;
	cvt.u32.u16	%r1, %rs3;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___moddd(
	.param .b64 ___moddd_param_0,
	.param .b64 ___moddd_param_1
)
{
	.reg .pred 	%p<20>;
	.reg .s32 	%r<23>;
	.reg .f64 	%fd<36>;


	ld.param.f64 	%fd35, [___moddd_param_0];
	ld.param.f64 	%fd20, [___moddd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd35;
	}
	and.b32  	%r8, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd20;
	}
	and.b32  	%r21, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd35;
	}
	mov.b64 	%fd34, {%r10, %r8};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd20;
	}
	mov.b64 	%fd2, {%r22, %r21};
	setp.gt.u32	%p1, %r8, 2146435071;
	setp.gt.u32	%p2, %r21, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB71_15;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB71_3;

	mov.f64 	%fd35, 0dFFF8000000000000;
	bra.uni 	BB71_18;

BB71_3:
	setp.ltu.f64	%p5, %fd34, %fd2;
	@%p5 bra 	BB71_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd2;
	}
	setp.lt.s32	%p6, %r11, 1048576;
	@%p6 bra 	BB71_6;

	mov.f64 	%fd30, 0d0000000000000000;
	bra.uni 	BB71_10;

BB71_6:
	setp.geu.f64	%p7, %fd2, %fd34;
	mov.f64 	%fd31, %fd2;
	@%p7 bra 	BB71_9;

	mov.f64 	%fd32, %fd2;

BB71_8:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p8, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd32;
	}
	setp.lt.s32	%p9, %r12, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p10 bra 	BB71_8;

BB71_9:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd30;
	}

BB71_10:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd34;
	}
	setp.lt.s32	%p11, %r13, 1048576;
	@%p11 bra 	BB71_12;

	and.b32  	%r14, %r21, 1048575;
	and.b32  	%r15, %r1, 2146435072;
	or.b32  	%r16, %r14, %r15;
	mov.b64 	%fd29, {%r22, %r16};

BB71_12:
	mul.f64 	%fd22, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd29, %fd34;
	selp.f64	%fd33, %fd22, %fd29, %p12;
	setp.ltu.f64	%p13, %fd33, %fd2;
	@%p13 bra 	BB71_14;

BB71_13:
	sub.f64 	%fd23, %fd34, %fd33;
	setp.ltu.f64	%p14, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd23, %p14;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd33, %fd2;
	@%p15 bra 	BB71_13;

BB71_14:
	and.b32  	%r17, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd34;
	}
	or.b32  	%r19, %r18, %r17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r20, %r19};
	bra.uni 	BB71_18;

BB71_15:
	setp.gtu.f64	%p16, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p17, %fd2, 0d7FF0000000000000;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	BB71_17;

	setp.eq.f64	%p19, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd35, %p19;
	bra.uni 	BB71_18;

BB71_17:
	add.f64 	%fd35, %fd35, %fd20;

BB71_18:
	st.param.f64	[funj_retval0+0], %fd35;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___remss(
	.param .b32 ___remss_param_0,
	.param .b32 ___remss_param_1
)
{
	.reg .pred 	%p<25>;
	.reg .s32 	%r<23>;
	.reg .f32 	%f<54>;


	ld.param.f32 	%f21, [___remss_param_0];
	ld.param.f32 	%f22, [___remss_param_1];
	abs.f32 	%f51, %f21;
	setp.gtu.f32	%p1, %f51, 0f7F800000;
	abs.f32 	%f2, %f22;
	setp.gtu.f32	%p2, %f2, 0f7F800000;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB72_20;

	setp.eq.f32	%p4, %f51, 0f7F800000;
	setp.eq.f32	%p5, %f2, 0f00000000;
	or.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB72_3;
	bra.uni 	BB72_2;

BB72_2:
	mov.f32 	%f53, 0f7FFFFFFF;
	bra.uni 	BB72_21;

BB72_3:
	setp.ge.f32	%p7, %f51, %f2;
	@%p7 bra 	BB72_5;

	mov.u32 	%r22, 0;
	bra.uni 	BB72_16;

BB72_5:
	lg2.approx.f32 	%f23, %f51;
	cvt.rzi.s32.f32	%r5, %f23;
	lg2.approx.f32 	%f24, %f2;
	cvt.rzi.s32.f32	%r6, %f24;
	sub.s32 	%r1, %r5, %r6;
	abs.f32 	%f3, %f2;
	setp.eq.f32	%p8, %f3, 0f00000000;
	setp.eq.f32	%p9, %f3, 0f7F800000;
	or.pred  	%p10, %p8, %p9;
	setp.eq.s32	%p11, %r5, %r6;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	BB72_11;

	abs.s32 	%r2, %r1;
	setp.lt.s32	%p13, %r2, 126;
	@%p13 bra 	BB72_10;

	setp.lt.s32	%p14, %r2, 252;
	@%p14 bra 	BB72_9;

	shr.s32 	%r7, %r1, 31;
	shr.u32 	%r8, %r7, 30;
	add.s32 	%r9, %r1, %r8;
	shr.s32 	%r10, %r9, 2;
	cvt.rn.f32.s32	%f26, %r10;
	// inline asm
	ex2.approx.ftz.f32 %f25,%f26;
	// inline asm
	mul.f32 	%f29, %f2, %f25;
	mul.f32 	%f30, %f29, %f25;
	mul.f32 	%f31, %f30, %f25;
	mad.lo.s32 	%r11, %r10, -3, %r1;
	cvt.rn.f32.s32	%f28, %r11;
	// inline asm
	ex2.approx.ftz.f32 %f27,%f28;
	// inline asm
	mul.f32 	%f44, %f31, %f27;
	bra.uni 	BB72_12;

BB72_9:
	shr.u32 	%r12, %r1, 31;
	add.s32 	%r13, %r1, %r12;
	shr.s32 	%r14, %r13, 1;
	cvt.rn.f32.s32	%f33, %r14;
	// inline asm
	ex2.approx.ftz.f32 %f32,%f33;
	// inline asm
	mul.f32 	%f36, %f2, %f32;
	sub.s32 	%r15, %r1, %r14;
	cvt.rn.f32.s32	%f35, %r15;
	// inline asm
	ex2.approx.ftz.f32 %f34,%f35;
	// inline asm
	mul.f32 	%f44, %f36, %f34;
	bra.uni 	BB72_12;

BB72_10:
	cvt.rn.f32.s32	%f38, %r1;
	// inline asm
	ex2.approx.ftz.f32 %f37,%f38;
	// inline asm
	mul.f32 	%f44, %f2, %f37;
	bra.uni 	BB72_12;

BB72_11:
	setp.leu.f32	%p15, %f3, 0f00000000;
	add.f32 	%f39, %f2, %f2;
	selp.f32	%f44, %f39, %f2, %p15;

BB72_12:
	mul.f32 	%f40, %f51, 0f3F000000;
	setp.gtu.f32	%p16, %f44, %f40;
	add.f32 	%f41, %f44, %f44;
	selp.f32	%f45, %f44, %f41, %p16;
	setp.ge.f32	%p17, %f45, %f2;
	@%p17 bra 	BB72_13;
	bra.uni 	BB72_22;

BB72_13:
	mov.f32 	%f52, %f51;

BB72_14:
	mov.f32 	%f10, %f52;
	mov.f32 	%f11, %f45;
	sub.f32 	%f42, %f10, %f11;
	setp.ltu.f32	%p18, %f10, %f11;
	selp.f32	%f52, %f10, %f42, %p18;
	mul.f32 	%f45, %f11, 0f3F000000;
	setp.ge.f32	%p19, %f45, %f2;
	@%p19 bra 	BB72_14;

	setp.ge.f32	%p20, %f10, %f11;
	selp.u32	%r22, 1, 0, %p20;
	mov.f32 	%f51, %f52;

BB72_16:
	mov.f32 	%f50, %f51;
	add.f32 	%f15, %f50, %f50;
	setp.gt.f32	%p21, %f15, %f2;
	@%p21 bra 	BB72_18;

	setp.eq.f32	%p22, %f15, %f2;
	setp.ne.s32	%p23, %r22, 0;
	and.pred  	%p24, %p22, %p23;
	@!%p24 bra 	BB72_19;
	bra.uni 	BB72_18;

BB72_18:
	sub.f32 	%f50, %f50, %f2;

BB72_19:
	mov.b32 	 %r18, %f21;
	and.b32  	%r19, %r18, -2147483648;
	mov.b32 	 %r20, %f50;
	xor.b32  	%r21, %r20, %r19;
	mov.b32 	 %f53, %r21;
	bra.uni 	BB72_21;

BB72_20:
	add.f32 	%f53, %f21, %f22;

BB72_21:
	st.param.f32	[funj_retval0+0], %f53;
	ret;

BB72_22:
	mov.u32 	%r22, 0;
	bra.uni 	BB72_16;
}

.visible .func  (.param .b32 funj_retval0) ___remii(
	.param .b32 ___remii_param_0,
	.param .b32 ___remii_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___remii_param_0];
	ld.param.u32 	%r2, [___remii_param_1];
	rem.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIiET_S0_S0_(
	.param .b32 _Z3remIiET_S0_S0__param_0,
	.param .b32 _Z3remIiET_S0_S0__param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [_Z3remIiET_S0_S0__param_0];
	ld.param.u32 	%r2, [_Z3remIiET_S0_S0__param_1];
	rem.s32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___remuu(
	.param .b32 ___remuu_param_0,
	.param .b32 ___remuu_param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [___remuu_param_0];
	ld.param.u32 	%r2, [___remuu_param_1];
	rem.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIjET_S0_S0_(
	.param .b32 _Z3remIjET_S0_S0__param_0,
	.param .b32 _Z3remIjET_S0_S0__param_1
)
{
	.reg .s32 	%r<4>;


	ld.param.u32 	%r1, [_Z3remIjET_S0_S0__param_0];
	ld.param.u32 	%r2, [_Z3remIjET_S0_S0__param_1];
	rem.u32 	%r3, %r1, %r2;
	st.param.b32	[funj_retval0+0], %r3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___remxx(
	.param .b64 ___remxx_param_0,
	.param .b64 ___remxx_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___remxx_param_0];
	ld.param.u64 	%rd2, [___remxx_param_1];
	rem.s64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3remIxET_S0_S0_(
	.param .b64 _Z3remIxET_S0_S0__param_0,
	.param .b64 _Z3remIxET_S0_S0__param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z3remIxET_S0_S0__param_0];
	ld.param.u64 	%rd2, [_Z3remIxET_S0_S0__param_1];
	rem.s64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___remyy(
	.param .b64 ___remyy_param_0,
	.param .b64 ___remyy_param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [___remyy_param_0];
	ld.param.u64 	%rd2, [___remyy_param_1];
	rem.u64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b64 funj_retval0) _Z3remIyET_S0_S0_(
	.param .b64 _Z3remIyET_S0_S0__param_0,
	.param .b64 _Z3remIyET_S0_S0__param_1
)
{
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z3remIyET_S0_S0__param_0];
	ld.param.u64 	%rd2, [_Z3remIyET_S0_S0__param_1];
	rem.u64 	%rd3, %rd1, %rd2;
	st.param.b64	[funj_retval0+0], %rd3;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___remjj(
	.param .b32 ___remjj_param_0,
	.param .b32 ___remjj_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [___remjj_param_0];
	ld.param.s8 	%rs2, [___remjj_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	rem.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIcET_S0_S0_(
	.param .b32 _Z3remIcET_S0_S0__param_0,
	.param .b32 _Z3remIcET_S0_S0__param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;


	ld.param.s8 	%rs1, [_Z3remIcET_S0_S0__param_0];
	ld.param.s8 	%rs2, [_Z3remIcET_S0_S0__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	rem.s32 	%r3, %r1, %r2;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[funj_retval0+0], %r4;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___remvv(
	.param .b32 ___remvv_param_0,
	.param .b32 ___remvv_param_1
)
{
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [___remvv_param_1];
	ld.param.u8 	%rs2, [___remvv_param_0];
	rem.u16 	%rs3, %rs2, %rs1;
	cvt.u32.u16	%r1, %rs3;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b32 funj_retval0) _Z3remIhET_S0_S0_(
	.param .b32 _Z3remIhET_S0_S0__param_0,
	.param .b32 _Z3remIhET_S0_S0__param_1
)
{
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<2>;


	ld.param.u8 	%rs1, [_Z3remIhET_S0_S0__param_1];
	ld.param.u8 	%rs2, [_Z3remIhET_S0_S0__param_0];
	rem.u16 	%rs3, %rs2, %rs1;
	cvt.u32.u16	%r1, %rs3;
	st.param.b32	[funj_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___remdd(
	.param .b64 ___remdd_param_0,
	.param .b64 ___remdd_param_1
)
{
	.reg .pred 	%p<24>;
	.reg .s32 	%r<35>;
	.reg .f64 	%fd<39>;


	ld.param.f64 	%fd22, [___remdd_param_0];
	ld.param.f64 	%fd23, [___remdd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd22;
	}
	and.b32  	%r12, %r1, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd23;
	}
	and.b32  	%r31, %r13, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd22;
	}
	mov.b64 	%fd37, {%r14, %r12};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd23;
	}
	mov.b64 	%fd2, {%r32, %r31};
	setp.gt.u32	%p1, %r12, 2146435071;
	setp.gt.u32	%p2, %r31, 2146435071;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB85_22;

	setp.neu.f64	%p4, %fd2, 0d0000000000000000;
	@%p4 bra 	BB85_3;

	mov.f64 	%fd38, 0dFFF8000000000000;
	bra.uni 	BB85_25;

BB85_3:
	setp.ge.f64	%p5, %fd37, %fd2;
	@%p5 bra 	BB85_5;

	mov.u32 	%r34, 0;
	bra.uni 	BB85_18;

BB85_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.s32	%p6, %r15, 1048576;
	@%p6 bra 	BB85_7;

	mov.f64 	%fd33, 0d0000000000000000;
	bra.uni 	BB85_11;

BB85_7:
	setp.geu.f64	%p7, %fd2, %fd37;
	mov.f64 	%fd34, %fd2;
	@%p7 bra 	BB85_10;

	mov.f64 	%fd35, %fd2;

BB85_9:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p8, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd35;
	}
	setp.lt.s32	%p9, %r16, 1048576;
	and.pred  	%p10, %p8, %p9;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p10 bra 	BB85_9;

BB85_10:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd33;
	}

BB85_11:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd37;
	}
	setp.lt.s32	%p11, %r17, 1048576;
	@%p11 bra 	BB85_13;

	and.b32  	%r18, %r31, 1048575;
	and.b32  	%r19, %r1, 2146435072;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd32, {%r32, %r20};

BB85_13:
	mul.f64 	%fd25, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p12, %fd32, %fd37;
	selp.f64	%fd36, %fd25, %fd32, %p12;
	setp.ge.f64	%p13, %fd36, %fd2;
	@%p13 bra 	BB85_15;

	mov.u32 	%r34, 0;
	bra.uni 	BB85_18;

BB85_15:
	mov.u32 	%r33, -1;

BB85_16:
	setp.ltu.f64	%p14, %fd37, %fd36;
	selp.u32	%r22, 1, 0, %p14;
	shl.b32 	%r23, %r33, 1;
	add.s32 	%r33, %r22, %r23;
	sub.f64 	%fd26, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd26, %p14;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p15, %fd36, %fd2;
	@%p15 bra 	BB85_16;

	not.b32 	%r24, %r33;
	and.b32  	%r34, %r24, 1;

BB85_18:
	add.f64 	%fd15, %fd37, %fd37;
	setp.gt.f64	%p16, %fd15, %fd2;
	@%p16 bra 	BB85_20;

	setp.eq.f64	%p17, %fd15, %fd2;
	setp.ne.s32	%p18, %r34, 0;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB85_21;
	bra.uni 	BB85_20;

BB85_20:
	sub.f64 	%fd37, %fd37, %fd2;

BB85_21:
	and.b32  	%r27, %r1, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd37;
	}
	xor.b32  	%r29, %r28, %r27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd37;
	}
	mov.b64 	%fd38, {%r30, %r29};
	bra.uni 	BB85_25;

BB85_22:
	setp.gtu.f64	%p20, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p21, %fd2, 0d7FF0000000000000;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	BB85_24;

	setp.eq.f64	%p23, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd22, %p23;
	bra.uni 	BB85_25;

BB85_24:
	add.f64 	%fd38, %fd22, %fd23;

BB85_25:
	st.param.f64	[funj_retval0+0], %fd38;
	ret;
}

.visible .func  (.param .b32 funj_retval0) ___hypotss(
	.param .b32 ___hypotss_param_0,
	.param .b32 ___hypotss_param_1
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<21>;


	ld.param.f32 	%f1, [___hypotss_param_0];
	ld.param.f32 	%f2, [___hypotss_param_1];
	abs.f32 	%f3, %f1;
	abs.f32 	%f4, %f2;
	max.f32 	%f5, %f3, %f4;
	min.f32 	%f6, %f3, %f4;
	setp.gt.f32	%p1, %f5, 0f7E800000;
	mul.f32 	%f7, %f5, 0f3E800000;
	mul.f32 	%f8, %f6, 0f3E800000;
	selp.f32	%f9, %f7, %f5, %p1;
	selp.f32	%f10, %f8, %f6, %p1;
	div.approx.f32 	%f11, %f10, %f9;
	mov.f32 	%f12, 0f3F800000;
	fma.rn.f32 	%f13, %f11, %f11, %f12;
	sqrt.rn.f32 	%f14, %f13;
	mul.f32 	%f15, %f5, %f14;
	setp.eq.f32	%p2, %f5, 0f00000000;
	add.f32 	%f16, %f5, %f6;
	selp.f32	%f17, %f16, %f15, %p2;
	setp.gtu.f32	%p3, %f3, 0f7F800000;
	setp.gtu.f32	%p4, %f4, 0f7F800000;
	or.pred  	%p5, %p3, %p4;
	add.f32 	%f18, %f1, %f2;
	selp.f32	%f19, %f18, %f17, %p5;
	setp.eq.f32	%p6, %f5, 0f7F800000;
	selp.f32	%f20, %f16, %f19, %p6;
	st.param.f32	[funj_retval0+0], %f20;
	ret;
}

.visible .func  (.param .b64 funj_retval0) ___hypotdd(
	.param .b64 ___hypotdd_param_0,
	.param .b64 ___hypotdd_param_1
)
{
	.reg .pred 	%p<6>;
	.reg .s32 	%r<34>;
	.reg .f64 	%fd<40>;


	ld.param.f64 	%fd14, [___hypotdd_param_0];
	ld.param.f64 	%fd15, [___hypotdd_param_1];
	abs.f64 	%fd1, %fd14;
	abs.f64 	%fd2, %fd15;
	max.f64 	%fd3, %fd1, %fd2;
	min.f64 	%fd37, %fd1, %fd2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd3;
	}
	shr.u32 	%r2, %r1, 20;
	add.s32 	%r3, %r2, -1023;
	mov.u32 	%r12, 1023;
	sub.s32 	%r4, %r12, %r2;
	abs.s32 	%r13, %r4;
	setp.lt.s32	%p1, %r13, 1023;
	shl.b32 	%r5, %r4, 20;
	@%p1 bra 	BB87_2;

	add.s32 	%r14, %r4, 2046;
	shl.b32 	%r15, %r14, 19;
	and.b32  	%r16, %r15, -1048576;
	shl.b32 	%r17, %r14, 20;
	sub.s32 	%r32, %r17, %r16;
	mov.u32 	%r18, 0;
	mov.b64 	%fd16, {%r18, %r16};
	mul.f64 	%fd37, %fd37, %fd16;
	bra.uni 	BB87_3;

BB87_2:
	add.s32 	%r32, %r5, 1072693248;

BB87_3:
	mov.u32 	%r19, 0;
	mov.b64 	%fd17, {%r19, %r32};
	mul.f64 	%fd7, %fd37, %fd17;
	setp.lt.s32	%p2, %r3, 0;
	@%p2 bra 	BB87_5;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd3;
	}
	add.s32 	%r21, %r5, %r1;
	mov.b64 	%fd38, {%r20, %r21};
	bra.uni 	BB87_6;

BB87_5:
	add.s32 	%r22, %r5, 1072693248;
	mov.b64 	%fd18, {%r19, %r22};
	mul.f64 	%fd38, %fd3, %fd18;

BB87_6:
	mul.f64 	%fd21, %fd7, %fd7;
	fma.rn.f64 	%fd20, %fd38, %fd38, %fd21;
	// inline asm
	rsqrt.approx.ftz.f64 %fd19, %fd20;
	// inline asm
	mul.rn.f64 	%fd22, %fd19, %fd19;
	neg.f64 	%fd23, %fd22;
	mov.f64 	%fd24, 0d3FF0000000000000;
	fma.rn.f64 	%fd25, %fd20, %fd23, %fd24;
	mov.f64 	%fd26, 0d3FE0000000000000;
	mov.f64 	%fd27, 0d3FD8000000000000;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mul.rn.f64 	%fd29, %fd25, %fd19;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd19;
	mul.f64 	%fd39, %fd20, %fd30;
	abs.s32 	%r24, %r3;
	setp.lt.s32	%p3, %r24, 1023;
	@%p3 bra 	BB87_8;

	add.s32 	%r25, %r2, 1023;
	shl.b32 	%r26, %r25, 19;
	and.b32  	%r27, %r26, -1048576;
	shl.b32 	%r28, %r25, 20;
	sub.s32 	%r33, %r28, %r27;
	mov.b64 	%fd31, {%r19, %r27};
	mul.f64 	%fd39, %fd39, %fd31;
	bra.uni 	BB87_9;

BB87_8:
	shl.b32 	%r30, %r3, 20;
	add.s32 	%r33, %r30, 1072693248;

BB87_9:
	mov.b64 	%fd32, {%r19, %r33};
	mul.f64 	%fd33, %fd39, %fd32;
	add.f64 	%fd34, %fd1, %fd2;
	setp.leu.f64	%p4, %fd34, %fd3;
	selp.f64	%fd35, %fd34, %fd33, %p4;
	setp.eq.f64	%p5, %fd3, 0d7FF0000000000000;
	selp.f64	%fd36, %fd3, %fd35, %p5;
	st.param.f64	[funj_retval0+0], %fd36;
	ret;
}

.visible .func  (.param .align 8 .b8 funj_retval0[8]) ___mincc(
	.param .align 8 .b8 ___mincc_param_0[8],
	.param .align 8 .b8 ___mincc_param_1[8]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;


	ld.param.f32 	%f1, [___mincc_param_0];
	ld.param.f32 	%f2, [___mincc_param_0+4];
	ld.param.f32 	%f3, [___mincc_param_1];
	ld.param.f32 	%f4, [___mincc_param_1+4];
	mul.f32 	%f5, %f2, %f2;
	fma.rn.f32 	%f6, %f1, %f1, %f5;
	mul.f32 	%f7, %f4, %f4;
	fma.rn.f32 	%f8, %f3, %f3, %f7;
	setp.lt.f32	%p1, %f6, %f8;
	selp.f32	%f9, %f2, %f4, %p1;
	selp.f32	%f10, %f1, %f3, %p1;
	st.param.f32	[funj_retval0+0], %f10;
	st.param.f32	[funj_retval0+4], %f9;
	ret;
}

.visible .func  (.param .align 16 .b8 funj_retval0[16]) ___minzz(
	.param .align 16 .b8 ___minzz_param_0[16],
	.param .align 16 .b8 ___minzz_param_1[16]
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<11>;


	ld.param.f64 	%fd1, [___minzz_param_0];
	ld.param.f64 	%fd2, [___minzz_param_0+8];
	ld.param.f64 	%fd3, [___minzz_param_1];
	ld.param.f64 	%fd4, [___minzz_param_1+8];
	mul.f64 	%fd5, %fd2, %fd2;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd5;
	mul.f64 	%fd7, %fd4, %fd4;
	fma.rn.f64 	%fd8, %fd3, %fd3, %fd7;
	setp.lt.f64	%p1, %fd6, %fd8;
	selp.f64	%fd9, %fd2, %fd4, %p1;
	selp.f64	%fd10, %fd1, %fd3, %p1;
	st.param.f64	[funj_retval0+0], %fd10;
	st.param.f64	[funj_retval0+8], %fd9;
	ret;
}

.visible .func  (.param .align 8 .b8 funj_retval0[8]) ___maxcc(
	.param .align 8 .b8 ___maxcc_param_0[8],
	.param .align 8 .b8 ___maxcc_param_1[8]
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;


	ld.param.f32 	%f1, [___maxcc_param_0];
	ld.param.f32 	%f2, [___maxcc_param_0+4];
	ld.param.f32 	%f3, [___maxcc_param_1];
	ld.param.f32 	%f4, [___maxcc_param_1+4];
	mul.f32 	%f5, %f2, %f2;
	fma.rn.f32 	%f6, %f1, %f1, %f5;
	mul.f32 	%f7, %f4, %f4;
	fma.rn.f32 	%f8, %f3, %f3, %f7;
	setp.gt.f32	%p1, %f6, %f8;
	selp.f32	%f9, %f2, %f4, %p1;
	selp.f32	%f10, %f1, %f3, %p1;
	st.param.f32	[funj_retval0+0], %f10;
	st.param.f32	[funj_retval0+4], %f9;
	ret;
}

.visible .func  (.param .align 16 .b8 funj_retval0[16]) ___maxzz(
	.param .align 16 .b8 ___maxzz_param_0[16],
	.param .align 16 .b8 ___maxzz_param_1[16]
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<11>;


	ld.param.f64 	%fd1, [___maxzz_param_0];
	ld.param.f64 	%fd2, [___maxzz_param_0+8];
	ld.param.f64 	%fd3, [___maxzz_param_1];
	ld.param.f64 	%fd4, [___maxzz_param_1+8];
	mul.f64 	%fd5, %fd2, %fd2;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd5;
	mul.f64 	%fd7, %fd4, %fd4;
	fma.rn.f64 	%fd8, %fd3, %fd3, %fd7;
	setp.gt.f64	%p1, %fd6, %fd8;
	selp.f64	%fd9, %fd2, %fd4, %p1;
	selp.f64	%fd10, %fd1, %fd3, %p1;
	st.param.f64	[funj_retval0+0], %fd10;
	st.param.f64	[funj_retval0+8], %fd9;
	ret;
}

.func  (.param .b64 funj_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .s32 	%r<48>;
	.reg .f32 	%f<3>;
	.reg .f64 	%fd<141>;


	ld.param.f64 	%fd14, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd15, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd14;
	}
	shr.u32 	%r45, %r44, 20;
	setp.ne.s32	%p1, %r45, 0;
	@%p1 bra 	BB92_2;

	mul.f64 	%fd16, %fd14, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd16;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd16;
	}
	shr.u32 	%r18, %r44, 20;
	add.s32 	%r45, %r18, -54;

BB92_2:
	add.s32 	%r46, %r45, -1023;
	and.b32  	%r19, %r44, -2146435073;
	or.b32  	%r20, %r19, 1072693248;
	mov.b64 	%fd138, {%r43, %r20};
	setp.lt.u32	%p2, %r20, 1073127583;
	@%p2 bra 	BB92_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd138;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd138;
	}
	add.s32 	%r23, %r22, -1048576;
	mov.b64 	%fd138, {%r21, %r23};
	add.s32 	%r46, %r45, -1022;

BB92_4:
	add.f64 	%fd18, %fd138, 0d3FF0000000000000;
	mov.f64 	%fd19, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd17,%fd18;
	// inline asm
	neg.f64 	%fd20, %fd18;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd19;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd17, %fd17;
	add.f64 	%fd24, %fd138, 0dBFF0000000000000;
	mul.f64 	%fd25, %fd24, %fd23;
	fma.rn.f64 	%fd26, %fd24, %fd23, %fd25;
	mul.f64 	%fd27, %fd26, %fd26;
	mov.f64 	%fd28, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd29, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3F6249249242B910;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3F89999999999DFB;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	sub.f64 	%fd41, %fd24, %fd26;
	add.f64 	%fd42, %fd41, %fd41;
	neg.f64 	%fd43, %fd26;
	fma.rn.f64 	%fd44, %fd43, %fd24, %fd42;
	mul.f64 	%fd45, %fd23, %fd44;
	fma.rn.f64 	%fd46, %fd40, %fd27, 0d3FB5555555555555;
	mov.f64 	%fd47, 0d3FB5555555555555;
	sub.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd40, %fd27, %fd48;
	add.f64 	%fd50, %fd49, 0d0000000000000000;
	add.f64 	%fd51, %fd50, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd52, %fd46, %fd51;
	sub.f64 	%fd53, %fd46, %fd52;
	add.f64 	%fd54, %fd53, %fd51;
	mul.rn.f64 	%fd55, %fd26, %fd26;
	neg.f64 	%fd56, %fd55;
	fma.rn.f64 	%fd57, %fd26, %fd26, %fd56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd45;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd45;
	}
	add.s32 	%r26, %r25, 1048576;
	mov.b64 	%fd58, {%r24, %r26};
	fma.rn.f64 	%fd59, %fd26, %fd58, %fd57;
	mul.rn.f64 	%fd60, %fd55, %fd26;
	neg.f64 	%fd61, %fd60;
	fma.rn.f64 	%fd62, %fd55, %fd26, %fd61;
	fma.rn.f64 	%fd63, %fd55, %fd45, %fd62;
	fma.rn.f64 	%fd64, %fd59, %fd26, %fd63;
	mul.rn.f64 	%fd65, %fd52, %fd60;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd60, %fd66;
	fma.rn.f64 	%fd68, %fd52, %fd64, %fd67;
	fma.rn.f64 	%fd69, %fd54, %fd60, %fd68;
	add.f64 	%fd70, %fd65, %fd69;
	sub.f64 	%fd71, %fd65, %fd70;
	add.f64 	%fd72, %fd71, %fd69;
	add.f64 	%fd73, %fd26, %fd70;
	sub.f64 	%fd74, %fd26, %fd73;
	add.f64 	%fd75, %fd74, %fd70;
	add.f64 	%fd76, %fd75, %fd72;
	add.f64 	%fd77, %fd76, %fd45;
	add.f64 	%fd78, %fd73, %fd77;
	sub.f64 	%fd79, %fd73, %fd78;
	add.f64 	%fd80, %fd79, %fd77;
	xor.b32  	%r27, %r46, -2147483648;
	mov.u32 	%r28, -2147483648;
	mov.u32 	%r29, 1127219200;
	mov.b64 	%fd81, {%r27, %r29};
	mov.b64 	%fd82, {%r28, %r29};
	sub.f64 	%fd83, %fd81, %fd82;
	mov.f64 	%fd84, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd85, %fd83, %fd84, %fd78;
	neg.f64 	%fd86, %fd83;
	fma.rn.f64 	%fd87, %fd86, %fd84, %fd85;
	sub.f64 	%fd88, %fd87, %fd78;
	sub.f64 	%fd89, %fd80, %fd88;
	mov.f64 	%fd90, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd91, %fd83, %fd90, %fd89;
	add.f64 	%fd92, %fd85, %fd91;
	sub.f64 	%fd93, %fd85, %fd92;
	add.f64 	%fd94, %fd93, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd15;
	}
	add.s32 	%r31, %r30, %r30;
	and.b32  	%r32, %r30, -15728641;
	setp.gt.u32	%p3, %r31, -33554433;
	selp.b32	%r33, %r32, %r30, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd15;
	}
	mov.b64 	%fd95, {%r34, %r33};
	mul.rn.f64 	%fd96, %fd92, %fd95;
	neg.f64 	%fd97, %fd96;
	fma.rn.f64 	%fd98, %fd92, %fd95, %fd97;
	fma.rn.f64 	%fd99, %fd94, %fd95, %fd98;
	add.f64 	%fd4, %fd96, %fd99;
	sub.f64 	%fd100, %fd96, %fd4;
	add.f64 	%fd5, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd4;
	}
	mov.b32 	 %f1, %r13;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p4, %f2, 0f40874911;
	@%p4 bra 	BB92_6;

	setp.lt.s32	%p5, %r13, 0;
	selp.f64	%fd101, 0d0000000000000000, 0d7FF0000000000000, %p5;
	abs.f64 	%fd102, %fd4;
	setp.gtu.f64	%p6, %fd102, 0d7FF0000000000000;
	add.f64 	%fd103, %fd4, %fd4;
	selp.f64	%fd140, %fd103, %fd101, %p6;
	bra.uni 	BB92_10;

BB92_6:
	mov.f64 	%fd104, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd105, %fd4, %fd104;
	mov.f64 	%fd106, 0d4338000000000000;
	add.rn.f64 	%fd107, %fd105, %fd106;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd107;
	}
	mov.f64 	%fd108, 0dC338000000000000;
	add.rn.f64 	%fd109, %fd107, %fd108;
	mov.f64 	%fd110, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd111, %fd109, %fd110, %fd4;
	mov.f64 	%fd112, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd113, %fd109, %fd112, %fd111;
	mov.f64 	%fd114, 0d3E928AF3FCA213EA;
	mov.f64 	%fd115, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd116, %fd115, %fd113, %fd114;
	mov.f64 	%fd117, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd118, %fd116, %fd113, %fd117;
	mov.f64 	%fd119, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd120, %fd118, %fd113, %fd119;
	mov.f64 	%fd121, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd122, %fd120, %fd113, %fd121;
	mov.f64 	%fd123, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd124, %fd122, %fd113, %fd123;
	mov.f64 	%fd125, 0d3F81111111122322;
	fma.rn.f64 	%fd126, %fd124, %fd113, %fd125;
	mov.f64 	%fd127, 0d3FA55555555502A1;
	fma.rn.f64 	%fd128, %fd126, %fd113, %fd127;
	mov.f64 	%fd129, 0d3FC5555555555511;
	fma.rn.f64 	%fd130, %fd128, %fd113, %fd129;
	mov.f64 	%fd131, 0d3FE000000000000B;
	fma.rn.f64 	%fd132, %fd130, %fd113, %fd131;
	fma.rn.f64 	%fd134, %fd132, %fd113, %fd19;
	fma.rn.f64 	%fd139, %fd134, %fd113, %fd19;
	abs.s32 	%r35, %r14;
	setp.lt.s32	%p7, %r35, 1023;
	@%p7 bra 	BB92_8;

	add.s32 	%r36, %r14, 2046;
	shl.b32 	%r37, %r36, 19;
	and.b32  	%r38, %r37, -1048576;
	shl.b32 	%r39, %r36, 20;
	sub.s32 	%r47, %r39, %r38;
	mov.u32 	%r40, 0;
	mov.b64 	%fd135, {%r40, %r38};
	mul.f64 	%fd139, %fd139, %fd135;
	bra.uni 	BB92_9;

BB92_8:
	shl.b32 	%r41, %r14, 20;
	add.s32 	%r47, %r41, 1072693248;

BB92_9:
	mov.u32 	%r42, 0;
	mov.b64 	%fd136, {%r42, %r47};
	mul.f64 	%fd140, %fd139, %fd136;

BB92_10:
	abs.f64 	%fd137, %fd140;
	setp.eq.f64	%p8, %fd137, 0d7FF0000000000000;
	@%p8 bra 	BB92_12;

	fma.rn.f64 	%fd140, %fd140, %fd5, %fd140;

BB92_12:
	st.param.f64	[funj_retval0+0], %fd140;
	ret;
}

.func  (.param .b64 funj_retval0) __internal_lgamma_pos(
	.param .b64 __internal_lgamma_pos_param_0
)
{
	.reg .pred 	%p<22>;
	.reg .s32 	%r<54>;
	.reg .f64 	%fd<298>;


	ld.param.f64 	%fd24, [__internal_lgamma_pos_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd24;
	}
	setp.gt.s32	%p1, %r50, 1074266111;
	@%p1 bra 	BB93_17;

	setp.gt.s32	%p2, %r50, 1073217535;
	@%p2 bra 	BB93_16;

	setp.gt.s32	%p3, %r50, 1072064101;
	@%p3 bra 	BB93_15;

	mov.f64 	%fd25, 0d3EA7B77CEB0625E8;
	mov.f64 	%fd26, 0dBE7844988BFE6590;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0dBE998C69C8710CC4;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0dBEF6527A5A11CF6E;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F20EC2950B1B5DE;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0dBF2C4D80C24BA278;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0dBF5315B4E8CC0D09;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3F7D917F15D50020;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0dBF83B4ABB41CB6FA;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0dBFA59AF1275B7120;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3FC5512321A168A0;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	mov.f64 	%fd46, 0dBFA5815E8FDCE74C;
	fma.rn.f64 	%fd47, %fd45, %fd24, %fd46;
	mov.f64 	%fd48, 0dBFE4FCF4026ADD1A;
	fma.rn.f64 	%fd49, %fd47, %fd24, %fd48;
	mov.f64 	%fd50, 0d3FE2788CFC6FB5C8;
	fma.rn.f64 	%fd51, %fd49, %fd24, %fd50;
	mul.f64 	%fd52, %fd51, %fd24;
	fma.rn.f64 	%fd1, %fd52, %fd24, %fd24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd1;
	}
	setp.gt.f64	%p4, %fd1, 0d0000000000000000;
	setp.lt.s32	%p5, %r46, 2146435072;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB93_9;

	abs.f64 	%fd53, %fd1;
	setp.gtu.f64	%p7, %fd53, 0d7FF0000000000000;
	@%p7 bra 	BB93_8;

	setp.neu.f64	%p8, %fd1, 0d0000000000000000;
	@%p8 bra 	BB93_7;

	mov.f64 	%fd54, 0dFFF0000000000000;
	neg.f64 	%fd297, %fd54;
	bra.uni 	BB93_32;

BB93_7:
	setp.eq.f64	%p9, %fd1, 0d7FF0000000000000;
	selp.f64	%fd2, %fd1, 0dFFF8000000000000, %p9;
	neg.f64 	%fd297, %fd2;
	bra.uni 	BB93_32;

BB93_8:
	add.f64 	%fd3, %fd1, %fd1;
	neg.f64 	%fd297, %fd3;
	bra.uni 	BB93_32;

BB93_9:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd1;
	}
	setp.lt.s32	%p10, %r46, 1048576;
	@%p10 bra 	BB93_11;

	mov.u32 	%r48, -1023;
	bra.uni 	BB93_12;

BB93_11:
	mul.f64 	%fd55, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd55;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd55;
	}
	mov.u32 	%r48, -1077;

BB93_12:
	shr.u32 	%r23, %r46, 20;
	add.s32 	%r49, %r48, %r23;
	and.b32  	%r24, %r46, -2146435073;
	or.b32  	%r25, %r24, 1072693248;
	mov.b64 	%fd294, {%r47, %r25};
	setp.lt.s32	%p11, %r25, 1073127583;
	@%p11 bra 	BB93_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd294;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd294;
	}
	add.s32 	%r28, %r27, -1048576;
	mov.b64 	%fd294, {%r26, %r28};
	add.s32 	%r49, %r49, 1;

BB93_14:
	add.f64 	%fd57, %fd294, 0d3FF0000000000000;
	mov.f64 	%fd58, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd56,%fd57;
	// inline asm
	neg.f64 	%fd59, %fd57;
	fma.rn.f64 	%fd60, %fd59, %fd56, %fd58;
	fma.rn.f64 	%fd61, %fd60, %fd60, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd56, %fd56;
	add.f64 	%fd63, %fd294, 0dBFF0000000000000;
	mul.f64 	%fd64, %fd63, %fd62;
	fma.rn.f64 	%fd65, %fd63, %fd62, %fd64;
	mul.f64 	%fd66, %fd65, %fd65;
	mov.f64 	%fd67, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd68, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd69, %fd68, %fd66, %fd67;
	mov.f64 	%fd70, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd71, %fd69, %fd66, %fd70;
	mov.f64 	%fd72, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd73, %fd71, %fd66, %fd72;
	mov.f64 	%fd74, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd75, %fd73, %fd66, %fd74;
	mov.f64 	%fd76, 0d3F624924923BE72D;
	fma.rn.f64 	%fd77, %fd75, %fd66, %fd76;
	mov.f64 	%fd78, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd79, %fd77, %fd66, %fd78;
	mov.f64 	%fd80, 0d3FB5555555555554;
	fma.rn.f64 	%fd81, %fd79, %fd66, %fd80;
	sub.f64 	%fd82, %fd63, %fd65;
	add.f64 	%fd83, %fd82, %fd82;
	neg.f64 	%fd84, %fd65;
	fma.rn.f64 	%fd85, %fd84, %fd63, %fd83;
	mul.f64 	%fd86, %fd62, %fd85;
	mul.f64 	%fd87, %fd81, %fd66;
	fma.rn.f64 	%fd88, %fd87, %fd65, %fd86;
	xor.b32  	%r29, %r49, -2147483648;
	mov.u32 	%r30, -2147483648;
	mov.u32 	%r31, 1127219200;
	mov.b64 	%fd89, {%r29, %r31};
	mov.b64 	%fd90, {%r30, %r31};
	sub.f64 	%fd91, %fd89, %fd90;
	mov.f64 	%fd92, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd93, %fd91, %fd92, %fd65;
	neg.f64 	%fd94, %fd91;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	sub.f64 	%fd96, %fd95, %fd65;
	sub.f64 	%fd97, %fd88, %fd96;
	mov.f64 	%fd98, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd99, %fd91, %fd98, %fd97;
	add.f64 	%fd7, %fd93, %fd99;
	neg.f64 	%fd297, %fd7;
	bra.uni 	BB93_32;

BB93_15:
	mov.f64 	%fd100, 0d3FF0000000000000;
	sub.f64 	%fd101, %fd100, %fd24;
	mov.f64 	%fd102, 0d3FA3EB504359EB88;
	mov.f64 	%fd103, 0d3F881F6D2A4C4310;
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0d3FAE35D8DEB06317;
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3FAED469A8B6ECCE;
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0d3FACC1B1C357BEFE;
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3FAD7154DB67F79F;
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0d3FAFCC622CF2F7BB;
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FB11747A4D1CC43;
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0d3FB24CE16A21B8AC;
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FB3B1C21A7BCB00;
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0d3FB556723452ED57;
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	mov.f64 	%fd123, 0d3FB748C00891544F;
	fma.rn.f64 	%fd124, %fd122, %fd101, %fd123;
	mov.f64 	%fd125, 0d3FB9A0207808CF40;
	fma.rn.f64 	%fd126, %fd124, %fd101, %fd125;
	mov.f64 	%fd127, 0d3FBC80673B8AE26B;
	fma.rn.f64 	%fd128, %fd126, %fd101, %fd127;
	mov.f64 	%fd129, 0d3FC010B364B7E555;
	fma.rn.f64 	%fd130, %fd128, %fd101, %fd129;
	mov.f64 	%fd131, 0d3FC2703A1D239658;
	fma.rn.f64 	%fd132, %fd130, %fd101, %fd131;
	mov.f64 	%fd133, 0d3FC5B40CB1137E6E;
	fma.rn.f64 	%fd134, %fd132, %fd101, %fd133;
	mov.f64 	%fd135, 0d3FCA8B9C17AC4F03;
	fma.rn.f64 	%fd136, %fd134, %fd101, %fd135;
	mov.f64 	%fd137, 0d3FD151322AC7CB52;
	fma.rn.f64 	%fd138, %fd136, %fd101, %fd137;
	mov.f64 	%fd139, 0d3FD9A4D55BEAB1D4;
	fma.rn.f64 	%fd140, %fd138, %fd101, %fd139;
	mov.f64 	%fd141, 0d3FEA51A6625307D6;
	fma.rn.f64 	%fd142, %fd140, %fd101, %fd141;
	mov.f64 	%fd143, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd144, %fd142, %fd101, %fd143;
	mul.f64 	%fd297, %fd144, %fd101;
	bra.uni 	BB93_32;

BB93_16:
	add.f64 	%fd145, %fd24, 0dC000000000000000;
	mov.f64 	%fd146, 0dBE71FA71D78C0EE2;
	mov.f64 	%fd147, 0d3E452636124338B3;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	mov.f64 	%fd149, 0d3E8D111F31E61306;
	fma.rn.f64 	%fd150, %fd148, %fd145, %fd149;
	mov.f64 	%fd151, 0dBEA0502BBE1B2706;
	fma.rn.f64 	%fd152, %fd150, %fd145, %fd151;
	mov.f64 	%fd153, 0d3EB06850B2970292;
	fma.rn.f64 	%fd154, %fd152, %fd145, %fd153;
	mov.f64 	%fd155, 0dBEC108474875033D;
	fma.rn.f64 	%fd156, %fd154, %fd145, %fd155;
	mov.f64 	%fd157, 0d3ED24ACCC62909DC;
	fma.rn.f64 	%fd158, %fd156, %fd145, %fd157;
	mov.f64 	%fd159, 0dBEE3CB25209E63BE;
	fma.rn.f64 	%fd160, %fd158, %fd145, %fd159;
	mov.f64 	%fd161, 0d3EF581CBBC8CDC7B;
	fma.rn.f64 	%fd162, %fd160, %fd145, %fd161;
	mov.f64 	%fd163, 0dBF078E04B85C7597;
	fma.rn.f64 	%fd164, %fd162, %fd145, %fd163;
	mov.f64 	%fd165, 0d3F1A12730CF45051;
	fma.rn.f64 	%fd166, %fd164, %fd145, %fd165;
	mov.f64 	%fd167, 0dBF2D3FD354062012;
	fma.rn.f64 	%fd168, %fd166, %fd145, %fd167;
	mov.f64 	%fd169, 0d3F40B36B0B4DE323;
	fma.rn.f64 	%fd170, %fd168, %fd145, %fd169;
	mov.f64 	%fd171, 0dBF538AC5C6D0317A;
	fma.rn.f64 	%fd172, %fd170, %fd145, %fd171;
	mov.f64 	%fd173, 0d3F67ADD6EAAB19FC;
	fma.rn.f64 	%fd174, %fd172, %fd145, %fd173;
	mov.f64 	%fd175, 0dBF7E404FC20E4D5B;
	fma.rn.f64 	%fd176, %fd174, %fd145, %fd175;
	mov.f64 	%fd177, 0d3F951322AC7DA390;
	fma.rn.f64 	%fd178, %fd176, %fd145, %fd177;
	mov.f64 	%fd179, 0dBFB13E001A5578A3;
	fma.rn.f64 	%fd180, %fd178, %fd145, %fd179;
	mov.f64 	%fd181, 0d3FD4A34CC4A60FA3;
	fma.rn.f64 	%fd182, %fd180, %fd145, %fd181;
	mov.f64 	%fd183, 0d3FDB0EE6072093CF;
	fma.rn.f64 	%fd184, %fd182, %fd145, %fd183;
	mul.f64 	%fd297, %fd184, %fd145;
	bra.uni 	BB93_32;

BB93_17:
	setp.gt.s32	%p12, %r50, 1075838975;
	@%p12 bra 	BB93_19;

	add.f64 	%fd185, %fd24, 0dC008000000000000;
	mov.f64 	%fd186, 0dC1122B7730207EF3;
	mov.f64 	%fd187, 0dC0AF7040BB18FB05;
	fma.rn.f64 	%fd188, %fd187, %fd185, %fd186;
	mov.f64 	%fd189, 0dC1585A0DB81DE7D0;
	fma.rn.f64 	%fd190, %fd188, %fd185, %fd189;
	mov.f64 	%fd191, 0dC18A992B8BA94677;
	fma.rn.f64 	%fd192, %fd190, %fd185, %fd191;
	mov.f64 	%fd193, 0dC1AAC5CB6957CC20;
	fma.rn.f64 	%fd194, %fd192, %fd185, %fd193;
	mov.f64 	%fd195, 0dC1BC0E2B308774BE;
	fma.rn.f64 	%fd196, %fd194, %fd185, %fd195;
	mov.f64 	%fd197, 0dC1C6BA13DCAE7F67;
	fma.rn.f64 	%fd198, %fd196, %fd185, %fd197;
	mov.f64 	%fd199, 0dC1CCF33B9C3D120C;
	fma.rn.f64 	%fd200, %fd198, %fd185, %fd199;
	add.f64 	%fd201, %fd185, 0dC08FF62E0BE189FE;
	mov.f64 	%fd202, 0dC10074FACE10C93F;
	fma.rn.f64 	%fd203, %fd201, %fd185, %fd202;
	mov.f64 	%fd204, 0dC151B662F8D75791;
	fma.rn.f64 	%fd205, %fd203, %fd185, %fd204;
	mov.f64 	%fd206, 0dC18EE64AB4D207F7;
	fma.rn.f64 	%fd207, %fd205, %fd185, %fd206;
	mov.f64 	%fd208, 0dC1B9051687C9951A;
	fma.rn.f64 	%fd209, %fd207, %fd185, %fd208;
	mov.f64 	%fd210, 0dC1D2B866BF0B853D;
	fma.rn.f64 	%fd211, %fd209, %fd185, %fd210;
	mov.f64 	%fd212, 0dC1D4E2130E9DC133;
	fma.rn.f64 	%fd213, %fd211, %fd185, %fd212;
	div.rn.f64 	%fd214, %fd200, %fd213;
	add.f64 	%fd297, %fd214, %fd185;
	bra.uni 	BB93_32;

BB93_19:
	// inline asm
	rcp.approx.ftz.f64 %fd215,%fd24;
	// inline asm
	neg.f64 	%fd13, %fd24;
	mov.f64 	%fd217, 0d3FF0000000000000;
	fma.rn.f64 	%fd218, %fd13, %fd215, %fd217;
	fma.rn.f64 	%fd219, %fd218, %fd218, %fd218;
	fma.rn.f64 	%fd220, %fd219, %fd215, %fd215;
	mul.f64 	%fd221, %fd220, %fd220;
	mov.f64 	%fd222, 0d3F4B68B992738FBF;
	mov.f64 	%fd223, 0dBF5AC321034783F9;
	fma.rn.f64 	%fd224, %fd223, %fd221, %fd222;
	mov.f64 	%fd225, 0dBF4380D01E4F7B8C;
	fma.rn.f64 	%fd226, %fd224, %fd221, %fd225;
	mov.f64 	%fd227, 0d3F4A019FA29F7264;
	fma.rn.f64 	%fd228, %fd226, %fd221, %fd227;
	mov.f64 	%fd229, 0dBF66C16C16B2ACEC;
	fma.rn.f64 	%fd230, %fd228, %fd221, %fd229;
	mov.f64 	%fd231, 0d3FB5555555555545;
	fma.rn.f64 	%fd232, %fd230, %fd221, %fd231;
	mov.f64 	%fd233, 0d3FED67F1C864BEAE;
	fma.rn.f64 	%fd14, %fd232, %fd220, %fd233;
	setp.lt.s32	%p13, %r50, 2146435072;
	setp.gt.f64	%p14, %fd24, 0d0000000000000000;
	and.pred  	%p15, %p14, %p13;
	@%p15 bra 	BB93_25;

	abs.f64 	%fd234, %fd24;
	setp.gtu.f64	%p16, %fd234, 0d7FF0000000000000;
	@%p16 bra 	BB93_24;

	setp.neu.f64	%p17, %fd24, 0d0000000000000000;
	@%p17 bra 	BB93_23;

	mov.f64 	%fd296, 0dFFF0000000000000;
	bra.uni 	BB93_31;

BB93_23:
	setp.eq.f64	%p18, %fd24, 0d7FF0000000000000;
	selp.f64	%fd296, %fd24, 0dFFF8000000000000, %p18;
	bra.uni 	BB93_31;

BB93_24:
	add.f64 	%fd296, %fd24, %fd24;
	bra.uni 	BB93_31;

BB93_25:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd24;
	}
	setp.lt.s32	%p19, %r50, 1048576;
	@%p19 bra 	BB93_27;

	mov.u32 	%r52, -1023;
	bra.uni 	BB93_28;

BB93_27:
	mul.f64 	%fd236, %fd24, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd236;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd236;
	}
	mov.u32 	%r52, -1077;

BB93_28:
	shr.u32 	%r34, %r50, 20;
	add.s32 	%r53, %r52, %r34;
	and.b32  	%r35, %r50, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd295, {%r51, %r36};
	setp.lt.s32	%p20, %r36, 1073127583;
	@%p20 bra 	BB93_30;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd295;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd295;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd295, {%r37, %r39};
	add.s32 	%r53, %r53, 1;

BB93_30:
	add.f64 	%fd238, %fd295, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd237,%fd238;
	// inline asm
	neg.f64 	%fd240, %fd238;
	fma.rn.f64 	%fd241, %fd240, %fd237, %fd217;
	fma.rn.f64 	%fd242, %fd241, %fd241, %fd241;
	fma.rn.f64 	%fd243, %fd242, %fd237, %fd237;
	add.f64 	%fd244, %fd295, 0dBFF0000000000000;
	mul.f64 	%fd245, %fd244, %fd243;
	fma.rn.f64 	%fd246, %fd244, %fd243, %fd245;
	mul.f64 	%fd247, %fd246, %fd246;
	mov.f64 	%fd248, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd249, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd250, %fd249, %fd247, %fd248;
	mov.f64 	%fd251, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd252, %fd250, %fd247, %fd251;
	mov.f64 	%fd253, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd254, %fd252, %fd247, %fd253;
	mov.f64 	%fd255, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd256, %fd254, %fd247, %fd255;
	mov.f64 	%fd257, 0d3F624924923BE72D;
	fma.rn.f64 	%fd258, %fd256, %fd247, %fd257;
	mov.f64 	%fd259, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd260, %fd258, %fd247, %fd259;
	mov.f64 	%fd261, 0d3FB5555555555554;
	fma.rn.f64 	%fd262, %fd260, %fd247, %fd261;
	sub.f64 	%fd263, %fd244, %fd246;
	add.f64 	%fd264, %fd263, %fd263;
	neg.f64 	%fd265, %fd246;
	fma.rn.f64 	%fd266, %fd265, %fd244, %fd264;
	mul.f64 	%fd267, %fd243, %fd266;
	mul.f64 	%fd268, %fd262, %fd247;
	fma.rn.f64 	%fd269, %fd268, %fd246, %fd267;
	xor.b32  	%r40, %r53, -2147483648;
	mov.u32 	%r41, -2147483648;
	mov.u32 	%r42, 1127219200;
	mov.b64 	%fd270, {%r40, %r42};
	mov.b64 	%fd271, {%r41, %r42};
	sub.f64 	%fd272, %fd270, %fd271;
	mov.f64 	%fd273, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd246;
	neg.f64 	%fd275, %fd272;
	fma.rn.f64 	%fd276, %fd275, %fd273, %fd274;
	sub.f64 	%fd277, %fd276, %fd246;
	sub.f64 	%fd278, %fd269, %fd277;
	mov.f64 	%fd279, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd280, %fd272, %fd279, %fd278;
	add.f64 	%fd296, %fd274, %fd280;

BB93_31:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd296;
	}
	add.s32 	%r44, %r43, -1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd296;
	}
	mov.b64 	%fd281, {%r45, %r44};
	add.f64 	%fd282, %fd24, 0dBFE0000000000000;
	fma.rn.f64 	%fd283, %fd281, %fd282, %fd14;
	fma.rn.f64 	%fd284, %fd281, %fd282, %fd13;
	add.f64 	%fd285, %fd284, %fd283;
	setp.eq.f64	%p21, %fd24, 0d7FF0000000000000;
	selp.f64	%fd297, %fd24, %fd285, %p21;

BB93_32:
	st.param.f64	[funj_retval0+0], %fd297;
	ret;
}


